1
TnT - A Statistical Part-Of-Speech Tagger
Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.
Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.
A recent comparison has even shown that TnT performs significantly better for the tested corpora.
We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.
Furthermore, we present evaluations on two corpora.
We achieve the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.

2
Sentence Reduction For Automatic Text Summarization
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose.
The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals.
Reduction can significantly improve the conciseness of automatic summaries.
We study a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed.
In our approach, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries.

3
Advances In Domain Independent Linear Text Segmentation
This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).
Inter-sentence similarity is replaced by rank in the local context.
Boundary locations are discovered by divisive clustering.
We design an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus.

4
A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation
This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context.
Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.
We present an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.

5
A Maximum-Entropy-Inspired Parser
We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] "standard" sections of the Wall Street Journal tree- bank.
This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].
The major technical innovation is the use of a "maximum-entropy-inspired" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.
We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.
As an alternative to hard coded heuristics, we proposed to recover the Penn functional tags automatically.
Our parser is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents.

6
An Unsupervised Method For Detecting Grammatical Errors
We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.
The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).
The error-recognition system, ALEK, performs with about 80% precision and 20% recall.
We attempt to identify errors on the basis of context -- more specifically a 2 word window around the word of interest, from which we consider function words and POS tags.
We use a mutual information measure in addition to raw frequency of n grams.
The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors.
We utilize mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus.

7
Cut And Paste Based Text Summarization
We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts.
The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resuiting phrases together as coherent sentences.
Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.
We first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output.
We manually analyze 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.

8
Trainable Methods For Surface Natural Language Generation
We present three systems for surface natural language generation that are trainable from annotated corpora.
The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.
All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation.
NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.
The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase.
We present experiments in which we generate phrases to describe flights in the air travel domain.
We use maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features.
We use a large collection of generation templates for surface realization.
We present maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain.

9
A Novel Use Of Statistical Parsing To Extract Information From Text
Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.
In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
Our rule-based methods employ a number of linguistic rules to capture relation patterns.
We take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations.
We integrate various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.
We combine entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable.

10
Assigning Function Tags To Parsed Text
It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree.
For example, the Penn Treebank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels.
We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice.
As an alternative to hard coded heuristics, we propose to recover the Penn functional tags automatically.

11
Using Semantic Preferences To Identify Verbal Participation In Role Switching Alternations
We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.
The method uses selectional preferences acquired as probability distributions over WordNet.
Preferences for the target slots are compared using a measure of distributional similarity.
The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.
We use skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions.

12
A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text
Our part of speech tagger can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.
Our part-of-speech tagger performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them.

13
Applied Text Generation
We divide tasks in the generation process into three stages: the text planner has access only to information about communicative goals, the discourse context, and semantics, and generates a non-linguistic representation of text structure and content. The sentence planner chooses abstract linguistic resources. It passes an abstract lexico-syntactic specification5 to the Realizer, which inflects, adds function words, and linearizes, thus producing the surface string.

14
A Practical Part-Of-Speech Tagger
We present an implementation of a part-of-speech tagger based on a hidden Markov model.
The methodology enables robust and accurate tagging with few resource requirements.
Only a lexicon and some unlabeled training text are required.
Accuracy exceeds 96%.
We describe implementation strategies and optimizations which result in high-speed operation.
Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
Our semi-supervised model makes use of both labeled training text and some amount of unlabeled text.
We train statistical models using unlabeled data with the expectation maximization algorithm.
We report very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.

15
A Simple Rule-Based Part Of Speech Tagger
Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods.
In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.
The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another.
Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging.
The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.
Our rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentence.
We also show that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy.

16
Termight: Identifying And Translating Technical Terminology
We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations.
The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations.
Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff.
The extraction algorithms emphasize completeness.
Alternative proposals are likely to miss important but infrequent terms/translations.
To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes.
Termight is currently being used by the translators at AT&T Business Translation Services (formerly AT&T Language Line Services).

17
Does Baum-Welch Re-Estimation Help Taggers?
In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.
Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.
More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch re-estimation to automatically refine the model.
In this paper, I report two experiments designed to determine how much manual training information is needed.
The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy.
The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation.
In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it.
The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged.
Heuristics for deciding how to use re-estimation in an effective manner are given.
The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.
We report an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags.

18
Three Heads Are Better Than One
Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ.
Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text.
In the latest version of the Pan-gloss MT project, we collect the results of three translation engines -- typically, sub-sentential chunks -- in a chart data structure.
Since the individual MT systems operate completely independently, their re- sults may be incomplete, conflicting, or redundant.
We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the "best cover").
This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced.
It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations.
The current system operates primarily in a human-aided MT mode.
The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method.
Individual MT engines will be reported separately and are not, therefore, described in detail here.
We produce the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines.
We develop a multi-engine MT system, which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence.

19
A Maximum Entropy Approach To Identifying Sentence Boundaries
We present a trainable model for identify- ing sentence boundaries in raw text.
Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of . , ?, and / as either a valid or invalid sentence boundary.
The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information.
The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language.
Performance is compa rable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.
Our statistical system, mxTerminator employs simpler lexical features of the words to the left and right of the candidate period.

20
A Non-Projective Dependency Parser
We describe a practical parser for unrestricted dependencies.
The parser creates links between words and names the links according to their syntactic functions.
We first describe the older Constraint Grammar parser where many of the ideas come from.
Then we proceed to describe the central ideas of our new parser.
Finally, the parser is evaluated.

21
An Annotation Scheme For Free Word Order Languages
We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.
Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.
The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata.
We release the NEGRA corpus, a hand parsed corpus of German newspaper text containing approximately 20,000 sentences.

22
Nymble: A High-Performance Learning Name-Finder
This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.
We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.
We develop Nymble, an HMM-based name tagging system operating in English and Spanish.
Nymble uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text.

23
Disambiguation Of Proper Names In Text
Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents.
We analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.
We use hand-written rules and knowledge bases to classify proper names into broad categories.

24
A Fast And Portable Realizer For Text Generation Systems
We release a surface realizer, RealPro, and it is  intended as off-the-shelf plug-in realizer.
Our RealPro surface realizer which produces a surface linguistic utterance.

25
Automatic Extraction Of Subcategorization From Corpora
We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora.
Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English.
An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes.
We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount.
We use a grammar and a sophisticated parsing tool for argument-adjunct distinction.

26
Exploiting A Probabilistic Hierarchical Model For Generation
Previous stochastic approaches to generation do not include a tree-based representation of syntax.
While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar.
We present initial results showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a hand-crafted grammar outperforms both.
Our system, FERGUS takes dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus.
The Fergus system employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees.

27
Effects Of Adjective Orientation And Gradability On Sentence Subjectivity
Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval.
We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity.
A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels.
Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity.
We report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity.
We show that automatically detected gradable adjectives are a useful feature for subjectivity classification.

28
The Automated Acquisition Of Topic Signatures For Text Summarization
In order to produce a good summary, one has to identify the most relevant portions of a given text.
We describe in this paper a method for automatically training topic signatures -- sets of related words, with associated weights, organized around head topics and illustrate with signature we created with 6,194 TREC collection texts over 4 selected topics.
We describe the possible integration of topic signatures with ontologies and its evaluaton on an automated text summarization system.
We first introduced topic signatures which are topic relevant terms for summarization.

29
Automatic Acquisition Of Domain Knowledge For Information Extraction
In developing an Information Extraction (IE) system for a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text.
This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events.
This paper presents an alternative approach, based on an automatic discovery procedure, EXDISCO, which identifies a set; of relevant documents and a set of event patterns from un-annotated text, starting from a small set of "seed patterns".
We evaluate EXDISCO by comparing the performance of discovered patterns against that of manually constructed systems on actual extraction tasks.
We propose an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck.
We choose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns.
ExDisco uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input.

30
More Accurate Tests For The Statistical Significance Of Result Differences
Statistical significance testing of differences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing.
Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques.
This underestimation comes from an independence assumption that is often violated.
We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests.
Standard deviations for F scores are estimated with bootstrap resampling.

31
A Comparison Of Alignment Models For Statistical Machine Translation
In this paper, we present and compare various alignnment models for statistical machine translation.
We propose to measure tile quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments.
We also comppre the impact of different alignment models on tile translation quality of a statistical machine translation system.
In order to improve transition models in the HMM based alignment, we extend the transition models to be word-class dependent.

32
Base Noun Phrase Translation Using Web Data And The EM Algorithm
We consider here the problem of Base Noun Phrase translation.
We propose a new method to perform the task.
For a given Base NP, we first search its translation candidates from the web.
We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed.
In one method, we employ an ensemble of Naive Bayesian Classifiers constructed with the EM Algorithm.
In the other method, we use TF-IDF vectors also constructed with the EM Algorithm.
Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies.
In our method, translation candidates of a term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term.

33
Efficient Support Vector Classifiers For Named Entity Recognition
Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date.
It is a key technology of Information Extraction and Open-Domain Question Answering.
First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.
However, off-the-shelf SVM classifiers are too inefficient for this task.
Therefore, we present a method that makes the system substantially faster.
This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging.
We also present an SVM-based feature selection method and an efficient training method.
We propose Kernel Expansion that is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function.
We propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast.

34
A Graph Model For Unsupervised Lexical Acquisition
This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms.
The graph model is built by linking pairs of words which participate in particular syntactic relationships.
We focus on the symmetric relationship between pairs of nouns which occur together in lists.
An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes.
The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word.
We try to find graph regions that are more connected internally than externally.

35
Identifying Anaphoric And Non-Anaphoric Noun Phrases To Improve Coreference Resolution
We present a supervised learning approach o identification of anaphoric and non-anaphoric noun phrases and how how such information can be incorporated into a coreference resolution system.
The resulting system outperforms the best MUC-6 and MUC-7 coreference resolution systems on the corresponding MUC coreference data sets, obtaining F-measures of 66.2 and 64.0, respectively.
We train a separate anaphoricity classifier in addition to a coreference model.
We challenge the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance.

36
Concept Discovery From Text
Broad-coverage lexical resources such as WordNet are extremely useful.
However, they often include many rare senses while missing domain-specific senses.
We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text.
It initially discovers a set of tight clusters called committees that are well scattered in the similarity space.
The centroid of the members of a committee is used as the feature vector of the cluster.
We proceed by assigning elements to their most similar cluster.
Evaluating cluster quality has always been a difficult task.
We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key).
Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality.
Mutual information (MI) is an information theoric measure and has been used in our method for  clustering words.

37
Building A Large-Scale Annotated Chinese Corpus
In this paper we address issues related to building a large-scale Chinese corpus.
We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.

38
Learning Question Classifiers
In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer.
These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
This paper presents a machine learning approach to question classification.
We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes.
We show accurate results on a large collection of free-form questions used in TREC 10.
We assign one of fifty possible types to a question based on features present in the question.
We have developed a machine learning approach which uses the SNoW learning architecture.

39
The LinGO Redwoods Treebank: Motivation And Preliminary Applications
The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank.
While several medium to large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the tree-bank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often year or decade-long) evolution of a large-scale treebank tend to fall behind the development of the field.
LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license.
We develop the HPSG LinGo Redwoods Treebank.
The Redwoods treebank has been created to provide annotated training material to permit statistical models for ambiguity resolution to be combined with the precise interpretations produced by the ERG.
Dynamic, discriminant-based treebanking is pioneered in the Redwoods treebank of English.

40
Deterministic Dependency Parsing Of English Text
This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.
When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.
Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme.
The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
We propose a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear.
Our deterministic shift/reduce classifier-based dependency parsing approach offers state-of-the-art accuracy with high efficiency due to a greedy search strategy.

41
Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.
The parser uses bit-vector operations to parallelise the basic parsing operations.
The parser is particularly useful when all analyses are needed rather than just the most probable one.
We apply the Viterbi algorithm, exploiting its ability to deal with highly-ambiguous grammars.

42
The Importance Of Supertagging For Wide-Coverage CCG Parsing
This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.
The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.
It also dramatically increases the speed of the parser.
We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser.
This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar.
We also further reduce the derivation space using constraints on category combination.
The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.
Our scores give an indication of how supertagging accuracy corresponds to overall dependency recovery.
We describe two log-linear parsing models for CCG: a normal-form derivation model and a dependency model.
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We propose a method for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.

43
Confidence Estimation For Machine Translation
We present a detailed study of confidence estimation for machine translation.
Various methods for determining whether MT output is correct are investigated, for both whole sentences and words.
Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed.
We present results on data from the NIST 2003 Chinese-to-English MT evaluation.
We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad.
We study sentence and word level features for translation error prediction.

44
Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources
We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster.
We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.
Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.
On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively.
Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.
The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
we introduce Microsoft Research Paraphrase Corpus (MSRPC).
We use Web-aggregated news stories to learn both sentence-level and word-level alignments.

45
Language Model Adaptation For Statistical Machine Translation Via Structured Query Models
We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
Specific language models are then build from the retrieved data and interpolated with a general background model.
Experiments show significant improvements when translating with these adapted language models.
We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus.
We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.
We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection.

46
ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson’s product moment correlation coefficient or  Spearman’s rank order correlation coefficient between human scores and automatic scores.
However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.
Unfortunately, these judgments are often inconsistent and very expensive to acquire.
In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.
We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included.
Smoothed per-sentence BLEU was used as a similarity metric.

47
Improving A Statistical MT System With Automatically Learned Rewrite Patterns
Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases.
Second, the ordering of target words in such systems does not respect linguistic phrase boundaries.
To address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target langauge.
Our system is a hybrid one.
The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences.
Our experiments show 10% relative improvement in Bleu measure.
We describe an approach for translation from French to English, where reordering rules are acquired automatically.
Our re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.

48
Part-Of-Speech Tagging In Context
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case.
Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons.
Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.
Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework.
While replicating earlier experiments, we discover that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens.
We show that he expectation maximization algorithm for bi tag HMMs is efficient and quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions.
We observe that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.

49
Chinese Segmentation And New Word Detection Using Conditional Random Fields
Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem.
This paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words.
We also present a probabilistic new word detection method, which further improves performance.
Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition.
State-of-the-art performance is obtained.
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation).
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and we use it for the Chinese word segmentation task by treating word segmentation as a binary decision task.
We first use this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.
We define the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary.

50
Question Answering Based On Semantic Structures
The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support.
In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model.
A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM).
In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions.
The results indicate enhanced accuracy over current state-of-the-art Q/A systems.
We explore the role of semantic structures in question answering.
We demonstrate that question answering can stand to benefit from broad coverage semantic processing.
Our question answering system takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference.

51
Towards Terascale Semantic Acquisition
Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it.
In this paper, we study the challenges of working at the terascale.
We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method.
We focus on the accuracy of these two systems as a function of processing time and corpus size.
We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.
We extend is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance.
We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).

52
Characterising Measures Of Lexical Distributional Similarity
This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used.
We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word.
We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy.
Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations).
Abstracting from results for concrete test sets, we try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.
We also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently.
We analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures.
We attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other.

53
Wide-Coverage Semantic Representations From A CCG Parser
This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser.
Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation.
We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.
We believe this is a major step towards wide-coverage semantic interpretation, one of the key objectives of the field of NLP.
We present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data.
We consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.

54
Semantic Role Labeling Via Integer Linear Programming Inference
We present a system for the semantic role labeling task.
The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process.
The system is tested on the data provided in CoNLL-2004 shared task on semantic role labeling and achieves very competitive results.
We formulate SRL as a constituent-by-constituent (C-by-C) tagging problem.

55
Determining The Sentiment Of Opinions
Identifying sentiments (the affective parts of opinions) is a challenging problem.
We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion.
The system contains a module for determining word sentiment and another for combining sentiments within a sentence.
We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.
We try to determine the final sentiment orientation of a given sentence by combining sentiment words within it.
We start with two lists of positive and negative seed words.
We use WordNet synonyms and antonyms to expand two lists of positive and negative seed words.

56
Sentence Compression Beyond Word Deletion
In this paper we generalise the sentence compression task.
Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.
We present a new corpus that is suited to our task and a discriminative tree-to-tree transduction model that can naturally account for structural and lexical mismatches.
The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.
Different from prior research, we achieve sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.
We present a model that can both compress and paraphrase individual sentences without however generating document-level summaries.
Our abstractive methods sheds more light on how people compress sentences, but do not always manage to outperform extractive methods.
We expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts.
We propose the first abstractive compression method.

57
A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English
In this paper, we present an approach to the automatic identification and correction of preposition and determiner errors in non-native (L2) English writing.
We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
In the context of automated preposition and determiner error correction in L2 English, we note that the process is often disrupted by misspellings.

58
Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging
We present a HMM part-of-speech tagging method which is particularly suited
for POS tagsets with a large number of fine-grained tags.
It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.
In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers.
Our fine-grained tag set contains approximately 800 tags.

59
Learning Entailment Rules for Unary Templates
Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules between templates with a single variable.
In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method.
The results show that the learned unary rule-sets outperform the binary rule-set.
In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
We propose a unary template, which is defined as a template consisting of one argument slot and one predicate phrase.
We use the distributional similarity of arguments to detect unary template entailment.
Two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources.
We try identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates.

60
The Ups and Downs of Preposition Error Detection in ESL Writing
In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers.
Our system performs at 84% precision and close to 19% recall on a large set of student essays.
In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation.
We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems.
We use the TOEFL data.
We show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions.
Our model is trained with lexical features.

61
A Uniform Approach to Analogies Synonyms Antonyms and Associations
Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms.
In the past, the four tasks have been treated independently, using a wide variety of algorithms.
These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach.
We propose to subsume a broad range of phenomena under analogies.
To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.
We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology.
We propose a simpler SVM-based algorithm for analogical classification called PairClass.
We argue that many NLP tasks can be formulated in terms of analogical reasoning, and we apply our PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words.
We advocate the need for a uniform approach to corpus-based semantic tasks.

62
Top Accuracy and Fast Dependency Parsing is not a Contradiction
In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long.
To determine why, we analyzed the time usage of a dependency parser.
We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity.
To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel.
The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training.
This has lead to a higher accuracy.
We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm.
We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation.
We show that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features.
The Mateparser is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren.

63
A Monolingual Tree-based Translation Model for Sentence Simplification
In this paper, we consider sentence simplification as a special form of translation with the complex sentence as the source and the simple sentence as the target.
We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally.
We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.
The evaluation shows that our model achieves better readability scores than a set of baseline systems.
We use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set.
We examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task.
We propose sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning.

64
Robust Sentiment Detection on Twitter from Biased and Noisy Data
In this paper, we propose an approach to automatically detect sentiments on Twitter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data.
These noisy labels were provided by a few sentiment detection websites over twitter data.
In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources.
We propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features.

65
Enhanced Sentiment Learning Using Twitter Hashtags and Smileys
Automated identification of diverse sentiment types can be beneficial for many NLP systems such as review summarization and public media analysis.
In some of these systems there is an option of assigning a sentiment value to a single sentence or a very short text.
In this paper we propose a supervised sentiment classification framework which is based on data from Twitter, a popular microblogging service.
By utilizing 50 Twitter tags and 15 smileys as sentiment labels, this framework avoids the need for labor intensive manual annotation, allowing identification and classification of diverse sentiment types of short texts.
We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences.
The quality of the sentiment identification was also confirmed by human judges.
We also explore dependencies and overlap between different sentiment types represented by smileys and Twitter hashtags.
We used 50 hash tags and 15 emoticons as noisy labels to create a dataset for twitter sentiment classification.

66
D-PATR: A Development Environment For Unification-Based Grammars
We describe systems in which FSs may be modified by default statements in such a way that this property does not automatically hold.

67
Categorial Unification Grammars
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research.
In this paper, the basic concepts of CUGs and simple examples of their application will be presented.
It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars.
Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.

68
A Statistical Approach To Language Translation
An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases.
The method is based on the availability of pairs of large corresponding texts that are translations of each other.
In our case, the texts are in English and French.
Fundamental to the technique is a complex glossary of correspondence of fixed locutions.
The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions.
(2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence.
(3) Arrange the words of the target fixed locutions into a sequence forming the target sentence.
We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.
While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences.
We develop the so-called IBM models, implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence.
In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder. The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output.

69
Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars
In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).
In our approach elementary structures are associated with their lexical heads.
These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated.
These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.
We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced.
We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure.
We show how adjunction allows us to 'lexicalize' a CFG freely.
We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.
A novel general parsing strategy for 'lexicalized' grammars is discussed.
In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set.
The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism.
However, we focus our attention on TAGs.
Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy.
Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated.
The parser is also able to use non-local information to guide the search.
We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach.
Lexicalized grammars offer significant parsing benefits as the number of applications of productions (i.e., derivation steps) is clearly bounded by the length of the input string.

70
A Uniform Architecture For Parsing And Generation
The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted.
In this paper, we discuss a more radical possibility: not only can a single grammar be used by different processes engaged in various "directions" of processing, but one and the same language-processing architecture can be used for processing the grammar in the various modes.
In particular, parsing and generation can be viewed as two processes engaged in by a single parameterized theorem prover for the logical interpretation of the formalism.
We discuss our current implementation of such an architecture, which is parameterized in such a way that it can be used for either purpose with grammars written in the PATR formalism.
Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena.
This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes.
We state that to guarantee completeness in using a precomputed entry in the chart, the entry must subsume the formula being generated top-down.

71
Feature Structures Based Tree Adjoining Grammars
We have embedded Tree Adjoining Grammars (TAG) in a feature structure based unification system.
The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG's.
We show that FTAG has an enhanced descriptive capacity compared to TAG formalism.
We consider some restricted versions of this system and some possible linguistic stipulations that can be made.
We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986] involving the logical formulation of feature structures.
A Feature-based TAG consists of a set of (auxiliary or initial) elementary trees and of two tree-composition operations: substitution and adjunction.

72
Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries
In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation.
Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.
The automatic construction of VLNNs enables real-size experiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.
We apply conventional spreading activation approaches to word sense disambiguation.

73
Constraint Grammar As A Framework For Parsing Running Text
Grammars which are used in parsers are often directly imported from autonomous grammar theory and descriptive practice that were not exercised for the explicit purpose of parsing.
Parsers have been designed for English based on e.g. Government and Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar.
We present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity.
The formalism is a linguistic one.
It relies on transitional probabilities in an indirect way.
The probabilities are not part of the description.
The descriptive statements, constraints, do not have the ordinary task of defining the notion 'correct sentence in L'.
They are less categorical in nature, more closely tied to morphological features, and more directly geared towards the basic task of parsing.
We see this task as one of inferring surface structure from a stream of concrete tokens in a basically bottom-up mode.
Constraints are formulated on the basis of extensive corpus studies.
They may reflect absolute, rule-like facts, or probabilistic tendencies where a certain risk is judged to be proper to take.
Constraints of the former rule-like type are of course preferable.
The ensemble of constraints for language L constitute a Constraint Grammar (CG) for L.
A CG is intended to be used by the Constraint Grammar Parser CGP, implemented as a Lisp interpreter.
Our input tokens to CGP are morphologically analyzed word-forms.
One central idea is to maximize the use of morphological information for parsing purposes.
All relevant structure is assigned directly via lexicon, morphology, and simple mappings from morphology to syntax.
The task of the constraints is basically to discard as many alternatives as possible, the optimum being a fully disambiguated sentence with one syntactic reading only.
The second central idea is to treat morphological disambiguation and syntactic labelling by the same mechanism of discarding improper alternatives.
A good parsing formalism should satisfy many requirements: the constraints should be declarative rather than procedural, they should be able to cope with any real-world text-sentence (i.e. with running text, not just with linguists' laboratory sentences), they should be clearly separated from the program code by which they are executed, the formalism should be language-independent, it should be reasonably easy to implement (optimally as finite-state automata), and it should also be efficient to run.
The CG formalism adheres to these desiderata.
we propose the Constraint Grammar framework.

74
Toward Memory-Based Translation
An essential problem of example-based translation is how to utilize more than one translation example for translating one source sentence.
This 1)aper proposes a method to solve this problem.
We introduce the representation, called matching expression, which represents the combination of fragments of translation examples.
The translation process consists of three steps: (1) Make the source matching expression from the source sentence. (2) Transfer the source matching expression into the target matching expression. (3) Construct the target sentence from the target matching expression.
This mechanism generates some candidates of translation.
To select, the best translation out of them, we define the score of a translation.
We combine a measure of structural similarity with a measure of word distance in order to obtain the overall distance measure that is used for matching.

75
Synchronous Tree-Adjoining Grammars
The unique properties of Tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language.
We present a variant of TAGs, called synchronous TAGs, which characterize correspondences between languages.
The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language, or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper.
We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation.
Synchronous Tree Adjoining Grammars is introduced primarily for semantics but will be later also proposed for translation.
A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars.

76
Typed Unification Grammars
We introduce TFS, a computer formalism in the class of logic formalisms which integrates a powerful type system.
Its basic data structures are typed feature structures.
The type system encourages an object-oriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels of linguistic description defined as classes of objects.
We illustrate this approach starting from a very simple DCG, and show how to make use of the typing system to enforce general constraints and modularize linguistic descriptions, and how further abstraction leads to a HPSG-like grammar.
The proposed approach inevitably leads to the consequence that the data structure becomes slightly complicated.

77
Automatic Processing Of Large Corpora For The Resolution Of Anaphora References
Manual acquisition of semantic constraints in broad domains is very expensive.
This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus.
To a large extent, these statistics reflect, semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities.
The scheme was implemented by gathering statistics on the output of other linguistic tools.
An experiment was performed to resolve references of the pronoun "it" in sentences that were randomly selected from the corpus.
Title results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool.
We use the distribution of a pronoun's context to determine which candidate antecedents can fit the context.
We present one of the earliest methods for using predicate-argument frequencies in pronoun resolution.

78
Word Identification For Mandarin Chinese Sentences
Chinese sentences are composed with string of characters without blanks to mark words.
However the basic unit for sentence parsing and understanding is word.
Therefore the first step of processing Chinese sentences is to identify the words.
The difficulties of identifying words include (l) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmentations.
In this paper, we propose the possible solutions for the above difficulties.
We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve a 99.77% of the success rate.
The statistical data supports that the maximal matching algorithm is the most effective heuristics.
we propose the forward maximum matching algorithm.

79
Two-Level Morphology With Composition
We recognize that a cascade of composed FSTs could implement the two-level model.
We observe that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself.

80
A Fast Algorithm For The Generation Of Referring Expressions
We simplify previous work in the development of algorithms for the generation of referring expressions while at the same time taking account of psycholinguistic findings and transcript data.
The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent.
We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system.
We apply generalizations about the salience of properties of objects and conventions about what words make base level attributions to incrementally select words for inclusion in a description.

81
Stochastic Lexicalized Tree-Adjoining Grammars
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word.
The characteristics of SLTAG are unique and novel since it is lexically sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).
Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside- like iterative algorithm for estimating the parameters of a SLTAG given a training corpus.
Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars.
In stochastic tree-adjoining grammar, this lack of context-sensitivity is overcome by assigning probabilities to larger structural units.

82
Word-Sense Disambiguation Using Statistical Models Of Roget's Categories Trained On Large Corpora
This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories.
Roget's categories serve as approximations of conceptual classes.
The categories listed for a word in Roger's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation.
The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.
Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon.
Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention.
Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.
We rely on the intuition that the senses of words are hinted at by their contextual information. From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense.

83
Automatic Acquisition Of Hyponyms From Large Text Corpora
We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text.
Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text.
We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest.
We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way.
A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built thesaurus.
Extensions and applications to areas such as information retrieval are suggested.
We find individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.

84
A Computational Model Of Language Performance: Data Oriented Parsing
Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing.
Analyzing a new input means that the system attempts to find find the most probable way to reconstruct the input out of fragments that already exist in the corpus.
Disambiguation occurs as a side-effect.
DOP can be implemented by using conventional parsing strategies.
In this work, super strong equivalence relations between other stochastic grammars are studied.
We show that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1.

85
Surface Grammatical Analysis For The Extraction Of Terminological Noun Phrases
LEXTER is a software package for extracting terminology.
A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated.
To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing.
In the first stage, LEXTER uses a base of rules designed to identify frontier markers in view to analysing the texts and extracting maximal-length noun phrases.
In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units.
In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach).
We present a surface-syntactic analyser that extracts maximal length noun phrases mainly sequences of determiners, premodifiers, nominal heads, and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications.
Our method relies purely on linguistic information, namely morpho-syntactic features of term candidates.

86
Part-Of-Speech Tagging With Neural Networks
Text corpora which are tagged with part-of-speech in- formation are useful in many areas of linguistic research.
In this paper, a new part-of-speech tagging method based on neural networks (Net-Tagger) is presented and its performance is compared to that of a HMM-tagger (Cutting et al. , 1992) and a trigram-based tagger (Kempe, 1993).
It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the HMM-tagger.
The correct rate of tagging has reached 95%, in part by using a very large amount of training data.

87
A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm
We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words.
It consists of a statistical language model and an efficient two-pass N-best search algorithm.
The algorithm does not require delimiters between words.
Thus it is suitable for written Japanese.
The proposed Japanese morphological analyzer achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus.
We propose a method to search for the N best sets.

88
Comlex Syntax: Building A Computational Lexicon
We describe the design of Complex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.
We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.
Our COMLEX syntax dictionary provides verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly.

89
PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser
We present an efficient, broad-coverage, principle-based parser for English.
The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows.
It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries.
We release MiniPar, a fast and robust parser for grammatical dependency relations.

90
Recognizing Text Genres With Simple Metrics Using Discriminant Analysis
A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus.
Discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres.
An application to information retrieval is discussed.
We word length as an indicator of formality for applications such as genre classification.

91
K-Vec: A New Approach For Aligning Parallel Texts
Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards).
Some of these methods generate a bilingual lexicon as a by-product.
We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon.
For example, it discovers that the English word fisheries is similar to the French peches by noting that the distribution of fisheries in the English text is similar to the distribution of peches in the French.
K-vec does not depend on sentence boundaries.

92
A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation
In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem.
We train a transformation-based learning algorithm on 12,766 quadruples from WSJ.
We use the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples.

93
Word Sense Disambiguation Using Conceptual Density
This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus.
The method relies on the use oil' the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose.
This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process.
The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.
Our Conceptual Density (CD) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy.
We use a conceptual distance formula that was created to by sensitive to the length of the shortest path that connects the concepts involved, the depth of the hierarchy and the density of concepts in the hierarchy.
To overcome the problem of varying link distances, we propose a semantic similarity measure (referred to conceptual density) which is sensitive to i) the length of the path, ii) the depth of the nodes in the hierarchy (deeper nodes are ranked closer) and iii) the density of nodes in the sub hierarchies (concepts involved in a denser sub hierarchy are ranked closer than those in a more sparse region).

94
Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser
We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994).
In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text.
Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the in-put text stream.
Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not -- cannot -- employ robust and reliable parsing components.
We also suggest that anaphora resolution is part of the discourse referents resolution.

95
Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues
This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.
We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy.
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.
Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources.
We show that if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly.

96
Three New Probabilistic Models For Dependency Parsing: An Exploration
After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it.
We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (e) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer.
We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank).
In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.
The proposed parsing algorithm is sufficient for searching over all projective trees in O (n3) time.

97
Message Understanding Conference-6: A Brief History
We have recently completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction.
MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted.
We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.
We demostrate that grammar-based IE systems can be effective in many scenarios.
We introduce name recognition and classification tasks.

98
HMM-Based Word Alignment In Statistical Translation
In this paper, we describe a new model for word alignment in statistical translation and present experimental results.
The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.
To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem.
The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings.
We describe the details of the model and test the model on several bilingual corpora.
We use a useful feature that assesses the goodness of the alignment path through the source sentence.

99
Motivations And Methods For Text Simplification
Long and complicated sentences prove to be a stumbling block for current systems relying on NL input.
These systems stand to gain from methods that syntactically simplify such sentences.
To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out.
Obviously a parser could be used to obtain the complete structure of the sentence.
However, full parsing is slow prone to failure, especially on complex sentences.
In this paper, we consider two alternatives to full parsing which could be used for simplification.
The first approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages.
We discuss the impact of these two input representations on the sim-plification process.
We introduce a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence.
Our text simplification techniques deal not only with helping readers with reading disabilities, but also to help NLP systems as a preprocessing tool.

100
Using Semantic Roles to Improve Question Answering
Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention.
Our work examines whether semantic role information is beneficial to question answering.
We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm.
We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching.
Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models.
We show that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question.
We also point out that the low coverage of the current version of FrameNet significantly limits the expected boost in performance.

101
What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions.
Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations.
We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.
Our model learns soft alignments as a hidden variable in discriminative training.
Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
We explore the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching.
We use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer.

102
Improving Statistical Machine Translation Using Word Sense Disambiguation
We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task - and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics.
Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT.
Yet SMT translation quality still obviously suffers from inaccurate lexical choice.
In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation.
Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems.
Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary.
We provide a machine translation system with the WSD probabilities for a phrase translation as extra features in a log-linear model.
We use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases.
We use a state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence.

103
Why Doesn't EM Find Good HMM POS-Taggers?
This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers.
We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed.
This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM.
We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced.
We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought.
we demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model.

104
V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure
We present V-measure, an external entropy-based cluster evaluation measure.
V-measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the problem of matching, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness.
We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results.
Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.
F score is not suitable for comparing results with different cluster numbers.
The V-measure (VM) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster).
A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class.

105
Lexical Semantic Relatedness with Random Graph Walks
Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness.
Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph.
By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph.
Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics.
We treat the graph as a Markov chain and compute a word-specific stationary distribution via a generalized PageRank algorithm.
Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions.
In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ? =.90.
we use random walks over WordNet, incorporating information such as meronymy and dictionary glosses.

106
Online Learning of Relaxed CCG Grammars for Parsing to Logical Form
We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).
A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar - for example allowing flexible word order, or insertion of lexical items - with learned costs.
We also present a new, online algorithm for inducing a weighted CCG.
Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).
We develop ATIS dataset for semantic parsing.
We develop a set which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed.
We introduce the standard application, composition and coordination combinators, as well as type-shifting rules to model spontaneous, unedited text.

107
The Infinite PCFG Using Hierarchical Dirichlet Processes
We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP).
Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available.
In addition to presenting a fully Bayesian model for the PCFG, we also develop an efficient variational inference procedure.
On synthetic data, we recover the correct grammar without having to specify its complexity in advance.
We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars.
We find that because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting.

108
Large-Scale Named Entity Disambiguation Based on Wikipedia Data
This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results.
It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia.
Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.
We find that topical coherence between a candidate entity and other entities in the context will improve NED accuracy.
We employ context vectors consisting of phrases and categories extracted from Wikipedia.
We introduce an entity disambiguation data set.
We make use of the explicit category information found within Wikipedia.
For evaluation, we use 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia.

109
Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information
This paper proposes a tree kernel with context-sensitive structured parse tree information for relation extraction.
It resolves two critical problems in previous tree kernels for relation extraction in two ways.
First, it automatically determines a dynamic context-sensitive tree span for relation extraction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.
Second, it proposes a context-sensitive convolution tree kernel, which enumerates both context-free and context-sensitive sub-trees by considering their ancestor node paths as their contexts.
Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.
Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy's convolution tree kernel.
It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels.
Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
Our composite kernel depends partially on a full parse, and partially on a collection of shallow syntactic features.

110
Chinese Syntactic Reordering for Statistical Machine Translation
Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems.
This paper introduces a reordering approach for translation from Chinese to English.
We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order.
The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order.
We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007).
The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data.
We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules.
Our rule set substantially decreases the total times of rule application about 60%, compared with a constituent-based approach.
Chinese ordering differs from English mainly in clause ordering.

111
Online Large-Margin Training for Statistical Machine Translation
We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm.
The millions of parameters were tuned only on a small development set consisting of less than 1K sentences.
Experiments on Arabic-to-English translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features.
We perform BLUE computations in the context of a set O of previously-translated sentences.
We find the possibility of overfitting in the dataset (Arabic-English newswire translation), especially when domain differences are present.

112
Large Language Models in Machine Translation
This paper reports on the benefits of large-scale statistical language modeling in machine translation.
A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams.
It is capable of providing smoothed probabilities for fast, single-pass decoding.
We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.
5-gram word language models in English are trained on a variety of monolingual corpora.
In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment.
To scale LMs to larger corpora with higher-order dependencies we consider distributed language models that scale more readily.
Stupid back off smoothing is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney.
We show that each doubling of the training data from the news domain (used to build the language model) leads to improvements of approximately 0.5 BLEU points.
We used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.

113
Factored Translation Models
We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level - may it be linguistic markup or automatically generated word classes.
In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data.
We also propose frameworks for the simultaneous use of different word-level representations.
We propose a tight integration of morpho syntactic information into the translation model, where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation.
We generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search.
Factored translation models facilitate a more data-oriented approach to agreement modeling.

114
The CoNLL 2007 Shared Task on Dependency Parsing
The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets.
In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track.
In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages.
In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.
We note that languages with free word order and high morphological complexity are the most difficult for dependency parsing.
Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish.

115
Single Malt or Blended? A Study in Multilingual Parser Optimization
We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing.
The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm.
The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language.
When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.
We extend the two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.
We point out that the official results for Chinese contained a bug, and the true performance of our system is actually much higher.
We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible.

116
Experiments with a Higher-Order Projective Dependency Parser
We present experiments with a dependency parsing model defined on rich factors.
Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children.
We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron.
Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption.
In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
We extend the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model.
Our second-order models include head grandparent relations.
Our second order algorithm uses the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild.
We introduce the left-most and right-most grandchild as factors.

117
Improving Translation Quality by Discarding Most of the Phrasetable
It is possible to reduce the bulk of phrase-tables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus.
The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score.
In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrase table smoothing is employed.
We use Fisher's exact test.
We filter out statistically unreliable translation pairs.

118
Hierarchical Phrase-Based Translation with Suffix Arrays
A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets.
In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly.
Hierarchical phrase-based translation introduces the added wrinkle of source phrases with gaps.
Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence.
We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.
The basis of our method is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings.

119
A Topic Model for Word Sense Disambiguation
We develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.
We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word.
Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts.
We use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation.
We describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned.
We integrate semantics into the topic model framework.

120
Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles
We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing.
Parser actions are determined by a classifier, based on features that represent the current state of the parser.
We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners.
In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme.
In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of-domain training set, in a scheme similar to one iteration of co-training.
We use a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis.
We generalize the standard deterministic framework to probabilistic parsing by using a best-first search strategy.

121
Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining
The technology of opinion extraction allows users to retrieve and analyze people's opinions scattered over Web documents.
We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment.
We use this definition as the basis for our opinion extraction task.
We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues.
Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks.
We analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains.
We adopt a supervised learning technique to search for useful syntactic patterns as contextual clues.

122
Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems
This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.
An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.
Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.
The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.
Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.
we propose using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.

123
Multilingual Subjectivity Analysis Using Machine Translation
Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language.
In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages.
Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages.
Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.
We demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment.
We hypothesize that subjectivity is expressed differently in various languages due to lexicalization, formal versus informal markers, etc.

124
Dependency Parsing by Belief Propagation
We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference.
As a parsing algorithm, BP is both asymptotically and empirically efficient.
Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor.
Furthermore, such features significantly improve parse accuracy over exact first-order methods.
Incorporating additional features would increase the runtime additively rather than multiplicatively.
We can encapsulate common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations.
DEP-TREE is a global combinatorial factor which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.

125
Revisiting Readability: A Unified Framework for Predicting Text Quality
We combine lexical, syntactic, and discourse features to produce a highly predictive model
of human readers' judgments of text readability.
This is the first study to take into account such a variety of linguistic factors and
the first to empirically demonstrate that discourse relations are strongly associated with
the perceived quality of text.
We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.
We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.
Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
We propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features.
Five annotators have assessed the overall text quality of each article on a scale from 1 to 5.
We find non-significant correlation for the mean number of words per sentence and the mean number of characters per word.

126
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
We improve the quality of paraphrases extracted from parallel corpora by requiring that
phrases and their paraphrases be the same syntactic type.
This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs.
In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced.
A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.
We show how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases.
Human evaluators are asked to score each pair of an original sentence and a paraphrased sentence with the following two 5-point scale grades: Grammaticality: whether the paraphrased sentence is grammatical, Meaning: whether the meaning of the original sentence is properly retained by the paraphrased sentence.
A problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are superior sub-strings of the original term.
We automatically acquire paraphrase dictionary.

127
Forest-based Translation Rule Extraction
Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bitext.
The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.
So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30-best parses.
When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
We use viterbi algorithm to prune the forest.

128
Online Large-Margin Training of Syntactic and Structural Translation Features
Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.
Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.
We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model.
In both cases we obtain significant improvements in translation performance.
Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 BLUE on a subset of the NIST 2006 Arabic-English evaluation data.
We introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length.
We show that MERT is competitive with small numbers of features compared to high-dimensional optimizers such as MIRA.
Our feature explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.

129
Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks
Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming.
We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web.
We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.
For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers.
For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.
We propose a technique for bias correction that significantly improves annotation quality on two tasks.
We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.
We compare the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and find that they required only four responses per item to emulate expert annotations.
We show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels.
We work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7.

130
Understanding the Value of Features for Coreference Resolution
In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques.
These works often show that complex models improve over a weak pairwise baseline.
However, less attention has been given to the importance of selecting strong features to support learning a coreference model.
This paper describes a rather simple pair-wise classification model for coreference resolution, developed with a well-designed set of features.
We show that this produces a state-of-the-art system that outperforms systems built with complex models.
We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used.
The paper also presents an ablation study and discusses the relative contributions of various features.
Our algorithm runs in time quadratic in the number of mentions.

131
Bayesian Unsupervised Topic Segmentation
This paper describes a novel Bayesian approach to unsupervised topic segmentation.
Unsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution.
We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation.
This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.
The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.
Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.
We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework.
We present a dynamic program for linear segmentation.
If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border, is sufficient.
We find the richer model beneficial for a meetings corpus but not for a textbook.

132
A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers
There is growing interest in applying Bayesian techniques to NLP problems.
There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.
This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.
We investigate a variety of samplers for HMMs, including some that these earlier papers did not study.
We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.
In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.
We consider three evaluation criteria: M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion.
We induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.
We show that sparse priors can gain 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.

133
A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing
Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations.
We study both approaches under the framework of beam-search.
By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.
More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers.
Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.
We define head rules to convert phrase structures into dependency structures.
We combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explore the addition of graph based features to a transition-based parser.

134
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses.
We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.
We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions.
The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks.
We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.
We consider Taylor approximations to the logarithm of BLEU.
We extend MBR to word lattices, which improves performance over k-best list MBR.
The log-BLEU function must be modified slightly to yield a linear Taylor approximation: we replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function.
We compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t.

135
Joint Unsupervised Coreference Resolution with Markov Logic
Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.
Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate.
In this paper, we present the first unsupervised approach that is competitive with supervised ones.
This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals.
On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.
We empirically report that global approaches achieve performance better than the ones based on incrementally processing a text.
Our method is based on the entity-mention model.
In the predicate nominative construction, the object of a copular verb (forms of the verb be) is constrained to corefer with its subject.

136
Lattice-based Minimum Error Rate Training for Statistical Machine Translation
Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.
The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.
Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder.
In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice.
Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.
Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.
We find that first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.
We present a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs.
We apply the SweepLine algorithm to the union to discard redundant linear functions and their associated hypotheses.
We theorize that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice.
In our MERT algorithm we compute the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space.
We extend the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it.
We find that the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10.

137
A Generative Model for Parsing Natural Language to Meaning Representations
In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures.
The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning.
We introduce dynamic programming techniques for efficient training and decoding.
In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora.
The generative model degrades robustly when presented with instances that are different from those seen in training.
This allows a notable improvement in recall compared to previous models.
Our hybrid tree model use a tree transformation based approach.
We present a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form.
We propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two).

138
Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis
Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach.
In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity.
In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.
Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).
We also find that “content-word negators”, not widely employed in previous work, play an important role in determining expression-level polarity.
Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered.
Content-word negators are words that are not function words, but act semantically as negators.
We combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysiss.
We propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity.
We hand-code compositional rules in order to model compositional effects of combining different words in the phrase.
We categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate.

139
A Simple and Effective Hierarchical Phrase Reordering Model
While phrase-based statistical machine translation systems currently deliver state-of-the- art performance, they remain weak on word order changes.
Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems.
In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency.
We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase.
We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).
Our hierarchical orientation model captures non-local phrase reordering by a shift reduce algorithm.
We introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history.
We introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.

140
Two Languages are Better than One (for Syntactic Parsing)
We show that jointly parsing a bitext can substantially improve parse quality on both sides.
In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.
Features include monolingual parse scores and various measures of syntactic divergence.
Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F at predicting English side trees and 1.8 F at predicting Chinese side trees (the highest published numbers on these corpora).
Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.
In bitext parsing, we use feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy.
We use word alignment density features which measure how well the aligned entity pair matches up with alignments from an independent word aligner.

141
Unsupervised Semantic Parsing
We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic.
Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning.
The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them.
We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions.
USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.
We consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments.
We model joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures.
We group parameters and impose local normalization constraints within each group.

142
First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests
Many statistical translation models can be regarded as weighted logical deduction.
Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).
We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).
This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.
We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point.
We consider minimum risk training using a linearly decomposable approximation of BLEU.
The sufficient statistics for graph expected BLEU can be computed using expectation semirings.
We extend the work of Smith and Eisner and obtain much better estimates of feature expectations by using a packed chart instead of an n-best list.
We perform expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007).

143
Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora
A significant portion of the world's text is tagged by readers on social bookmarking websites.
Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document.
Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa.
This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags.
This allows Labeled LDA to directly learn word-tag correspondences.
We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us.
Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets.
As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.
L-LDA extends standard LDA to include supervision for specific target categories, and the generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.

144
Fast Cheap and Creative: Evaluating Translation Quality Using Amazon&rsquo;s Mechanical Turk
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.
We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators.
For $10 we redundantly recreate judgments from a WMT08 translation task.
We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.
We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
We find that lazy annotators tended to stay longer and do more annotations.
We treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators.
We show the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks.

145
An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing
This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach.
We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008).
Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008).
The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach.
We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech.
Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech.
We present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.

146
Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another.
We propose quasi-synchronous grammar (QG) features for these structured learning tasks.
That is, we score an aligned pair of source and target trees based on local features of the trees and the alignment.
Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to asynchronous grammar, which would insist on some form of structural parallelism.
In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence.
On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.
Our experiments show that unsupervised QG projection improves on parses trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.
When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees.
We think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.

147
Polylingual Topic Models
Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.
Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.
We introduce a polylingual topic model that discovers topics aligned across multiple languages.
We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.
We retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations.
We show that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) does not degrade significantly.
We extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles).
Polylingual topic models learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.

148
Web-Scale Distributional Similarity and Entity Set Expansion
Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.
Parallelization and optimizations are necessary.
We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
The pairwise similarity between 500 million terms is computed in 50 hours using 200 quadcore nodes.
We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.
We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
Our DASH stores the case for each phrase in Wikipedia.
We find that 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts of all occurrences of all the seeds in the corpus.

149
Supervised Models for Coreference Resolution
Traditional learning-based coreference resolvers operate by training a mention-pair classifier for determining whether two mentions are coreferent or not.
Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mention-ranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention.
We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entity-mention models.
We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution.
Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.
In each query we include a null-cluster instance, to allow joint learning of discourse-new detection.
We show that the CR model is stronger than the MP model.
Our cluster ranking model proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.

150
Simple Coreference Resolution with Rich Syntactic and Semantic Features
Coreference systems are driven by syntactic, semantic, and discourse constraints.
We present a simple approach which completely modularizes these three aspects.
In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.
Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.
Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
We show that coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility.
In our SYN-CONSTR setting, each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions).
When searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree.

151
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Jointly parsing two languages has been shown to improve accuracies on either or both sides.
However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.
Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well.
We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.
Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (∼6%) efficiency overhead, thus much faster than biparsing.
We keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1.
We improve English prepositional phrase attachment using features from an unparsed Chinese sentence.

152
Phrase Dependency Parsing for Opinion Mining
In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them.
By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level.
This concept is then implemented for extracting relations between product features and expressions of opinions.
Experimental evaluations show that the mining task can benefit from phrase dependency parsing.
We utilize the dependency parser to extract the noun phrases and verb phrases from the reviews as the aspect candidates.
For a monolingual task, we use a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies.

153
On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing
This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems.
The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles.
The approach provably solves a linear programming (LP) relaxation of the global inference problem.
It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation.
We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.
We use the highest scoring output of the parsing submodel over all iterations.

154
Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation
We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.
This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.
We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.
We rank the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models.
We apply linear interpolation to combine the instance weighted out-of-domain model with an in-domain model.
We propose a method for machine translation that uses features to capture degrees of generality.

155
A Multi-Pass Sieve for Coreference Resolution
Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features.
This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones.
To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision.
Each tier builds on the previous tier’s entity cluster output.
Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.
This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time.
The framework is highly modular: new coreference modules can be plugged in without any change to the other modules.
In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora.
This suggests that sieve-based approaches could be applied to other NLP tasks.
Our rule based model obtains competitive result with less time.
The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity.
We develop accurate unsupervised systems that exploit simple but robust linguistic principles.

156
Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space
We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.
Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.
A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.
We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
We find that the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out).
The adjective-specific linear map (alm) model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density.

157
Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification
This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.
Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.
The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.
We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.
Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
We present an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method.
We initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.

158
Using Universal Linguistic Knowledge to Guide Grammar Induction
We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages.
During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.
We also automatically refine the syntactic categories given in our coarsely tagged input.
Across six languages our approach outperforms state-of-the-art unsupervised methods by a significant margin.
Our system is  weakly supervised, in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model.

159
A Latent Variable Model for Geographic Lexical Variation
The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation.
In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions.
High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency.
The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models.
We gathered the text and geographical locations of 9,250 microbloggers on the website twitter.com to construct a dataset.
We collected about 380,000 tweets from Twitter's official API.
We predict locations based on Gaussian distributions over the earth's surface as part of a hierarchical Bayesian model.
We consider all tweets of a user concatenated as a single document, and use the earliest collected GPS-assigned location as the gold location.

160
Dual Decomposition for Parsing with Non-Projective Head Automata
This paper introduces algorithms for non-projective parsing based on dual decomposition.
We focus on parsing algorithms for non-projective head automata, a generalization of head-automata models to non-projective structures.
The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.
They provably solve an LP relaxation of the non-projective parsing problem.
Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.
The accuracy of our models is higher than previous work on a broad range of datasets.
We consider third-order features such as grand-siblings and tri-siblings.

161
Multi-Source Transfer of Delexicalized Dependency Parsers
We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.
Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.
The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
We show that part-of-speech tags contain significant amounts of information for unlabeled dependency parsing.
We demonstrate an alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor.
Tree banks in other languages can still serve as a kind of proxy for learning which features generally transfer useful in formation.
We demonstrate that projecting from a single oracle chosen language can lead to good parsing performance.

162
Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions
We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.
Our method learns vector space representations for multi-word phrases.
In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.
We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project.
The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.
Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.
we introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.

163
Domain Adaptation via Pseudo In-Domain Data Selection
We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain.
These sentences may be selected with simple cross-entropy based methods, of which we present three.
As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora.
These subcorpora - 1% the size of the original - can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.
Performance is further improved when we use these domain-adapted models in combination with a true in-domain model.
The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.
We improve the perplexity based approach and propose bilingual cross entropy difference as a ranking function with in- and general-domain language models.
We propose a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperform monolingual cross-entropy difference.

164
Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora
We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing.
Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets.
In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality.
We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators.
The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.

165
Tuning as Ranking
We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).
Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.
Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement.
It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours.
We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.
PRO casts the problem of tuning as a ranking problem between pairs of translation candidates.
We optimize ranking in n-best lists, but learn parameters in an online fashion.
We minimize logistic loss sampled from the merged n-bests, and sentence-BLEU is used for determining ranks.

166
Experimental Support for a Categorical Compositional Distributional Model of Meaning
Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.
We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.
The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.
The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
Our model matches the results of its competitors in the first experiment, and betters them in the second.
The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
We suggest that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation.

167
Named Entity Recognition in Tweets: An Experimental Study
People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.
The performance of standard NLP tools is severely degraded on tweets.
This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition.
Our novel T-NER system doubles F1 score compared with the Stanford NER system.
T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.
LabeledLDA outperforms co-training, increasing F1 by 25% over ten common entity types.
Our NLP tools are available at: http:// github.com/aritter/twitter_nlp
We use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.
Our system exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities.

168
Identifying Relations for Open Information Extraction
Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary.
This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions.
To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs.
We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpos.
More than 30% of REVERB’s extractions are at precision 0.8 or higher compared to virtually none for earlier systems.
The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work.
We show that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations.
We develop a large scale web-based ReVerb corpus, comprising tuple extractions of predicate templates with their argument instantiations.
Our ReVerb corpus is a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions, automatically extracted from the ClueWeb09 web crawl.

169
A Comparison of Vector-based Representations for Semantic Composition
In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.
We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.
Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora.
We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.
The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.
We compute a weighted linear combination of the embeddings for words that appear in the document to be classified.
We compare count and predict representations as input to composition functions.
For paraphrase detection, we use cosine similarity between sentence pairs together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length.
Add and mult attained the top performance with the simple models for both figures of merit.

170
A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing
Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.
We present a transition-based system for joint part-of-speech tagging and labeled dependency parsing with non-projective trees.
Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages.
we introduce a transition-based system that jointly performed POS tagging and dependency parsing.

171
An Efficient Implementation Of A New DOP Model
Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.
This paper proposes an integration of the two models which outperforms each of them separately.
Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank.
Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.
We note that it is the highest ranking parse, not derivation, that is desired.
We show that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well.
We redress subtree probabilit by a simple correction factor.

172
Bootstrapping Statistical Parsers From Small Datasets
We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.
Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
We examine self-training for PCFG parsing in the small seed case (< 1k labeled data).
We report either minor improvements or significant damage from using self-training for parsing.
We find degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.

173
Combining Distributional And Morphological Information For Part Of Speech Induction
In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.
We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
We propose a perplexity based test for the quality of the POS induction algorithm.
We find that many-to-1 accuracy has several defects.

174
Investigating GIS And Smoothing For Maximum Entropy Taggers
This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.
We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary.
We also explore the use of a Gaussian prior and a simple cutoff for smoothing.
The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.
Our supertagger finds the single most probable category sequence given the sentenc and uses additional features defined in terms of the previously assigned categories.

175
Empirical Methods For Compound Splitting
Compounded words are a challenge for NLP applications such as machine translation (MT).
We introduce methods to learn splitting rules from monolingual and parallel corpora.
We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
We present a method requiring no linguistically motivated morphological analysis to split compounds.
We split German compound words, based on the frequency of the words in the potential decompositions.

176
Using Encyclopedic Knowledge For Named Entity Disambiguation
We present a new method for detecting and disambiguating named entities in open domain text.
A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia.
The resulting model significantly outperforms a less informed baseline.
We measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate.
We use context matching to link noun phrase subjects into Wikipedia.

177
Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment
This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
The outputs are combined and a possibly new translation hypothesis can be generated.
Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.
The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task.
The method was also tested in the framework of multi-source and speech translation.
On all tasks and conditions, we achieved significant improvements in translation quality, increasing e.g. the BLEU score by as much as 15% relative.
We align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments.
We use pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model.
Different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).
We propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination.

178
Online Learning Of Approximate Dependency Parsing Algorithms
In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.
We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
We propose a second-order graph-based dependency parsing model which incorporates features from the two kinds of subtrees.
We use the Viterbi decoding algorithm to achieve O (n3) parsing time.
We show that non-projective dependency parsing with horizontal Markovization is FNP-hard.
We define a second-order dependency parsing model in which interactions between adjacent siblings are allowed.

179
Making Tree Kernels Practical For Natural Language Learning
In recent years tree kernels have been proposed for the automatic learning of natural language applications.
Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
We introduce a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules.

180
Determining Term Subjectivity And Term Orientation For Opinion Mining
Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.
To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.
This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter.
We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case.
In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation.
We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection.
Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.

181
Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses
Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features.
We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP).
We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists.
The 58 runs were then collapsed into a single set of 7,813 unique words.
For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive.
We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.
We find that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets.
Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags.
WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds.

182
CDER: Efficient MT Evaluation Using Block Movements
Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.
In many cases though such movements still result in correct or almost correct sentences.
In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.
Our measure can be exactly calculated in quadratic time.
We consider edit distance for word substitution and reordering.
Our CDER measure is based on edit distance, such as the well-known WER, but allows reordering of blocks.

183
Re-Evaluation The Role Of Bleu In Machine Translation Research
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu's correlation with human judgments of quality.
This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
The problems of Blue include: (1) synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; (2) The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; (3) The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall.
Blue has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems.
We find that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.

184
Discriminative Sentence Compression With Soft Syntactic Evidence
We present a model for sentence compression that uses a discriminative large-margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
The parsers are trained out-of-domain and contain a significant amount of noise.
We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.
This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.
We provide a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.
We use the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.
We use semi-Markov model which allows incorporating a language model for the compression.

185
Comparing Automatic And Human Evaluation Of NLG Systems
We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems.
We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE.
We find that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.
We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available.
However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.
We use several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.
We demonstrate that automatic metrics can correlate highly with human ratings if the training dataset is of high quality.
The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain.

186
A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra-sentential context.
Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community.
For scoring, Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster).
We model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.

187
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions.
We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.
We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation.
To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness.
We come up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and use a corpus based statistical measure to determine the canonical form (s).

188
Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.
We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.
We performed experiments on extracting gene and protein interactions from two different data sets.
The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.
In addition to word features, we extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.

189
Personalizing PageRank for Word Sense Disambiguation
In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.
We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
We propose Personalized PageRank (PPR) that tries to trade-off between the amount of the employed lexical information and the overall efficiency.
We initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices).
We present a novel use of PageRank for word sense disambiguation.
The key idea is to adapt the matrix initialization step in order to exploit the available contextual evidence.

190
Bayesian Word Sense Induction
Sense induction seeks to automatically identify word senses directly from a corpus.
A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.
Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense.
Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task.
The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.
Our latent variable formulation serves as a foundation for more robust models of other linguistic phenomena.
We extract pseudo documents from a 10-word window centered on the corresponding word token for each word type.
We combine different feature sets using a probabilistic Word Sense Induction model and find that only some combinations produced an improved system.

191
Nonconcatenative Finite-State Morphology
Instead of modeling morphology along the more traditional finite-state transducer, we suggest modeling it with a n-tape automaton, where tapes would carry precisely this interleaving that is called for in Semitic interdigitation.
We propose a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form.

192
Inference In DATR
DATR is a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance.
The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout.
In this paper we present the syntax and inference mechanisms for the language.
The goal of the DATR enterprise is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics.
The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii).
we introduce DATR, a formal language for representing lexical knowledge.

193
Translation By Structural Correspondences
We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as formalized in the LFG notion of codescriptions.
The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis.
The architecture can provide a formal basis for specifying complex source-target translation relationships in a declarative fashion that builds on monolingual grammars and lexicons that are independently motivated and theoretically justified.

194
Named Entity Recognition Without Gazetteers
It is often claimed that Named Entity recognition systems need extensive gazetteers - lists of names of people, organisations, locations, and other named entities.
Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.
We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models.
We report on the system's performance with gazetteers of different types and different sizes, using test material from the MUC-7 competition.
We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names.
We conclude with observations about the domain independence of the competition and of our experiments.
We utilize the discourse level to disambiguate items in non predictive contexts.
We exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures.

195
An Efficient Method For Determining Bilingual Word Classes
In statistical natural language processing we always face the problem of sparse data.
One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.
We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm.
We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
We show improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.
We describe a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.

196
Representing Text Chunks
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.
(Ramshaw and Marcus, 1995) have introduced a "convenient" data representation for chunking by converting it to a tagging task.
In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.
We will show that the data representation choice has a minor influence on chunking performance.
However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.
We describe in detail the IOB schemes.

197
Inducing Multilingual Text Analysis Tools Via Robust Projection Across Aligned Corpora
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
Case studies include French, Chinese, Czech and Spanish.
Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments.
Simple direct annotation projection is quite noisy, however, even with optimal alignments.
Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections.
Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure.
The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system.
This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text.
Performance also significantly exceeds that obtained by direct annotation projection.
We perform early work in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora.

198
On Coreference Resolution Performance Metrics
The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution.
The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity.
We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm.
Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system.
The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.
We use a Bell tree to score and store the searching path.

199
A Discriminative Matching Approach To Word Alignment
We present a discriminative, large-margin approach to feature-based matching for word alignment.
In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on.
Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time.
Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.
We use a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link.
We use a one-to-one constraint, where words in either sentence can participate in at most one link.
We cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.

200
A Discriminative Framework For Bilingual Word Alignment
Bilingual word alignment forms the foundation of most approaches to statistical machine translation.
Current word alignment methods are predominantly based on generative models.
In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used.
These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data.
LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations.
We train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them.
We use statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations.

201
A Maximum Entropy Word Aligner For Arabic-English Machine Translation
This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data.
We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance.
The probabilistic model used in the alignment directly mod-els the link decisions.
Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests.
Performance of the algorithm is contrasted with human annotation performance.
We present a discriminatively trained 1-to-N model with feature functions specifically designed for Arabic.
We train a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperforms a GIZA++ baseline.

202
Local Phrase Reordering Models For Statistical Machine Translation
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences.
They are implemented by Weighted Finite State Transducers.
We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering.
Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks.
We also show that the procedure scales as the bitext size is increased.
We present a polynomial-time strategy.
We define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2.

203
Extracting Product Features And Opinions From Reviews
Consumers are often forced to wade through many on-line reviews in order to make an informed product choice.
This paper introduces OPINE, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products.
Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task.
OPINE's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.
Our dictionary-based method utilizes Wikipedia to find an entry page for a phrase or a single term in a query.
We not only analyze polarity of opinions regarding product features but also rank opinions based on their strength.
We present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues.
The relevance ranking and extraction was performed with Pointwise Mutual Information.

204
Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis
This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions.
With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.
We propose supervised learning, dividing the resources into prior polarity and context polarity.
Our experiments indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts.
We manually construct polarity lexicon, in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral).
Our MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters, which are used for identifying opinion roots, modifiers and negation words.

205
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns
Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength).
We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments.
We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a).
While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns.
Our results show that the combination of these two methods performs better than either one alone.
The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.

206
Domain-Specific Sense Distributions And Predominant Sense Acquisition
Distributions of the senses of words are often highly skewed.
This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough.
The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest.
In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words.
We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin.
We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus.
Our dataset is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE).

207
Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data
This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking.
The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time.
We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost.
Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines.
We propose easiest-first deterministic decoding.

208
Non-Projective Dependency Parsing Using Spanning Tree Algorithms
We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs.
Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time.
More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm.
We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.
The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.

209
Emotions From Text: Machine Learning For Text-Based Emotion Prediction
In addition to information, text contains attitudinal, and more specifically, emotional content.
This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture.
The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis.
Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a narrative baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning.
We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations.
In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.

210
Recognising Textual Entailment With Logical Inference
We use logical inference techniques for recognising textual entailment.
As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment.
Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE test set, given the state of the art.
Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature.
It is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs.
Our system is based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource.

211
A Shortest Path Dependency Kernel For Relation Extraction
We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph.
Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.
This work on relation extraction shows that the shortest dependency path between any two entities captures the in formation required to assert a relationship between them.

212
OpinionFinder: A System For Subjectivity Analysis
We provide a subjectivity lexicon.
We provide a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values.

213
Identifying Word Correspondences In Parallel Texts
Motivated by the need to reduce on the memory requirement and to insure robustness in estimation of probability, we propose an alternative algorithm in which probabilities are not estimated and stored for all word pairs.
We propose a statistic to measure the strength of correlation between source and target words.
We use the phi2 statistics as the correspondence level of the word pairs and show that it was more effective than the mutual information.

214
A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars
We define PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse.

215
Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing
We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse.
In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
We present history-based parsing, using features of the parsing history to predict the next parser action.

216
One Sense Per Discourse
It is well-known that there are polysemous words like sentence whose "meaning" or "sense" depends on the context of use.
We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia).
As this work was nearing completion, we observed a very strong discourse effect.
That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense.
This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%).
This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm.
In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint.
We claim on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.

217
Corpus-Based Statistical Sense Resolution
The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word co-occurrences.
The techniques were based on Bayesian decision theory, neural networks, and content vectors as used in information retrieval.
To understand these methods better, we posed a very specific problem: given a set of contexts, each containing the noun line in a known sense, construct a classifier that selects the correct sense of line for new contexts.
To see how the degree of polysemy affects performance, results from three- and six-sense tasks are compared.
The results demonstrate that each of the techniques is able to distinguish six senses of line with an accuracy greater than 70%.
Furthermore, the response patterns of the classifiers are, for the most part, statistically indistinguishable from one another.
Comparison of the two tasks suggests that the degree of difficulty involved in resolving individual senses is a greater performance factor than the degree of polysemy.
We construct 2094-word line dataset for word sense disambiguation.

218
One Sense Per Collocation
Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse.
In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation.
We test this empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities.
We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context.
We define collocation as a co-occurrence of two words in a defined relation.
In order to analyze and compare the behavior of several kinds of collocations, we use a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists.
We find that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.

219
A Semantic Concordance
A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon.
Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions.
A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation).
The Brown Corpus is the text and WordNet is the lexicon.
Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task.
Another interface supports searches of the tagged text.
Some practical uses for semantic concordances are proposed.
We present SemCor, a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.

220
The Penn Treebank: Annotating Predicate Argument Structure
The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure.
This paper discusses the implementation of crucial aspects of this new annotation scheme.
It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as "underlying" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.
Our Switchboard corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees.
The PennTreebank II marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role.

221
Using A Semantic Concordance For Sense Identification
This paper proposes benchmarks for systems of automatic sense identification.
A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic.
When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time.
When statistics for sense-frequancies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%.
And when a co-occurence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed.
The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic.
We prepare a sense-tagged corpus SEMCOR containing a substantial subset of the Brown corpus tagged with the refined senses of WORDNET.

222
A Maximum Entropy Model For Prepositional Phrase Attachment
We construct a benchmark dataset of 27,937 pp-attachment quadruples extracted from the Wall Street Journal corpus.
We train a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieve 81.6% accuracy.
Our maximum entropy approach uses the mutual information clustering algorithm.

223
Syntax Annotation for the GENIA Corpus
Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio-textmining.
As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand.
A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML-based format based on Penn Treebank II (PTB) scheme.
Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts.
Our GENIA Treebank Corpus is estimated to have no imperative sentences and only seven interrogative sentences.

224
The Second International Chinese Word Segmentation Bakeoff
The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation.
Twenty three groups submitted 130 result sets over two tracks and four different corpora.
We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still of paramount importance.
In the Second International Chinese Word Segmentation Bakeoff, two of the highest scoring systems in the closed track competition were based on a CRF model.

225
A Maximum Entropy Approach to Chinese Word Segmentation
We participated in the Second International Chinese Word Segmentation Bakeoff.
Specifically, we evaluated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU).
Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR.
We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy.
We present a post processing method to enhance the unknown word segmentation.
We use templates representing numbers, dates, letters etc.

226
A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005.
Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features.
Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin.
Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.
Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
We develop the Stanford Chinese word segmenter.

227
Using Contextual Speller Techniques and Language Modeling for ESL Error Correction
We present a modular system for detection and correction of errors made by non-native (English as a Second Language = ESL) writers.
We focus on two error types: the incorrect use of determiners and the choice of prepositions.
We use a decision-tree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions.
We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements.
We use a language model in addition to a classifier and combine the classifier output and language model scores in a meta classifier.
We use a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates.

228
Learning Dependency Translation Models As Collections Of Finite-State Head Transducers
The paper defines weighted head transducers, finite-state machines that perform middle-outstring transduction.
These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers.
Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically.
A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model.
A method for automatically training a dependency transduction model from a set of input-output example strings is presented.
The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments.
Experimental results are given for applying the training method to translation from English to Spanish and Japanese.
We treat translation as a process of simultaneous induction of source and target dependency trees using head transduction.
We present a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers.
We induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer.

229
Models Of Translational Equivalence Among Words
Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data.
First, most words translate to only one other word.
Second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text.
This article presents methods for biasing statistical translation models to reflect these properties.
Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model.
This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs.
Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks.
Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.
We measure the orthographic similarity using longest common subsequence ratio (LCSR).
We define a direct association as an association between two words where the two words are indeed mutual translations.
We propose Competitive Linking Algorithm (CLA) to align the words to construct confusion network.
We use competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association.
We argue that there are ways to determine the boundaries of some multi-words phrases, allowing to treat several words as a single token.

230
Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech
We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY.
Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence.
The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states.
Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.
The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act.
We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy.
Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech.
We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.
We use HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations.

231
A Compression-Based Algorithm For Chinese Word Segmentation
Chinese is written without using spaces or other word delimiters.
Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.
Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example,full-text search, word-based compression, and keyphrase extraction.
We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.
It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.
This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.
Our n-gram generative language modeling based approach does not use domain knowledge.

232
An Empirically Based System For Processing Definite Descriptions
We present an implemented system for processing definite descriptions in arbitrary domains.
The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora.
The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions.
A major obstacle in the resolution of definite noun phrases with full lexical heads is that only a small proportion of them is actually anaphoric (ca. 30%).
In our system, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora.
We classify each definite description as either direct anaphora, discourse-new, or bridging description.
We distinguish restrictive from non-restrictive post modification by ommitting modifiers that occur between commas, which should not be classified as chain starting.
For the discourse-new classification task, the model's most important feature is whether the head word of the NP to be classified has occurred previously.

233
On Coreferring: Coreference In MUC And Related Annotation Schemes
In this paper, it is argued that "coreference" annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper.
As a result, it is not always clear what semantic relation these annotations are encoding.
The paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded.
In particular, it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative NP.
It suffers however from a number of problems (van Deemter and Kibble, 2000), chief among which is the fact that the one semantic relation expressed by the scheme, ident, conflates a number of relations that semanticists view as distinct: besides COREFERENCE proper, there are IDENTITY ANAPHORA, BOUND ANAPHORA, and even PREDICATION.

234
Unsupervised Learning Of The Morphology Of A Natural Language
This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words.
We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not.
The resulting grammar matches well the analysis that would be developed by a human morphologist.
In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar.
We propose a recursive structure such that stems can consist of a sub-stem and a suffix.
We use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology.
We observe that less frequent and shorter affixes are more likely to be erroneous.

235
Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems
We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system.
We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
After comparison, their outputs are combined using several voting strategies and second-stage classifiers.
All combination taggers outperform their best component.
The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
We report on accuracy of arounf 97% with in-domain training data for POS tagging using the Penn Treebank.

236
Probabilistic Top-Down Parsing And Language Modeling
This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.
The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.
A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.
A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.
Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.
A small recognition experiment also demonstrates the utility of the model.
Our parser works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning.
At each word in the string, our top-down parser provides access to the weighted set of partial analyses in the beam.

237
The Interaction Of Knowledge Sources In Word Sense Disambiguation
Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research.
An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results.
We present a sense tagger which uses several knowledge sources.
Tested accuracy exceeds 94% on our evaluation corpus
Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words.
It is argued that this approach is more likely to assist the creation of practical systems.
We present a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999).
We use Longman Dictionary of Contemporary English (LDOCE) as sense inventory.
We use POS tags of the focus word itself to aid sense disambiguations related to syntactic differences.
We suggest that use of both syntactic and lexical features will improve disambiguation accuracies.

238
Automatic Verb Classification Based On Statistical Distributions Of Argument Structure
Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks.
Especially important is knowledge about verbs, which are the primary source of relational information in a sentence--the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom).
In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure--specifically, the thematic roles they assign to participants.
We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%.
A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs.
Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means.
We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques.
We work with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop.

239
A Machine Learning Approach To Coreference Resolution Of Noun Phrases
In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text.
The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.
It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of "organization," "person," or other types.
We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches.
Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.
We include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data.
We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model.

240
A Critique And Improvement Of An Evaluation Metric For Text Segmentation
The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution.
We propose a simple modification to the Pk metric that remedies these problems.
This new metric—called WindowDiff — moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
As a measure for segmentation quality we develop WindowDiff, which only evaluates segment boundaries not the labels assigned to them.

241
Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm
This paper brings a logical perspective to the generation of referring expressions, addressing the incompleteness of existing algorithms in this area.
After studying references to individual objects, we discuss references to sets, including Boolean descriptions that make use of negated and disjoined properties.
To guarantee that a distinguishing description is generated whenever such descriptions exist, the paper proposes generalizations and extensions of the Incremental Algorithm of Dale and Reiter (1995).

242
Class-Based Probability Estimation Using A Semantic Hierarchy
This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate.
In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.
There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
A procedure is developed that uses a chi-square test to determine a suitable level of generalization.
In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods.
Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik’s measure of selectional preference.
In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
Briefly, we populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy.

243
Automatic Labeling Of Semantic Roles
We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.
Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as Agent or Patient, or more domain-specific semantic roles, such as Speaker, Message, and Topic.
The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.
We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.
These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.
We used various lexical clustering algorithms to generalize across possible fillers of roles.
Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.
Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents.
At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.
Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task.
We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
We propose the first SRL model on FrameNet.

244
Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status
In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work.
We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles.
We present several experiments measuring our judges’ agreement on these annotations.
We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories.
The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.
We examine the problem of summarizing scientific articles using rhetorical analysis of sentences.
We summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts.

245
A Systematic Comparison Of Various Statistical Alignment Models
We present and compare various methods for computing word alignments using statistical or heuristic models.
We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.
These statistical models are compared with two heuristic models based on the Dice coefficient.
We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models.
As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.
We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.
An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.
In the Appendix, we present an efficient training algorithm for the alignment models presented.
The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments.

246
Graph-Based Generation Of Referring Expressions
This article describes a new approach to the generation of referring expressions.
We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem.
Cost functions are used to guide the search process and to give preference to some solutions over others.
The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches.
One of the strengths of the Graph-Based Algorithm is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to, on top of, etc.).

247
Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation
In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP).
The search algorithm uses the translation model presented in Brown et al. (1993).
Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm.
Word reordering restrictions especially useful for the translation direction German to English are presented.
The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions.
The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary).
For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article.
In our work, a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively.

248
Introduction To The Special Issue On The Web As Corpus
The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.
This special issue of Computational Linguistics explores ways in which this dream is being explored.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results.

249
The Web As A Parallel Corpus
Parallel corpora have become an essential resource for work in multilingual natural language processing.
In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web, first reviewing the original algorithm and results and then presenting a set of significant enhancements.
These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new content based measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale.
Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair.
We mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment method.
We exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents.

250
Using The Web To Obtain Frequencies For Unseen Bigrams
This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
Our study reveals that the large amount of data available for the web counts could outweigh the noisy problems.

251
Head-Driven Statistical Models For Natural Language Parsing
This article describes three statistical models for natural language parsing.
The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.
Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment.
All of these preferences are expressed by probabilities conditioned on lexical heads.
The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.
To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.
We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples.
Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.
We propose to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies.

252
Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences
Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.
The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.
We also investigate use of the one-sense-per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.
Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.
We report that the Word-Class Model performs well in unsupervised WSD.

253
CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System
CorMet is a corpus-based system for discovering metaphorical mappings between concepts.
It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora.
Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain.
The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain.
This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors.
Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples.
Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings.
CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991).
The CorMet system dynamically mines domain specific corpora to find less frequent usages and identifies conceptual metaphors.
We show how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge.

254
The Kappa Statistic: A Second Look
In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks.
In this squib, we highlight issues that affect κ and that the community has largely neglected.
First, we discuss the assumptions underlying different computations of the expected agreement component of κ.
Second, we discuss how prevalence and bias affect the κ measure.

255
Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information
In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.
In particular, existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another.
The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.
We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.
In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.
We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.
The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.
The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole! task.
We decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model.
We describe a method that combines morphologically split verbs in German, and also reorders questions in English and German.

256
Learning Subjective Language
Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations.
There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.
The goal of this work is learning subjective language from corpora.
Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.
The features are also examined working together in concert.
The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.
In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.
Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.
We show that low-frequency words and some collocations are a good indicators of subjectivity.

257
The Alignment Template Approach To Statistical Machine Translation
A phrase-based statistical machine translation approach — the alignment template approach — is described.
This translation approach allows for general many-to-many relations between words.
Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly.
The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach.
Thereby, the model is easier to extend than classical statistical machine translation systems.
We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm.
The evaluation of this approach is performed on three different tasks.
For the German–English speech Verbmobil task, we analyze the effect of various system components.
On the French–English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model.
In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.
We describe a phrase-extract algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment.

258
Intricacies Of Collins Parsing Model
This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.
We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.
Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.
The results suggest that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words.
We show that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance.

259
Discriminative Reranking For Natural Language Parsing
This article considers approaches which rerank the output of an existing probabilistic parser.
The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.
A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.
The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.
We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).
We apply the boosting method to parsing the Wall Street Journal treebank.
The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.
The new model achieved 89.75% F-measure, a 13% relative decrease in F-measure error over the baseline model’s score of 88.2%.
The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.
Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.
We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models.
Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
We show that applying reranking techniques to the n-best output of a base parser can improve parsing performance.
We propose a method only updates values of features co-occurring with a rule feature on examples at each iteration.

260
The Proposition Bank: An Annotated Corpus Of Semantic Roles
The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank.
The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.
We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.
We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty "trace" categories of the treebank.
As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent semantic role labels across different syntactic realizations of the same verb.

261
Sentence Fusion For Multidocument News Summarization
A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading.
In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents.
Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence.
Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.
We represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output.
We introduce the problem of converting multiple sentences into a single summary sentence.

262
Improving Machine Translation Performance By Exploiting Non-Parallel Corpora
We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.
We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
We use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.
We filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary).
We define features primarily based on IBM Model 1 alignments (Brown et al, 1993).

263
Evaluating WordNet-based Measures of Lexical Semantic Relatedness
The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed.
We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors.
An information-content–based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik.
In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.
WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure.

264
Similarity of Semantic Relations
There are at least two kinds of similarity.
Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes.
When two words have a high degree of attributional similarity, we call them synonyms.
When two pairs of words have a high degree of relational similarity, we say that their relations are analogous.
For example, the word pair mason:stone is analogous to the pair carpenter:wood.
This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity.
LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval.
Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions.
In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus.
LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs.
LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%.
On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.
We develop a corpus based approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms.
We describe a method (Latent Relational Analysis) that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.

265
Hierarchical Phrase-Based Translation  
We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases.
The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations.
Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation.
We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy.
Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system.
The hierarchical phrase-based model makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope.

266
CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank
This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.
The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank.
It is available from the Linguistic Data Consortium, and has been used to train wide coverage statistical parsers that obtain state-of-the-art rates of dependency recovery.
In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.
We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks.
The CCGbank-style dependency is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child.
CCGbank is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank.

267
Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models
This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.
The models are "full" parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.
Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.
The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.
The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.
Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.
A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence.
The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser.
Surprisingly, given CCG’s 'spurious ambiguity,' the parsing speeds are significantly higher than those reported for comparable parsers in the literature.
We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG’s nonstandard derivations.
This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate–argument dependencies from CCGbank.
The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types.
The evaluation on DepBank raises a number of issues regarding parser evaluation.
This article provides a comprehensive blueprint for building a wide-coverage CCG parser.
We demonstrate that both accurate and highly efficient parsing is possible with CCG.
From a parsing perspective, the C & C parser has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009).

268
Modeling Local Coherence: An Entity-Based Approach
This article proposes a novel framework for representing and measuring local coherence.
Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.
The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.
We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.
Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
An entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity.
We experiment on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).

269
Feature Forest Models for Probabilistic HPSG Parsing
Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.
This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.
For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.
These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.
This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.
The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests.
Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.
Feature forest models are maximum entropy models defined over feature forests.
A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.
Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.
This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests.
Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.
The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.

270
A Global Joint Model for Semantic Role Labeling
We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.
We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases.
The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.
We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees.
The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank.
For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively.
We also present results on the CoNLL 2005 shared task data set.
Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.
We present a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task.

271
The Importance of Syntactic Parsing and Inference in Semantic Role Labeling
We present a general framework for semantic role labeling.
The framework combines a machine learning technique with an integer linear programming–based inference procedure, which incorporates linguistic and structural constraints into a global decision process.
Within this framework, we study the role of syntactic parsing information in semantic role labeling.
We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first stage—the pruning stage.
Surprisingly, the quality of the pruning stage cannot be solely determined based on its recall and precision.
Instead, it depends on the characteristics of the output candidates that determine the difficulty of the downstream problems.
Motivated by this observation, we propose an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves its performance.
Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling, and achieves the highest F1 score among 19 participants.
The verb SRL system consists of four stages: candidate generation, argument identification, argument classification and inference.

272
Algorithms for Deterministic Incremental Dependency Parsing
Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars.
Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations.
In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems.
We then describe and analyze two families of such algorithms: stack-based and list-based algorithms.
In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant.
For each of the four algorithms, we give proofs of correctness and complexity.
In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages.
We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.
However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing.
The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.
Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm.
Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.
We give a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing.

273
Survey Article: Inter-Coder Agreement for Computational Linguistics
This article is a survey of methods for measuring agreement among corpus annotators.
It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks — but that their use makes the interpretation of the value of the coefficient even harder.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in this work.

274
Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis
Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).
However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity.
Positive words are used in phrases expressing negative sentiments, or vice versa.
Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.
The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.
Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.
The evaluation includes assessing the performance of features across multiple machine learning algorithms.
For all learning algorithms except one, the combination of all features together gives the best performance.
Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity.
These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral.
We explore the difference between prior and contextual polarity: words that lose polarity in context, or whose polarity is reversed because of context.

275
Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods
The task of paraphrasing is inherently familiar to speakers of all languages.
Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language — words, phrases, and sentences — is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications.
In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research.
Recent work done in manual and automatic construction of paraphrase corpora is also examined.
We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation.
We survey a variety of data driven paraphrasing techniques, categorizing them based on the type of data that they use.

276
Distributional Memory: A General Framework for Corpus-Based Semantics
Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.
As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.
In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
We use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure.

277
A Plan-Based Analysis Of Indirect Speech Act
We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement.
This cooperative behaviour is independently motivated and may or may not be intended by speakers.
If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly.
Heuristics are suggested to decide among the interpretations.

278
Extraposition Grammars
Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses.
The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax.
Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, we, introducing extraposition grammars, is focused on displacement of noun phrases in English.

279
Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table
Sentences are far more ambiguous than one might have thought.
There may be hundreds, perhaps thousands, of syntactic parse trees for certain very natural sentences of English.
This fact has been a major problem confronting natural language processing, especially when a large percentage of the syntactic parse trees are enumerated during semantic/pragmatic processing.
In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees.
These regularities will be expressed as linear combinations of ATN networks, and also as sums and products of formal power series.
We believe that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together.
The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function.

280
Attention Intentions And The Structure Of Discourse
In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse.
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure), a structure of purposes (called the intentional structure), and the state of focus of attention (called the attentional state).
The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate.
The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them.
The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds.
The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse.
The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions.
The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses.
Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored.
This theory provides a framework for describing the processing of utterances in a discourse.
Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and tracking the discourse through the operation of the mechanisms associated with attentional state.
This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain.
We proposed a theory of discourse structure to account for why an utterance was said and what was meant by it.

281
An Efficient Augmented-Context-Free Parsing Algorithm
An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed.
The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.
Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a "graph-structured stack".
The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way.
We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.
The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a five- to tenfold speed advantage over Earley's context-free parsing algorithm.
The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence.
A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations.
The parser is used in the multi-lingual machine translation project at CMU.
Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU.

282
An Algorithm For Generating Quantifier Scopings
The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination.
But scope dependencies are not so transparent.
As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow.
This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure.
The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy.
We extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach.
We presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination.
We introduce an algorithm for generating all possible quantifier scopings.

283
Grammatical Category Disambiguation By Statistical Optimization
Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information.
An innovative method (called "CLAWS") was recently developed by those working with the Lancaster-Oslo/Bergen Corpus of British English.
This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags.
Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways.
The effects upon accuracy of this manual augmentation are not individually known.
The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments.
Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%.
This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English.

284
Temporal Ontology And Temporal Reference
A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed.
This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a "nucleus."
A nucleus can be thought of as an association of a goal event, or "culmination," with a "preparatory process" by which it is accomplished, and a "consequent state," which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure.
The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category.
We claim that any manageable formalism for natural language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language.
We describe temporal expressions relating to changes of state.

285
Tense As Discourse Anaphor
In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1. They specify entities in an evolving model of the discourse that the listener is constructing; 2. The particular entity specified depends on another entity in that part of the evolving "discourse model" that the listener is currently attending to.
Such expressions have been called anaphors.
I show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases.
This not only allows us to capture in a simple way the oft-stated but difficult-to-prove intuition that tense is anaphoric, but also contributes to our knowledge of what is needed for understanding narrative text.
We improve upon the above work by specifying rules for how events are related to one another in a discourse and Sing and Sing defined semantic constraints through which events can be related (Sing, 1997).

286
Word Association Norms Mutual Information And Lexicography
The term word association is used in a very particular sense in the psycholinguistic literature.
(Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.)
We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
(The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.)
The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.
In our work, the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.

287
Semantic-Head-Driven Generation
We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable.
In particular, unlike a previous bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion.
The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.
We introduce a head-driven algorithm for generating from logical forms.

288
A Statistical Approach To Machine Translation
In this paper, we present a statistical approach to machine translation.
We describe the application of our approach to translation from French to English and give preliminary results.
We estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.

289
Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text
In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
These lexical chains are a direct result of units of text being 'about the same thing,' and finding text structure involves finding units of text that are about the same thing.
Hence, computing the chains is useful, since they will have a correspondence to the structure of the text.
Determining the structure of text is an essential step in determining the deep meaning of the text.
In this paper, a thesaurus is used as the major knowledge base for computing lexical chains.
Correspondences between lexical chains and structural elements are shown to exist.
Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure.
The lexical chains also provide a semantic context for interpreting words, concepts, and sentences.
We propose the idea of using lexical chains as indicators of lexical cohesion.
We propose the concept of Lexical Chains to explore the discourse structure of a text.

290
met*: A Method For Discriminating Metonymy And Metaphor By Computer
The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences.
In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate.
Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly.
Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter.
The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5.
Some examples of meta5's analysis of metaphor and metonymy are given.
The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology.
We use selectional preference violation technique to detect metaphors.
We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly.
We build a system met*, which is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language.
We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly.

291
The Generative Lexicon
In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations.
In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues.
Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon.
I argue that lexical decomposition is possible if it is performed generatively.
Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions.
I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry.
Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance.
This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.
We propose the Generative Lexicon Theory (GLT), which can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI.

292
Using Multiple Knowledge Sources For Word Sense Discrimination
This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences.
To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions.
However, current approaches make use of only small subsets of this information.
Here we will describe how to use the whole range of information.
Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts.
We will describe a method of combining cues on the basis of their individual specificity, rather than a fixed ranking among cue-types.
We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.
We are one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP.
We describe a study of different sources useful for word sense disambiguation, including morphological information.

293
TINA: A Natural Language System For Spoken Language Applications
A new natural language system, TINA, has been developed for applications involving spoken language tasks.
TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept.
TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.
An initial set of context-free rewrite rules provided by hand is first converted to a network structure.
Probability assignments on all arcs in the network are obtained automatically from a set of example sentences.
The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal with long-distance movement, agreement, and semantic constraints.
TINA provides an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer.
The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
We propose the language understanding system, TINA, that integrates key ideas context free grammar, augmented transition network and unification concepts.

294
Class-Based N-Gram Models Of Natural Language
We address the problem of predicting a word from previous words in a sample of text.
In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.
We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
We propose a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance).

295
A Problem For RST: The Need For Multi-Level Discourse Analysis
We note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the reader's beliefs or attitudes) levels of discourse.
We argue that both informational (semantic) and intentional relations can hold between clause simultaneously and independently.

296
Introduction To The Special Issue On Computational Linguistics Using Large Corpora
A historical account of this empirical renaissance is provide in this work.
Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach.

297
Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars
We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques.
The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis.
We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars.
The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser.
The latter is constructed by associating probabilities with the LR parse table directly.
This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism.
We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser.
We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions.
Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic)frequency of occurrence.
Our work on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags.
Our statistical parser is an extension of the ANLT grammar development system.

298
Accurate Methods For The Statistics Of Surprise And Coincidence
Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed.
In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.
This assumption of normal distribution limits the ability to analyze rare events.
Unfortunately rare events do make up a large fraction of real text.
However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms.
In some cases, these measures perform much better than the methods previously used.
In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.
This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.
Since it was first introduced to the NLP community by us, the G log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations.

299
A Program For Aligning Sentences In Bilingual Corpora
Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English).
One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.
This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths.
The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.
A probabilistic score is assigned to each proposed correspondence of sen tences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference.
This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.
It is remarkable that such a simple approach works as well as it does.
An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German.
The method correctly aligned all but 4% of the sentences.
Moreover, it is possible to extract a large subcorpus that has a much smaller error rate.
By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%.
There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.
To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI).
In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.
We present a hybrid approach, and the basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences.
We propose a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences.

300
Structural Ambiguity And Lexical Relations
We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.
This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.
We are the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results.
We propose one of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions.
We used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n.

301
Text-Translation Alignment
We present an algorithm for aligning texts with their translations that is based only on internal evidence.
The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions.
It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate.
The algorithm appears to converge to the correct sentence alignment in only a few iterations.
Our morphology algorithm is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms.

302
Retrieving Collocations From Text: Xtract
Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages.
Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres.
Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data.
These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations.
However, none of these techniques provides functional information along with the collocation.
Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.
In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora.
These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output.
These techniques have been implemented and resulted in a lexicographic tool, Xtract.
The techniques are described and some results are presented on a 10 million-word corpus of stock market news reports.
A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80 %.
We develop Xtract, a term extraction system.
We propose a statistical model by measuring the spread of the distribution of co occurring pairs of words with higher strength.
In terms of practical MWE identification systems, we propose a well known approach that uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.

303
From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax
Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text.
No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words?
This paper describes an approach based on two principles.
First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences.
Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure.
Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue.
The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner.
Lerner starts out with no knowledge of content words--it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.
Our study is focused on large-scaled automatic acquisition of subcategorization frames (SCF).

304
The Mathematics Of Statistical Machine Translation: Parameter Estimation
We describe a series of five statistical models of the translation process and give algorithms, for estimating the parameters of these models given a set of pairs of sentences that are translations of one another.
We define a concept of word-by-word alignment between such pairs of sentences.
For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments.
We give an algorithm for seeking the most probable of these alignments.
Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences.
We have a great deal of data in French and English from the proceedings of the Canadian Parliament.
Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages.
We also feel again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.
Our model for Statistical machine translation (SMT) is focused on word to word translation and was based on the noisy channel approach.

305
Building A Large Annotated Corpus Of English: The Penn Treebank

306
Lexical Semantic Techniques For Corpus Analysis
In this paper we outline a research program for computational linguistics, making extensive use of text corpora.
We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence.
The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items.
Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems.
We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.
In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools.
Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.
we present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles.
We show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate.
We use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus.

307
Coping With Ambiguity And Unknown Words Through Probabilistic Models
From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models.
This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning case frame information for verbs from example uses.
From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text.
Our model incorporates the treatment of unknown words within the probability model.

308
Empirical Studies On The Disambiguation Of Cue Phrases
Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse.
For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment.
However, while cue phrases may convey discourse structure, each also has one or more alternate uses.
While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression.
Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed.
This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power.
Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing.
A prosodic model that characterizes these distinctions is identified.
This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech.
In the literature, there is still no consistent definition for discourse markers.
We find that into national phrasing and pitch accent play a role in disambiguating cue phrases, and hence in helping determine discourse structure.

309
Tagging English Text With A Probabilistic Model
In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.
The main novelty of these experiments is the use of untagged text in the training of the model.
We have used a simple triclass Marlcov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.
Two approaches in particular are compared and combined:
using text that has been tagged by hand and computing relative frequency counts,
using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.
Experiments show that the best training is obtained by using as much tagged text as possible.
They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.
In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.
we attempted to improve HMM POS tagging by expectation maximization with unlabeled data.
we introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization.
In the context of POS tagging, we introduce a method that he calls maximum likelihood tagging.

310
Japanese Discourse And The Process Of Centering
This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation.
We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process; we demonstrate this argument with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese.
The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical TOPIC, the postposition wa, as well as those for grammatical functions such as SUBJECT, ga, OBJECT, o and OBJECT2, ni.
In addition, we investigate the role of speaker's EMPATHY, which is the viewpoint from which an event is described.
This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as kureta, kita.
Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors.
We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS, but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity.
We propose a discourse rule of ZERO TOPIC ASSIGNMENT, and show that CENTERING provides constraints on when a ZERO can be interpreted as the ZERO TOPIC.
We propose forward center ranking for Japanese.

311
Regular Models Of Phonological Rule Systems
This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology.
It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism.
This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.
we provide an algorithm for compilation into transducers.
we describe a general method representing a replacement procedure as finite-state transduction.

312
A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures
This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures.
Analysis of long sentences is one of the most difficult problems in natural language processing.
The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences.
Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts.
Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure.
This is realized using a dynamic programming technique.
A long sentence can be reduced into a shorter form by recognizing conjunctive structures.
Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules.
A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components.
Through our dependency analysis process, we can find the ellipses and recover the omitted components.
We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method.
we propose a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus.
we propose a similarity-based method to resolve both of the two tasks for Japanese.
we propose a Japanese parsing method that included coordinate structure detection.

313
An Algorithm For Pronominal Anaphora Resolution
This paper presents an algorithm for identifying the noun phrase antecedents of third person pronouns and lexical anaphors (reflexives and reciprocals).
The algorithm applies to the syntactic representations generated by McCord's Slot Grammar parser and relies on salience measures derived from syntactic structure and a simple dynamic model of attentional state.
Like the parser, the algorithm is implemented in Prolog.
The authors have tested it extensively on computer manual texts and conducted a blind test on manual text containing 360 pronoun occurrences.
The algorithm successfully identifies the antecedent of the pronoun for 86% of these pronoun occurrences.
The relative contributions of the algorithm's components to its overall success rate in this blind test are examined.
Experiments were conducted with an enhancement of the algorithm that contributes statistically modelled information concerning semantic and real-world relations to the algorithm's decision procedure.
Interestingly, this enhancement only marginally improves the algorithm's performance (by 2%).
The algorithm is compared with other approaches to anaphora resolution that have been proposed in the literature.
In particular, the search procedure of Hobbs' algorithm was implemented in the Slot Grammar framework and applied to the sentences in the blind test set.
The authors" algorithm achieves a higher rate of success (4%) than Hobbs' algorithm.
The relation of the algorithm to the centering approach is discussed, as well as to models of anaphora resolution that invoke a variety of informational factors in ranking antecedent candidates.
In the heuristic salience-based algorithm for pronoun resolution, we introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements.
we describe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%).

314
Word Sense Disambiguation Using A Second Language Monolingual Corpus
This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language.
This approach exploits the differences between mappings of words to senses in different languages.
The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable.
The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon.
The preferred senses are then selected according to statistics on lexical relations in the target language.
The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence.
The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation.
The paper includes a detailed comparative analysis of statistical sense disambiguation methods.
we propose an approach to WSD using monolingual corpora,a bilingual lexicon and a parser for the source language.

315
Machine Translation Divergences: A Formal Description And Proposed Solution
There are many cases in which the natural translation of one language into another results in a very different form than that of the original.
The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical.
Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of the systematic relation between lexical-semantic structure and syntactic structure.
This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved.
This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.
We categorize sources of syntactic divergence between languages.

316
An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities
We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities.
Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input.
Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure.
It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm.
Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.
An Earley chart is used for keeping track of all derivations that are consistent with the input.

317
Centering: A Framework For Modeling The Local Coherence Of Discourse
This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment.
It presents a framework and initial theory of centering intended to model the local component of attentional state.
The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state.
It demonstrates that the attentional state properties modeled by centering can account for these differences.
Our centering model uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts.
Our centering theory postulates strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference.
Our centering Theory is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions.
Our centering Theory is an influential framework for modelling entity coherence in computational linguistics in the last two decades.

318
Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging
Recently, there has been a rebirth of empiricism in the field of natural language processing.
Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge.
Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics.
This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior.
In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge.
This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance.
We present a detailed case study of this learning method applied to part-of-speech tagging.
We outline a transformation-based learned which learns guessing rules from a pre-tagged training corpus.
We propose non-sequential transformation-based learning.
We introduce a symbolic machine learning method, a class sequence example Transformation-based learning.

319
Translating Collocations For Bilingual Lexicons: A Statistical Approach
Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis.
We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations.
Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains.
The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same.
For example, Champollion translates make ... decision, employment equity, and stock market into prendre ... decision, equite en matiere d'emploi, and bourse respectively.
Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average.
In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation.
The relationship between pointwise Mutual Information and the Dice coefficient is discussed in this work.
We propose a corpus-based method to extract bilingual lexicons.
We propose a statistical association measure of the Dice coefficient to deal with the problem of collocation translation.

320
A Maximum Entropy Approach To Natural Language Processing
The concept of maximum entropy can be traced back along multiple threads to Biblical times.
Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition.
In this paper, we describe a method for statistical modeling based on maximum entropy.
We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.
We propose a gain-informed selection method.
.

321
Assessing Agreement On Classification Tasks: The Kappa Statistic
Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other.
Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic.
We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.
Our method, kappa statistic, is used extensively in empirical studies of discourse (Carletta, 1996).

322
A Stochastic Finite-State Word-Segmentation Algorithm For Chinese
The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.
For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.
In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.
In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.
The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.
We evaluate the system's performance by comparing its segmentation 'judgments' with the judgments of a pool of human segmenters, and the system is shown to perform quite well.
We built a word uni gram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon.
We proposed a method to estimate a set of initial word frequencies without segmenting the corpus.

323
The Reliability Of A Dialogue Structure Coding Scheme
This paper describes the reliability of a dialogue structure coding scheme based on utterance function, game structure, and higher-level transaction structure that has been applied to a corpus of spontaneous task-oriented spoken dialogues.
We computed agreement on a coarse segmentation level that was constructed on the top of finer segments, by determining how well coders agreed on where the coarse segments started, and, for agreed starts, by computing how coders agreed on where coarse segments ended.

324
TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages
TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics.
The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution.
The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts.
Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.
We compute chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt).

325
Discourse Segmentation By Human And Automated Means
The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse.
However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them.
We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues.
The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments.
We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion.
In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation.
On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses).
We then develop a second set using two methods: error analysis and machine learning.
Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves.
We describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie.

326
Finite-State Transducers In Language And Speech Processing
Finite-state machines have been used in various domains of natural language processing.
We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.
We recall classical theorems and give new ones characterizing sequential string-tostring transducers.
Transducers that output weights also play an important role in language and speech processing.
We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.
Some applications of these algorithms in speech recognition are described and illustrated.
Application of cascades of weighted string transducers (WSTs) has been well-studied in this work.

327
Stochastic Inversion Transduction Grammars And Bilingual Parsing Of Parallel Corpora
We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications.
Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm.
A convenient normal form is shown to exist.
Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints.
We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.
We use an inside-outside type of training algorithm to learn statistical context free transduction.
Our Bilingual Bracketing is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment.
We introduce a polynomial-time solution for the alignment problem based on synchronous binary trees.

328
Automatic Rule Induction For Unknown-Word Guessing
Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers.
In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments.
The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus.
Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules.
Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words.
Our model, LTPOS, performs both sentence identification and POS tagging.
Our ltpos is a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module.
Our ltpos is a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module.

329
Stochastic Attribute-Value Grammars
Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research.
To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm.
In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters.
The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995).
To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields.
In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations.
The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm.
We proposes a Markov Random Field or log linear model for SUBGs.

330
Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art
We present a very concise survey of the history of ideas used in word sense disambiguation.
In general, the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches.
We argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval).

331
Using Corpus Statistics And WordNet Relations For Sense Identification
Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.
We describe a statistical classifier that combines topical context with local cues to identity a word sense.
The classifier is used to disambiguate a noun, a verb, and an adjective.
A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.
Test results are compared with those from manually tagged training examples.
We present a method to obtain sense-tagged examples using monosemous relatives.

332
A Corpus-Based Investigation Of Definite Description Use
We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation.
We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of l,412 definite descriptions.
We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text.
The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K = 0.63) that we obtained using versions of Hawkins's and Prince's classification schemes; better results (K = 0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention.
The agreement about antecedents was also not complete.
These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation.
From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation.
We propose an annotation scheme, which is a product of a corpus based analysis of definite description (DD) use showing that more than 50% of the DDs in their corpus are discourse new or unfamiliar.

333
Generalizing Case Frames Using A Thesaurus And The MDL Principle
A new method for automatically acquiring case frame patterns from large corpora is proposed.
In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed.
In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as "cuts" in the thesaurus tree, thus reducing the generalization problem to that of estimating a "tree cut model" of the thesaurus tree.
An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL.
Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity.
Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.
We use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results.
We propose a model in which the appropriate cut c is selected according to the MinimumDescription Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length.

334
New Figures Of Merit For Best-First Probabilistic Chart Parsing
Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first.
Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser.
While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits.
We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars.
We present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string.

335
Generating Natural Language Summaries From Multiple On-Line Sources
We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information.
The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information.
We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences.
A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.
We combine work in information extraction and natural language processing.

336
Machine Transliteration
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.
These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.
For example, "computer" in English comes out as "konpyuutaa" in Japanese.
Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.
We describe and evaluate a method for performing backwards transliterations by machine.
This method uses a generative model, incorporating several distinct stages in the transliteration process.
We proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds.

337
PCFG Models Of Linguistic Tree Representations
The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.
This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8 %, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.
The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.
We annotate each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank.

338
Bitext Maps And Alignment Via Pattern Recognition
Texts that are available in two languages (bitexts) are becoming more and more plentiful, both in private data warehouses and on publicly accessible sites on the World Wide Web.
As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools.
The first step in extracting useful information from bitexts is to find corresponding words and/or text segment boundaries in their two halves (bitext maps).
This article advances the state of the art of bitext mapping by formulating the problem in terms of pattern recognition.
From this point of view, the success of a bitext mapping algorithm hinges on how well it performs three tasks: signal generation, noise filtering, and search.
The Smooth Injective Map Recognizer (SIMR) algorithm presented here integrates innovative approaches to each of these tasks.
Objective evaluation has shown that SIMR's accuracy is consistently high for language pairs as diverse as French/English and Korean/English.
If necessary, S IMR's bitext maps can be efficiently converted into segment alignments using the Geometric Segment Alignment (GSA) algorithm, which is also presented here.
SIMR has produced bitext maps for over 200 megabytes of French-English bitexts.
GSA has converted these maps into alignments. Both the maps and the alignments are available from the Linguistic Data Consortium.
We normalize LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR).

339
Supertagging: An Approach To Almost Parsing
In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.
Our thesis is that the computation of linguistic structure can be localized iflexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.
The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag.
Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear.
This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser.
But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses.
We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.
The supertags in LTAG combine both phrase structure information and dependency information in a single representation.
Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need "only" combine the individual supertags.
This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.
We indicate that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enable effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing.

340
Functional Centering Grounding Referential Coherence In Information Structure
Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model.
We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances.
These new criteria are based on the distinction between hearer-old and hearer-new discourse entities.
We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora.
Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammatical role-driven centering algorithm and from a functional centering algorithm.
The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model.
we introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities.

341
Semiring Parsing
We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers.
Each parser performs abstract computations using the operations of a semiring.
The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings.
We also show how to use the same representation, interpreted differently, to compute outside values.
The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.
We show how a parsing logic can be combined with various semirings to compute different kinds of information about the input.
We augment such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure.

342
Decoding Complexity In Word-Replacement Translation Models
Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer.
Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts.
The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel).
In order to translate (or "decode") a French string, we look for the most likely English source.
We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.
We trace this complexity to factors not present in other decoding problems.
we proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model.
we show that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case.

343
The Penn Discourse TreeBank 2.0.
We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus.
We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments.
We list the differences between PDTB-1.0 and PDTB-2.0.
We present representative statistics for several aspects of the annotation in the corpus.
we present The Penn Discourse Treebank (PDTB) ,such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicate argument approach (Webber, 2004).

344
A Model-Theoretic Coreference Scoring Scheme
This note describes a scoring scheme for the coreference task in MUC6.
It improves on the original approach l by: (1) grounding the scoring scheme in terms of a model ; (2) producing more intuitive recall and precision scores ; and (3) not requiring explicit computation of the transitive closure of coreference.
The principal conceptual difference is that we have moved from a syntactic scoring model based on following coreference links to an approach defined by the model theory of those links.
In brief, the scheme operates by comparing the equivalence classes defined by the links in the key and the response, rather than the links themselves (thus, this is only well defined for identity links, at the moment).
These classes are of course the models of the IDENT equivalence relation, and this strategy is preferable for a number of reasons, one being that the scores are independent of the particular links used to encode the equivalence relation.
The scores themselves are obtained by determining the minimal perturbations to the response that are required to transform its corresponding equivalence classes into those of the key.
Specifically, the recall (respectively precision) error terms are found by calculating the least number of links that need to be added to the respons e (respectively the key) in order to have the classes align.
Although at first blush this seems combinatorially explosive, due to references to minimal spanning subsets of the equivalence relation, it turns out it can be accomplished with a very simple counting scheme.
we introduce the link-based MUC evaluation metric for the MUC-6 and MUC 7 co reference tasks.

345
MITRE: Description Of The Alembic System Used For MUC-6
As with several other veteran Muc participants, MITRE'S Alembic system has undergone a major transformation in the past two years.
The genesis of this transformation occurred during a dinner conversation at the last Muc conference, MUC-5.
At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax.
We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categorial pseudo-parser we had in place at the time.
The problem was, which alternative?
The answer came in the form of rule sequences, an approach Eric Brill originally laid out in his work on part-of-speech tagging [5, 7].
Rule sequences now underlie all the major processing steps in Alembic: part-of-speech tagging, syntactic analysis, inference, and even some of the set-fill processing in the Template Elemen t task (TE).
We have found this approach to provide almost an embarrassment of advantages, speed and accuracy being the most externally visible benefits.
In addition, most of our rule sequence processors are trainable, typically from small samples.
The rules acquired in this way also have the characteristic that they allow one to readily mix hand-crafted and machine-learned elements.
We have exploited this opportunity to apply both machine-learned and hand-crafted rules extensively, choosing in some instances to run sequences that were primarily machine-learned, and in other cases to run sequences that were entirely crafted by hand.
Our typical machine learning approaches for English NE are transformation-based learning.

346
Transformation Based Learning In The Fast Lane
Transformation-based learning has been successfully employed to solve many natural language processing problems.
It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance.
The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).
The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner.
This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.
we propose fnTBL toolkit, which implements several optimizations in rule learning to drastically speed up the time needed for training.

347
Text And Knowledge Mining For Coreference Resolution
Traditionally coreference is resolved by satisfying a combination of salience, syntactic, semantic and discourse constraints.
The acquisition of such knowledge is time-consuming, difficult and error-prone.
Therefore, we present a knowledge minimalist methodology of mining coreference rules from annotated text corpora.
Semantic consistency evidence, which is a form of knowledge required by coreference, is easily retrieved from WordNet.
Additional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts.
We use paths through Wordnet, using not only synonym and is-a relations, but also parts, morphological derivations, gloss texts and polysemy, which are weighted with a measure based on the relation types and number of path elements.
The path patterns in WordNet are utilized to compute the semantic consistency between NPs.

348
A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense
This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.
This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.
It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.
We compare decision trees, decision stumps and a Naive Bayesian classifier to show that bigrams are very useful in identifying the intended sense of a word.

349
Edit Detection And Parsing For Transcribed Speech
We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words.
The edit detector achieves a misclassification rate on edited words of 2.2%.
(The NULL-model, which marks everything as not edited, has an error rate of 5.9%.)
To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes.
By this metric the parser achieves 85.3% precision and 86.5% recall.
Our work in statistically parsing conversational speech has examined the performance of a parser that removes edit regions in an earlier step.

350
Multipath Translation Lexicon Induction Via Bridge Languages
This paper presents a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages.
Bilingual lexicons within languages families are induced using probabilistic string edit distance models.
Translation lexicons for arbitrary distant language pairs are then generated by a combination of these intra-family translation models and one or more cross-family on-line dictionaries.
Up to 95% exact match accuracy is achieved on the target vocabulary (30-68% of inter-family test pairs).
Thus substantial portions of translation lexicons can be generated accurately for languages where no bilingual dictionary or parallel corpora may exist.
We present a method for inducing translation lexicons based on transduction modules of cognate pairs via bridge languages.
We present a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages.

351
A Probabilistic Earley Parser As A Psycholinguistic Model
In human sentence processing, cognitive load can be defined many ways.
This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix w0...i−1 on a phrase-structural language model.
These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis.
Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.
Since the introduction of a parser-based calculation for surprisal, statistical techniques have been become common as models of reading difficulty and linguistic complexity.

352
Applying Co-Training Methods To Statistical Parsing
We propose a novel Co-Training method for statistical parsing.
The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.
The algorithm iteratively labels the entire data set with parse trees.
Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.
Our co-training a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.

353
Knowledge-Free Induction Of Inflectional Morphologies
We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input.
Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English.
Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.
We use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English.

354
Chunking With Support Vector Machines
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces.
Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality.
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations.
Experimental results show that our approach achieves higher accuracy than previous approaches.
In this paper, we develop an SVMs-based chunking tool YamCha.

355
Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora
This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora.
First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences.
Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language.
We induce a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources.
We are the first to propose the use of parallel texts to bootstrap the creation of taggers.

356
Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment
We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing.
Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences.
The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
We propose to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR.
We construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm.
We propose a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations.
We present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus.

357
Inducing History Representations For Broad Coverage Statistical Parsing
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.
Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
Of the previous work on using neural net works for parsing natural language, the most empirically successful has been our work using Simple Synchrony Networks.
We test the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs.

358
A* Parsing: Fast Exact Viterbi Parse Selection
We present an extension of the classic A* search procedure to tabular PCFG parsing.
The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.
We discuss various estimates and give efficient algorithms for com- puting them.
On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.
Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.
Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
We describe admissible heuristics and an A*framework for parsing.

359
Statistical Phrase-Based Translation
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.
Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.
Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance.
Learning only syntactically motivated phrases degrades the performance of our systems.
We propose STIR, as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese.

360
Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
We are the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.
We propose ROUGE, a semi automatic approach,  which is primarily based on n gram co-occurrence between automatic and human summaries.

361
Multitext Grammars And Synchronous Parsers
Multitext Grammars (MTGs) generate arbitrarily many parallel texts via production rules of arbitrary length.
Both ordinary MTGs and their bilexical subclass admit relatively efficient parsers.
Yet, MTGs are more expressive than other synchronous formalisms for which parsers have been described in the literature.
The combination of greater expressive power and relatively low cost of inference makes MTGs an attractive foundation for practical models of translational equivalence.
We present algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars.
We discuss the applicability of the so-called hook trick for parsing bilexical multi text grammars.

362
COGEX: A Logic Prover For Question Answering
Recent TREC results have demonstrated the need for deeper text understanding methods.
This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system.
The approach is to transform questions and answer passages into logic representations.
World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text.
Moreover, the trace of the proofs provide answer justifications.
The results show that the prover boosts the performance of the QA system on TREC questions by 30%.
COGEX uses its logic prover to extract lexical relationships between the question and its candidate answers.

363
Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.
These FSAs are good representations of paraphrases.
They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.
Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.
We describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases.
We propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences.

364
Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar
We present an application of ambiguity pack- ing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection.
Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems.
An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings.
Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator.
We present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions.
We apply linguistically rich LFG grammars to a sentence compression system.

365
Shallow Parsing With Conditional Random Fields
Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.
Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods.
We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.
Improved training methods based on modern optimization algorithms were critical in achieving these results.
We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models
CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking in this work.

366
Sentence Level Discourse Parsing Using Syntactic And Lexical Information
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser.
A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
within Rhetorical Structure Theory (RST), we have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level.
We introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels.

367
Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network
We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.
Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result
We present a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts.

368
Factored Language Models And Generalized Parallel Backoff
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
An FLM represents words as bundles of features (e.g. , morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.
GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.
These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.
This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.
Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.
In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.
We show that factored language models are able to outperform standard n-gram techniques in terms of perplexity.
A factored language model (FLM) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.

369
Precision And Recall Of Machine Translation
Machine translation can be evaluated using precision, recall, and the F-measure.
These standard measures have significantly higher correlation with human judgments than recently proposed alternatives.
More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved.
The relevant software is publicly available.
We formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.

370
A Statistical Model For Multilingual Entity Detection And Tracking
Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks.
In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text.
Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features.
In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers.
The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.

371
Speed And Accuracy In Shallow And Deep Stochastic Parsing
This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems.
The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers.
The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log-linear disambiguation component and provides much richer representations theory.
We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times.
We report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank.

372
Training Tree Transducers
Many probabilistic models for natural language are now written in terms of hierarchical tree structure.
Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling.
The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.
We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers.
We define training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.
We describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.

373
Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization
We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear.
We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
We then apply our method to two complementary tasks: information ordering and extractive summarization.
Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.
We proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations.

374
The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks
Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks.
So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.
The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams.
For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus.
However, in most cases, web-based models fail to outperform more sophisticated state-of-the-art models trained on small corpora.
We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.
Our web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as the training corpus.

375
Evaluating Content Selection In Summarization: The Pyramid Method
We present an empirically grounded method for evaluating content selection in summarization.
It incorporates the idea that no single best model summary for a collection of documents exists.
Our method quantifies the relative importance of facts to be conveyed.
We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
We propose a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents.

376
A Smorgasbord Of Features For Statistical Machine Translation
We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.
Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list.
Feature weights were optimized directly against the BLEU evaluation metric on held-out data.
We present results for a small selection of features at each level of syntactic representation.
At the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score.
The effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.

377
Minimum Bayes-Risk Decoding For Statistical Machine Translation
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
We report the performance of the MBR decoders on a Chinese-to-English translation task.
Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system's translations relative to the model's distribution over possible translations.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder.

378
Discriminative Reranking For Machine Translation
This paper describes the application of discriminative reranking techniques to the problem of machine translation.
For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n-best list of candidate translations in the target language.
We introduce two novel perceptron-inspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.
We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.
We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-the-art performance in machine translation.
We compare different algorithms for tuning the log-linear weights in a re-ranking framework and achieve results comparable to the standard minimum error rate training.
We present approaches to re-rank the output of the decoder using syntactic information.

379
A Language Modeling Approach To Predicting Reading Difficulty
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data.
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.
We show that with minimal changes, the classifier may be retrained for use with French Web documents.
For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets.
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).
We use a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.

380
Shallow Semantic Parsing Using Support Vector Machines
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.
Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.
We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.
We first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system.

381
Improvements In Phrase-Based Statistical Machine Translation
In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.
We describe the baseline phrase-based translation system and various refinements.
We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length.
We present translation results for three tasks: Verb-mobil, Xerox and the Canadian Hansards.
For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words.
The translation results for the Xerox and Canadian Hansards task are very promising.
The system even outperforms the alignment template system.
In our approach smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table.

382
What's In A Translation Rule?
We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora.
We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.
We describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and we describe probability estimators for those rules.
Our translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.

383
Automatically Labeling Semantic Classes
Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.
The current state of the art discovers many semantic classes but fails to label their concepts.
We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach.
The relationships automatically learned in our system include appositions, nominal subjects, such as relationships, and like relationships.
Our syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space.
Given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels.

384
Accurate Information Extraction From Research Papers Using Conditional Random Fields
With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.
This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.
The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.
This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order.
On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.
Accuracy compares even more favorably against HMMs.
CORA consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher.

385
Name Tagging With Word Clusters And Discriminative Training
We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus.
Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model.
Active learning is used to select training examples.
We evaluate the technique for named-entity tagging.
Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance.
Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.
We use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.
We use the Brown algorithm for clustering (Brown et al 1992).

386
WordNet::Similarity - Measuring The Relatedness Of Concepts
WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets).
It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet.
These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.
Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package.
The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn.

387
Morphological Analysis For Statistical Machine Translation
We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities.
The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into the sequence of prefix(es)-stem-suffix(es) and part-of-speech tagging of the parallel corpus.
The algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry.
The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs.
We show that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality.

388
A Unigram Orientation Model For Statistical Machine Translation
In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.
The segmentation model uses a novel orientation component to handle swapping of neighbor blocks.
During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block.
The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model.
We show experimental results on a standard Arabic-English translation task.
This work introduces lexical features for distortion modeling.

389
Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks
To date, there are no fully automated systems addressing the community's need for fundamental language processing tools for Arabic text.
In this paper, we present a Support Vector Machine (SVM) based approach to automatically tokenize (segmenting off clitics), part-of- speech (POS) tag and annotate base phrases (BPs) in Arabic text.
We adapt highly accurate tools that have been developed for English text and apply them to Arabic text.
Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but this work has collapsed the tag set to simplify tagging.
We describe a part-of-speech tagger based on support vector machines that is trained on tokenized data (clitics are separate tokens), reporting a tagging accuracy of 95.5%.

390
Improved Statistical Machine Translation Using Paraphrases
Parallel corpora are crucial for training SMT systems.
However, for many language pairs they are available only in very limited quantities.
For these language pairs a huge portion of phrases encountered at run-time will be unknown.
We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases.
Our results show that augmenting a state-of-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality.
For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.
We propose a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence and then proceeds to use the translation of that paraphrase in the production of the target-language result.

391
Learning To Recognize Features Of Valid Textual Entailments
This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.
Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.
We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.
Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.
We emphasize that there is more to inferential validity than close lexical or structural correspondence: negations, models, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment.

392
Named Entity Transliteration And Discovery From Multilingual Comparable Corpora
Named Entity recognition (NER) is an important part of many natural language processing tasks.
Most current approaches employ machine learning techniques and require supervised data.
However, many languages lack such resources.
This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.
We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
Character unigrams and bigrams are used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model.

393
Alignment By Agreement
We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.
Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER.
Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
We use discriminative SMT on large training data, training 1.5 million features on 67,000 sentences.

394
Effective Self-Training For Parsing
We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data.
We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.
Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
Finally, we provide some analysis to better understand the phenomenon.
We presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model.

395
Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution
In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.
These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
We show that including predicate argument pairs as features improved the performance of a coreference resolver.
We train a model for classifying whether two mentions are co-referring or not.
We suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet.

396
Synchronous Binarization For Machine Translation
Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive.
The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large.
We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.
Synchronous binarization simultaneously binarizes both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible.

397
Preemptive Information Extraction Using Unrestricted Relation Discovery
We are trying to extend the boundary of Information Extraction (IE) systems.
Existing IE systems require a lot of time and human effort to tune for a new scenario.
Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention.
We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables.
We present a preliminary system that obtains reasonably good results.
We apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs.
We rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations.
Preemptive IE is a paradigm that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters.

398
Prototype-Driven Learning For Sequence Models
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label.
This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
We also compare to semi-supervised learning and discuss the system's error trends.
Prototype-driven learning (PDL) optimizes the joint marginal likelihood of data labeled with prototype input features for each label.
We ask the user to suggest a few prototypes (examples) for each class and use those as features.

399
Learning For Semantic Parsing With Statistical Machine Translation
We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence.
A semantic parser is learned given a set of sentences annotated with their correct meaning representations.
The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques.
A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model.
We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
We use the maximum-entropy model, which defines a conditional probability distribution over derivations given an observed NL sentence.

400
Paraphrasing For Automatic Evaluation
This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
We apply our paraphrasing method in the context of machine translation evaluation.
Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.
We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
We show that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy.

401
Arabic Preprocessing Schemes For Statistical Machine Translation
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
Our results show that given large amounts of training data, splitting off only proclitics performs best.
However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.
Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
We show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.

402
OntoNotes: The 90% Solution
We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement.
An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.
Ontonotes includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc.
In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement.

403
Parser Combination By Reparsing
We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers.
We introduce a threshold for the constituent count and search for the tree with the largest count number from all the possible constituent combinations.
We combine five parsers to obtain a score of 92.1, whereas the best single parser obtains a score of 91.0.

404
First-Order Probabilistic Models for Coreference Resolution
Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.
In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference.
We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases.
This result demonstrates an example of how a first-order logic representation can be incorporated into a probabilistic model and scaled efficiently.
We present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not.
We introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities.

405
Bayesian Inference for PCFGs via Markov Chain Monte Carlo
This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm.
We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.
We describe Gibbs samplers for Bayesian inference of PCFG rule probabilities.
We introduce adaptor grammars, a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree.

406
Lexicalized Markov Grammars for Sentence Compression
We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000).
We define a head-driven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions.
We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora.
Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.

407
Combining Outputs from Multiple Machine Translation Systems
Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based.
These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge.
The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems.
This paper describes three different approaches to MT system combination.
These combination methods operate on sentence, phrase and word level exploiting information from N-best lists, system scores and target-to-source phrase alignments.
The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.
We use minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network.
We collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations.

408
Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming
Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments.
This joint ILP formulation provides f-score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
By using joint inference for anaphoricity and coreference, we avoid cascade-induced errors without the need to separately optimize the threshold.

409
Multiple Aspect Ranking Using the Good Grief Algorithm
We address the problem of analyzing multiple related opinions in a text.
For instance, in a restaurant review such opinions may include food, ambience and service.
We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect.
We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.
This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast.
We prove that our agreement-based joint model is more expressive than individual ranking models.
Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.
We combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification.

410
Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion
Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes.
Typically, the alignments are limited to one-to-one alignments.
We present a novel technique of training with many-to-many alignments.
A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists.
We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.
The many-to-many alignments result in significant improvements over the traditional one-to-one approach.
Our system achieves state-of-the-art performance on several languages and data sets.
The M2M-aligner is based on the expectation maximization (EM) algorithm.
M2M-aligner is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes.

411
Improved Inference for Unlexicalized Parsing
We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs.
First, we present a novel coarse-to-fine method in which a grammar's own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank.
In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy.
Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs.
Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.
An approach found to be effective for coarse-to-fine parsing is to use likelihood-based hierarchical EM training.

412
ISP: Learning Inferential Selectional Preferences
Semantic inference is a key component for advanced natural language understanding.
However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.
This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences.
We evaluate ISP and present empirical evidence of its effectiveness.
Context-sensitive extensions of DIRT focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule.
We build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, we use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes.
We augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy.

413
TextRunner: Open Information Extraction on the Web
Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora.
In contrast, the TEXTRUNNER system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input.
(Banko et al., 2007) TEXTRUNNER is a fully-implemented, highly scalable example of OIE.
TEXTRUNNER's extractions are indexed, allowing a fast query mechanism.
Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages.
It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism.
And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions.
We have built a faster user interface for querying the results.
We provide an online demo of TextRunner.

414
A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches
This paper presents and compares WordNet-based and distributional similarity approaches.
The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.
Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.
Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.
We derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs.
Examining the relations between the words in each pair, we further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.

415
Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction
We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.
This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.
We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.
We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.
We see our largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing.

416
Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing
Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.
Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.
In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.
Our model produces state-of-the-art results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.
We use the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved.
We also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.

417
11001 New Features for Statistical Machine Translation
We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system.
On a large-scale Chinese-English translation task, we obtain statistically significant improvements of +1.5 Bleu and +1.1 Bleu, respectively.
We analyze the impact of the new features and the performance of the learning algorithm.
We only use 100 most frequent words for word context feature.
We introduce the features for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals.

418
Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages
We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.
Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.
We show that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data.
On Web text, we report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.

419
Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars
One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities.
Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models.
This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task.
With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus.
We find that the adaptor grammar with syllable structure phontactic constraints and three levels of collocational structure yields the highest word segmentation token f-score.

420
Joint Parsing and Named Entity Recognition
For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser).
This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system.
We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.
Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.
The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of up to 1.36% absolute F1 for parsing, and up to 9.0% F1 for named entity recognition.
While performing named entity recognition jointly with constituency parsing shows improvement in performance on both tasks, the only aspect of the sytnax which is leveraged by the NER component is the location of noun phrases.

421
Exploring Content Models for Multi-Document Summarization
We present an exploration of generative probabilistic models for multi-document summarization.
Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.
Our final model, HIERSUM, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.
At the task of producing generic DUC-style summaries, HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system.
We also explore HIERSUM's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.
In TOPICSUM, each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster.
We build a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt.

422
Using a maximum entropy model to build segmentation lattices for MT
Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries.
However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices.
In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding.
Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-of-the-art baselines.
We find that unigram weights are an effective feature in our segmentation model.

423
Semantic Roles for SMT: A Hybrid Two-Pass Model
We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.
The approach avoids major complexity limitations via a two-pass architecture.
The first pass is performed using a conventional phrase-based SMT model.
The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels.
Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline – to our knowledge, the first successful application of semantic role labeling to SMT.
We perform semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation.

424
Multi-Prototype Vector-Space Models of Word Meaning
Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word.
However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.
This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy.
Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
We introduce a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.

425
Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing
We present results from a range of experiments on article and preposition error correction for non-native speakers of English.
We first compare a language model and error-specific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction.
We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the meta-classifier.
The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain.
The meta-classification approach results in substantial gains over the classifier-only and language-model-only scenario.
Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier.
All evaluations are conducted on a large error-annotated corpus of learner English.
We remove sentences from the data where some other error appears immediately next to a preposition or determiner error.

426
Unsupervised Modeling of Twitter Conversations
We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain.
Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances.
Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium.
We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task.
This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available.
This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium.
We propose an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data.
We limit our dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations.
We propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts.
Under the block HMM, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model.

427
For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia
We report on work in progress on extracting lexical simplifications (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task.
We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.
We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
We learn lexical simplifications without taking syntactic context into account.

428
Coreference Resolution in a Modular Entity-Centered Model
Coreference resolution is governed by syntactic, semantic, and discourse constraints.
We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner.
Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions.
By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.

429
Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment
The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).
One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.
We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.
We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.
Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.
We report significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model.

430
An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner.
Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built.
A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right.
In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step.
This allows incorporation of features from already built structures both to the left and to the right of the attachment point.
The parser learns both the attachment preferences and the order in which they should be performed.
The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.
We observe that parsing time is dominated by feature extraction and score calculation.

431
The viability of web-derived polarity lexicons
We examine the viability of building large polarity lexicons semi-automatically from the web.
We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; BlairGoldensohn et al., 2008; Rao and Ravichandran, 2009).
We then apply this technique to build an English lexicon that is significantly larger than those previously studied.
Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems.
As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc.
We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet.
We construct a graph where the nodes are 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries.

432
Batch Tuning Strategies for Statistical Machine Translation
There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach.
We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM.
We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings.
Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.

433
Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure
It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.
While previous work has focused primarily on English, we extend these results to other languages along two dimensions.
First, we show that these results hold true for a number of languages across families.
Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.
Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.
When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.

434
Better Evaluation for Grammatical Error Correction
We present a novel method for evaluating grammatical error correction.
The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation.
This optimal edit sequence is subsequently scored using F1 measure.
We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.
We propose an alternative evaluation scheme which operates in terms of tokens rather than character offsets.

435
Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters
We consider the problem of part-of-speech tagging for informal, online conversational text.
We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy.
With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute).
Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre.
Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines.
Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the CMU Twitter Part-of-Speech Tagger and annotated data.

436
Linguistic Regularities in Continuous Space Word Representations
Continuous space language models have recently demonstrated outstanding results across a variety of tasks.
In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.
We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.
This allows vector-oriented reasoning based on the offsets between words.
For example, the male/female relationship is automatically learned, and with the induced vector representations, "King - Man + Woman" results in a vector very close to "Queen".
We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.
We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.
Remarkably, this method outperforms the best previous systems.
We reach top accuracy on the syntactic subset (an syn) with a CBOW predict model.

437
Robust Temporal Processing Of News
We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news.
The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (F-measure) against hand-annotated data.
Some initial steps towards tagging event chronologies are also described.
The main part of the system is a temporal expression tagger that employs finite state transducers based on hand-written rules.
We work on news and introduce an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article.
We attribute over half the errors of our baseline method to propagation of an incorrect event time to neighboring events.

438
Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking
This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.
Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.
Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
We use an ensemble based on bagging and partitioning for active learning for base NP chunking.

439
Minimally Supervised Morphological Analysis By Multimodal Alignment
This paper presents a corpus-based algorithm capable of inducing inflectional morphological analyses of both regular and highly irregular forms (such as brought→bring) from distributional patterns in large monolingual text with no direct supervision.
The algorithm combines four original alignment models based on relative corpus frequency, contextual similarity, weighted string similarity and incrementally retrained inflectional transduction probabilities.
Starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations, accuracy of the induced analyses of 3888 past-tense test cases in English exceeds 99.2% for the set, with currently over 80% accuracy on the most highly irregular forms and 99.7% accuracy on forms exhibiting non-concatenative suffixation.
We obtain outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech.
We propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).
The supervised morphological learner presented in this paper models lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes.

440
An Improved Error Model For Noisy Channel Spelling Correction
The noisy channel model has been applied to a wide range of problems, including spelling correction.
These models consist of two components: a source model and a channel model.
Very little research has gone into improving the channel model for spelling correction.
This paper describes a new channel model for spelling correction, based on generic string to string edits.
Using this model gives significant performance improvements compared to previously proposed models.
We present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions.
We show that adding a source language model increases the accuracy significantly.
We characterise the error model by computing the product of operation probabilities on slice-by-slice string edits.
We introduce a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred.

441
Headline Generation Based On Statistical Translation
Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.
A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation.
The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.
This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus.
We approximate the length distribution with a Gaussian.
We draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization.

442
Improved Statistical Alignment Models
In this paper, we present and compare various single-word based alignment models for statistical machine translation.
We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.
We present different methods to combine alignments.
As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
The Alignment Error Rate (AER) measures the fraction of links by which the automatic alignment differs from the reference alignment.

443
Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar
We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
We extract a stochastic tree-insertion grammar or STIG from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences of 40 words.

444
Automatic Labeling Of Semantic Roles
We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.
Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.
We split the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers.
We present a system that uses completely syntactic features to classify the Frame Elements in a sentence in the FrameNet database.

445
The Structure And Performance Of An Open-Domain Question Answering System
This paper presents the architecture, operation and results obtained with the LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU.
To find answers, the system relies on a combination of syntactic and semantic techniques.
The search for the answer is based on a novel form of indexing called paragraph indexing.
A score of 55.5% for short answers and 64.5% for long answers was achieved at the TREC-8 competition.
We transform a natural language question into an IR query.
We select as keywords all named entities that were recognized as proper nouns.

446
Scaling To Very Very Large Corpora For Natural Language Disambiguation
The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.
Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.
In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.
We are fortunate that for this particular application, correctly labeled training data is free.
Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.
We suggest that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.
We show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words.

447
Extracting Paraphrases From A Parallel Corpus
While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases.
We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
We incorporate part-of-speech information and other morphosyntactic clues into our co-training algorithm.

448
Immediate-Head Parsing For Language Models
We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c.
While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.
The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model.
For the better of our two models these improvements are 24% and 14% respectively.
We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.
The model presented identifies both syntactic structural and lexical dependencies that aid in language modeling.
These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent.

449
An Algebra For Semantic Construction In Constraint-Based Grammars
We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG.
The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification- based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.
The semantic interpretations are expressed using Minimal Recursion Semantics (MRS), which provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers.
An MRS consists of a bag of labeled elementary predicates and their arguments, a list of scoping constraints, and a pair of relations that provide a hook into the representation - a label, which must outscope all the handles, and an index.

450
Methods For The Qualitative Evaluation Of Lexical Association Measures
This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora.
In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified "true positives".
We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples.
We extract German PP-verb combinations from a chunk-parsed version of the Frankfurter Rundschau Corpus.
We use four collocation measures: Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus.
The t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies.

451
Fast Decoding And Optimal Decoding For Machine Translation
A good decoding algorithm is critical to the success of any statistical machine translation system.
The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).
Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.
In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.
We compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem.

452
A Statistical Model For Domain-Independent Text Segmentation
We propose a statistical method that finds the maximum-probability segmentation of a given text.
This method does not require training data because it estimates probabilities from the given text.
Therefore, it can be applied to any text in any domain.
An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.
We model the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm.
We introduce one of the first probabilistic approaches using Dynamic Programming (DP) called U00.

453
A Syntax-Based Statistical Translation Model
We present a syntax-based statistical translation model.
Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node.
These operations capture linguistic differences such as word order and case marking.
Model parameters are estimated in polynomial time using an EM algorithm.
The model produces word alignments that are better than those produced by IBM Model 5.
We use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string.
We present an algorithm for estimating probabilistic parameters for a model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.

454
Parameter Estimation For Probabilistic Finite-State Transducers
Weighted finite-state transducers suffer from the lack of a training algorithm.
Training is even harder for transducers that have been assembled via finite-state operations such as composition, minimization, union, concatenation, and closure, as this yields tricky parameter tying.
We formulate a “parameterized FST” paradigm and give training algorithms for it, including a general bookkeeping trick (“expectation semirings”) that cleanly and efficiently computes expectations and gradients.
We use finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.
We claim that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs.
We give a general EM algorithm for parameter estimation in probabilistic finite-state transducers.
We describe the expectation semiring for parameter learning.

455
Learning Surface Text Patterns For A Question Answering System
In this paper we explore the power of surface text patterns for open-domain question answering systems.
In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.
A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.
Patterns are then automatically extracted from the returned documents and standardized.
We calculate the precision of each pattern, and the average precision for each question type.
These patterns are then applied to find answers to new questions.
Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.
We present an alternatve ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns.

456
Improving Machine Learning Approaches To Coreference Resolution
We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets - F-measures of 70.4 and 63.4, respectively.
Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.
In the testing phase, we used the best-first clustering.
We expand the feature set of Soon et al (2001) from 12 to 53 features.
We propose a rule-induction system with rule pruning.

457
A Generative Constituent-Context Model For Improved Grammar Induction
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.
Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus.
Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets.
We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.
We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.
We induce parts-of-speech from the full WSJ tree bank together with additional WSJ newswire.

458
A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents
This paper describes a simple pattern matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.
The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a gold standard corpus.
Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the pattern matching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
We propose an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing.
While Charniak's parser does not generate empty category information, we have developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output.
It is the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees.

459
Pronunciation Modeling For Improved Spelling Correction
This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction.
The proposed method builds an explicit error model for word pronunciations.
By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction.
We consider a pronunciation variation model to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary.
We extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.
We use the noisy channel model approach to determine the types and weights of edit operations.
Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, we derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations.

460
GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications
In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion.
The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated.
The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.
We include the ANNIE IE system in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging.
We propose mechanisms to help heterogeneous linguistic modules to communicate through a common XML interface.

461
The Necessity Of Parsing For Predicate Argument Recognition
Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.
In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter "chunked" representation of the input can be as effective for the purposes of semantic role identification.
We note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function.
We experiment with the set of features: Pred HW, Arg HW, Phrase Type, Position, Path, Voice.

462
An Unsupervised Method For Word Sense Tagging Using Parallel Corpora
We present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora.
The technique takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent, preserving some core element of its semantics, and yet also variable, reflecting differing translator preferences and the influence of context.
Working with parallel corpora introduces an extra complication for evaluation, since it is difficult to find a corpus that is both sense tagged and parallel with another language; therefore we use pseudo-translations, created by machine translation systems, in order to make possible the evaluation of the approach against a standard test set.
The results demonstrate that word-level translation correspondences are a valuable source of information for sense disambiguation.
We present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that are artificially created by applying commercial MT systems on a sense-tagged English corpus.
Cross-language tagging is the goal and we present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory.

463
New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron
This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.
We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.
We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.
Convolution kernels are used to implicitly define a tree substructure space.
The tree kernel is proposed for syntactic parsing reranking.
Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures.

464
Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank.
The model combines full and partial parsing techniques to reach full grammar coverage on unseen data.
The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models.
Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets.
On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score.
An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score.
We describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure.
XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model that works on the packed representations.

465
Discriminative Training And Maximum Entropy Models For Statistical Machine Translation
We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.
This approach allows a baseline machine translation system to be extended easily by adding new feature functions.
We show that a baseline statistical machine translation system is significantly improved using this approach.

466
A Decoder For Syntax-Based Statistical MT
This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).
The model has been extended to incorporate phrasal translations as presented here.
In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language.
As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary.
We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4.
We also discuss issues concerning the relation between this decoder and a language model.
We propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.

467
Bleu: A Method For Automatic Evaluation Of Machine Translation
Human evaluations of machine translation are extensive but expensive.
Human evaluations can take months to finish and involve human labor that can not be reused.
We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
BLEU is a system for automatic evaluation of machine translation.
BLEU is based on measuring string level similarity between the reference translation and translation hypothesis.

468
Building Deep Dependency Structures Using A Wide-Coverage CCG Parser
This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.
The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.
We provide examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables.
We define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments.

469
Generative Models For Statistical Parsing With Combinatory Categorial Grammar
This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser.
These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar.
In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies.
The CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing.
The dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.

470
Bootstrapping
This paper refines the analysis of co-training, defines and evaluates a new co-training algorithm that has theoretical justification, gives a theoretical justification for the Yarowsky algorithm, and shows that co-training and the Yarowsky algorithm are based on different independence assumptions.
We show that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption.
We refine Dasgupta et al's result by relaxing the view independence assumption with a new constraint.
We propose the Greedy Agreement Algorithm, which, based on two independent views of the data, learns two binary classifiers from a set of hand-typed seed rules.
We show that if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T.
We argue that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices.

471
An Unsupervised Approach To Recognizing Discourse Relations
We present an unsupervised approach to recognizing discourse relations of CONTRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts.
We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
We use a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora.
We propose a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus.

472
Evaluating Translational Correspondence Using Annotation Projection
Recently, statistical machine translation models have begun to take advantage of higher level linguistic structures such as syntactic dependencies.
Underlying these models is an assumption about the directness of translational correspondence between sentences in the two languages; however, the extent to which this assumption is valid and useful is not well understood.
In this paper, we present an empirical study that quantifies the degree to which syntactic dependencies are preserved when parses are projected directly from English to Chinese.
Our results show that although the direct correspondence assumption is often too restrictive, a small set of principled, elementary linguistic transformations can boost the quality of the projected Chinese parses by 76% relative to the unimproved baseline.
The dependency projection method DPA (Hwa et al, 2005) based on Direct Correspondence Assumption (Hwa et al, 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently.
We align the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees.

473
Translating Named Entities Using Monolingual And Bilingual Resources
Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries.
We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources.
We report on the application and evaluation of this algorithm in translating Arabic named entities to English.
We also compare our results with the results obtained from human translations and a commercial system for the same task.
We show that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.
A spelling-based model is described that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations.
The phonetics-based and spelling-based models have been linearly combined into a single transliteration model.
We use Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation.

474
Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews
This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down).
The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs.
A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”).
In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”.
A review is classified as recommended if the average semantic orientation of its phrases is positive.
The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations).
The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.
We describe a way to automatically build a lexicon based on looking at co-occurrences of words with other words whose sentiment is known.

475
Named Entity Recognition Using An HMM-Based Chunk Tagger
This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.
Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.
In this way, the NER problem can be resolved effectively.
Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.
It shows that the performance is significantly better than reported by any other machine-learning system.
Moreover, the performance is even consistently better than those based on handcrafted rules.
Our named entity recognition system recognizes various types of MUC-style named entities such as organization, location, person, date, time, money and percentage.

476
Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron
This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.
The first approach uses a boosting algorithm for ranking problems.
The second approach uses the voted perceptron algorithm.
Both algorithms give comparable, significant improvements over the maximum-entropy baseline.
The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.
We describe a mapping from words to word types which groups words with similar orthographic forms into classes.

477
Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked
Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns.
We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.
We evaluate our strategy on a challenging subset of questions, i.e. “Who is …” questions, against a state of the art web-based Question Answering system.
Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.
We use part of speech patterns to extract a subset of hyponym relations involving proper nouns.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise.

478
Using Predicate-Argument Structures For Information Extraction
In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures.
We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm.
It is based on: (1) an extended set of features; and (2) inductive decision tree learning.
The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.
We apply semantic parsing to capture the predicate-argument sentence structure.

479
A Noisy-Channel Approach To Question Answering
We introduce a probabilistic noisy-channel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.
Our noisy-channel system outperforms a state-of-the-art rule-based QA system that uses similar resources.
We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.
We have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations.
Another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data.

480
Fast Methods For Kernel-Based Text Analysis
Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).
In NLP, although feature combinations are crucial to improving performance, they are heuristically selected.
Kernel methods change this situation.
The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs.
Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.
In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier.
Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.
We propose polynomial kernel inverted (PKI).
PKI - Inverted Indexing, stores for each feature the support vectors in which it appears.
The PKE approach uses a basket mining approach to prune many features from the expansion.
An extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.

481
Clustering Polysemic Subcategorization Frame Distributions Semantically
Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.
In contrast to previous work, we particularly focus on clustering polysemic verbs.
A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data.
We evaluate hard clusterings based on a gold standard with multiple classes per verb.
We create a test set by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997).
Prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.

482
Reliable Measures For Aligning Japanese-English News Articles And Sentences
We have aligned Japanese and English news articles and sentences to make a large parallel corpus.
We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.
However, the results included many incorrect alignments.
To remove these, we propose two measures (scores) that evaluate the validity of alignments.
The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.
They enhance each other to improve the accuracy of alignment.
Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public.
We build an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs.
We use the BM25 similarity measure.

483
Loosely Tree-Based Alignment For Machine Translation
We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
We found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees.
We train a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments.
The "clone" operation allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment.

484
A Probability Model To Improve Word Alignment
Word alignment plays a crucial role in statistical machine translation.
Word-aligned corpora have been found to be an excellent source of translation-related knowledge.
We present a statistical model for computing the probability of an alignment given a sentence pair.
This model allows easy integration of context-specific features.
Our experiments show that this model can be an effective tool for improving an existing word alignment.
We propose a direct alignment formulation and argue that it would be straightforward to estimate the parameters given a supervised alignment corpus.
We use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner.

485
Probabilistic Parsing For German Using Sister-Head Dependencies
We present a probabilistic parsing model for German trained on the Negra treebank.
We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.
Learning curves show that this effect is not due to lack of training data.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.
This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
We show that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains.
We show that the task of assigning correct grammatical functions is harder than mere constituent-based parsing.

486
A Comparative Study On Reordering Constraints In Statistical Machine Translation
In statistical machine translation, the generation of a translation hypothesis is computationally expensive.
If arbitrary word-reorderings are permitted, the search problem is NP-hard.
On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.
In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.
This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.
We show a connection between the ITG constraints and the since 1870 known Schroder numbers.
We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task.
The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints.
Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.
The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.
Therefore, we present an extension to the ITG constraints.
These extended ITG constraints increase the alignment coverage from about 87% to 96%.
We show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus).
We introduce a normal form ITG which avoids over-counting.

487
Minimum Error Rate Training In Statistical Machine Translation
Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
In this paper, we analyze various training criteria which directly optimize translation quality.
These training criteria make use of recently proposed automatic evaluation metrics.
We describe a new algorithm for efficient training an unsmoothed error count.
We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.
In our model, feature weights are tuned with Minimum Error Rate Training (MERT) to maximize BLEU.

488
A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue
We apply a decision tree based approach to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP- and non-NP-antecedents.
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.
We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron's (2002) manually tuned system.
We aim at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data.

489
Coreference Resolution Using Competition Learning Approach
In this paper we propose a competition learning approach to coreference resolution.
Traditionally, supervised machine learning approaches adopt the single-candidate model.
Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.
By contrast, our approach adopts a twin-candidate learning model.
Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected.
Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution.
The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.
We make use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improve the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively.

490
An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition
Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction.
Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain.
The effect of these alternative models has not been previously studied.
In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees.
We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns.
Our method consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user.
We use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain.
We also propose representations for IE patterns which extends the SVO representation.

491
Improved Source-Channel Models For Chinese Word Segmentation
This paper presents a Chinese word segmentation system that uses improved source-channel models of Chinese sentence generation.
Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities.
Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition.
The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.

492
Counter-Training In Discovery Of Semantic Patterns
This paper presents a method for unsupervised discovery of semantic patterns.
Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction.
The method builds upon previously described approaches to iterative unsupervised pattern acquisition.
One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision.
Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously.
This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination.
We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure.
We develop Counter-Trainin for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time.
We use predicate-argument (SVO) model, which allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates.

493
Language Model Based Arabic Word Segmentation
We approximate Arabic’s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme).
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus.
The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input.
The language model is initially estimated from a small manually segmented corpus of about 110,000 words.
To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus.
The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens.
We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.
We demonstrate a technique for segmenting Arabic text and use it as a morphological processing step in machine translation.

494
Accurate Unlexicalized Parsing
We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.
Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art.
This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.
We also present a manual symbol refinement method.

495
Is It Harder To Parse Chinese Or The Chinese Treebank?
We present a detailed investigation of the challenges posed when applying parsing models developed against English corpora to Chinese.
We develop a factored-model statistical parser for the Penn Chinese Treebank, showing the implications of gross statistical differences between WSJ and Chinese Treebanks for the most general methods of parser adaptation.
We then provide a detailed analysis of the major sources of statistical parse errors for this corpus, showing their causes and relative frequencies, and show that while some types of errors are due to difficult ambiguities inherent in Chinese grammar, others arise due to treebank annotation practices.
We show how each type of error can be addressed with simple, targeted changes to the independence assumptions of the maximum likelihood-estimated PCFG factor of the parsing model, which raises our F1 from 80.7% to 82.6% on our development set, and achieves parse accuracy close to the best published figures for Chinese parsing.
We argue that a careful error classification can reveal possible improvements.
Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology.
There are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese.

496
Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
Our investigation reveals that this method of acquiring sense-tagged data is promising.
On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage.
Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs.
We address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus.
When several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.

497
Probabilistic Text Structuring: Experiments With Sentence Ordering
Ordering information is a critical task for natural language generation applications.
In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation.
We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives.
We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task.
We also assess the appropriateness of such a model for multi-document summarization.
We build a conditional model of words across adjacent sentences, focusing on words in particular semantic roles.
We proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences.
As the features, we propose the Cartesian product of content words in adjacent sentences.

498
Discourse Segmentation Of Multi-Party Conversation
We present a domain-independent topic segmentation algorithm for multi-party speech.
Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech.
This segmentation algorithm uses automatically induced decision rules to combine the different features.
The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information.
A significant error reduction is obtained by combining the two knowledge sources.
We provide gold standard for thematic segmentations by considering the agreement between at least three human annotations.
We propose the lexical chain based unsupervised segmenter (LCSeg) and a supervised segmenter for segmenting meeting transcripts.
Our LCseg system is the only word distribution based system evaluated on ICSI meeting data.

499
Automatic Error Detection In The Japanese Learners' English Spoken Data
This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.
In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners' errors.
We use error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.
In the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories.
The usage of articles has been found to be the most frequent error class in the JLE (Japanese Learner English) corpus.

500
Learning Non-Isomorphic Tree Mappings For Machine Translation
Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topology.
We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
We argue that if the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem.
We consider synchronous tree substitution grammar, a formalism that can account for structural mismatches, and is trained discriminatively.

501
A TAG-Based Noisy-Channel Model Of Speech Repairs
This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.
A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model.
The use of TAG is motivated by the intuition that the reparandum is a "rough copy" of the repair.
The model is trained and tested on the Switchboard disfluency-annotated corpus.
Noisy channel models do well on the disfluency detection task.
Although the standard noisy channel model performs well, a log linear re-ranker can be used to increase performance.
Our TAG system achieves a high EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations.

502
Discriminative Training Of A Neural Network Statistical Parser
Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.
One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.
We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.
We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.
The latter model outperforms the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).
We provide a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.
We use neural networks to induce latent left-corner parser states.
We find that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of the generative model.

503
Parsing The WSJ Using CCG And Log-Linear Models
This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG).
A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.
We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies.
We compare models which use all CCG derivations, including non-standard derivations, with normal-form models.
The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers.
Our CCG parser is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.
Our parsing peformance relies on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word).
Our parsing performance provides an indication of how super tagging accuracy corresponds to overall dependency recovery.

504
Incremental Parsing With The Perceptron Algorithm
This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.
A beam-search algorithm is used during both training and decoding phases of the method.
The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.
We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.
We propose an early update mechanism, where decoding is stopped to update model weights whenever the single gold action falls outside the beam.
The best results of our parser (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead.
The early-update strategy is used so as to improve accuracy and speed up the training.

505
A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree
This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes.
A Maximum Entropy model is used to rank these paths.
The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.
We also train a coreference system using the MUC6 data and competitive results are obtained.
We apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning.
We use a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree.
To cope with computational complexity, we heuristically search for the most probable partition by performing a beam search through a Bell tree.
We show that one can obtain a very high MUC score simply by lumping all mentions together.
We applied the ANY predicate to generate cluster-level features for their entity-mention model, which did not perform as well as the mention-pair model.

506
A Joint Source-Channel Model For Machine Transliteration
Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents.
The transliteration is usually achieved through intermediate phonemic mapping.
This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM).
With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary.
The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms.
The modeling framework is validated through several experiments for English-Chinese language pair.
We find that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes.
Our grapheme-based approach, which treats transliteration as statistical machine translation problem under monotonic constraint, aims to obtain a direct orthographical mapping (DOM) to reduce possible errors introduced in multiple conversions.
Phoneme-based approaches are usually not good enough, because name entities have various etymological origins and transliterations are not always decided by pronunciations.
Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin.
Our direct orthographic mapping, making use of individual Chinese graphemes, tends to overcome the problem and model the character choice directly.

507
A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts
Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down".
To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document.
Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.
We created a movie-review dataset for opinion detection.
We argue that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter.
We show that sentence level classification can improve document level analysis.
In our subjectivity detection method, soft local consistency constraints are created between every sentence in a document and inference is solved using a min-cut algorithm.

508
Finding Predominant Word Senses In Untagged Text
In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.
The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data.
Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.
The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task.
This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.
Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora.
The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.
We restrict the word sense disambiguation task to determining predominant sense in a given domain.

509
Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations
This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.
Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees.
Currently our best automatically induced grammars achieve 80.97% f-score for f-structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004).
Our f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English uses configurational, categorial, function tag and trace information.
We automatically map c-structures to f-structures by assigning grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head.
Our parser automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates.

510
A Study On Convolution Kernels For Shallow Statistic Parsing
In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.
Their main property is the ability to process structured representations.
Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify PropBank predicate arguments with accuracy higher than the current argument classification state-of-the-art.
Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.
We can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure.
Our convolution kernel is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities.

511
Discovering Relations Among Named Entities From Large Corpora
Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization.
Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort.
We propose an unsupervised method for relation discovery from large corpora.
The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities.
Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations.
We introduce a fully unsupervised Open IE systems, based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations.
We use large corpora and an Extended Named Entity tagger to find novel relations and their participants.

512
Dependency Tree Kernels For Relation Extraction
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.
We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a bag-of-words kernel.
To compare relations in two instance sentences, we propose to compare the subtrees induced by the relation arguments i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree.
We also use a dependency tree kernel to detect the Named Entity classes in natural language texts.

513
Collective Information Extraction With Relational Markov Networks
Most information extraction (IE) systems treat separate potential extractions as independent.
However, in many cases, considering influences between different potential extractions could improve overall accuracy.
Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems.
We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.
This allows for "collective information extraction" that exploits the mutual influence between possible extractions.
Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.
We present AImed, a corpus for the evaluation of PPI extraction systems.

514
Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency
We present a generative model for the unsupervised learning of dependency structures.
We also describe the multiplicative combination of this dependency model with a model of linear constituency.
The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.
We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.
Our contributions include the generative Dependency Model with Valence (DMV).
We argue that consistent syntactic representations are desirable in the evaluation of unsupervised syntactic parsers.

515
Improving IBM Word Alignment Model 1
We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.
We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
A limitation of IBM Model 1 is that each word in the target sentence can be generated by at most one word in the source sentence.
We also suggested adding multiple empty words to the target sentence for IBM Model 1.
Our method also alleviates another related limitation by enabling translation between contiguous words across the query and documents.

516
Multi-Criteria-Based Active Learning For Named Entity Recognition
In this paper, we propose a multi-criteria-based active learning approach and effectively apply it to named entity recognition.
Active learning targets to minimize the human annotation efforts by selecting examples for labeling.
To maximize the contribution of the selected examples, we consider the multiple criteria: informativeness, representativeness and diversity and propose measures to quantify them.
More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method.
The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance.
We consider Active Learning (AL) for entity recognition based on Support Vector Machines.
Our diversity-motivated intra-stratum sampling scheme considers K-diverse neighbors and aims to maximize the training utility of all seeds from a stratum.

517
Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics
In this paper we describe two new objective automatic evaluation methods for machine translation.
The first method is based on longest common subsequence between a candidate translation and a set of reference translations.
Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically.
The second method relaxes strict n-gram matching to skip-bigram matching.
Skip-bigram is any pair of words in their sentence order.
Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency.
We experimented with a wide set of metrics, including NIST, WER (Niefen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM.

518
Statistical Machine Translation By Parsing
In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings.
This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples.
Such algorithms can infer the synchronous structures hidden in parallel texts.
It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system.
When a parser's grammar can have fewer dimensions than the parser's input, we call it a synchronizer.
We formalize machine translation problem as synchronous parsing based on multi text grammars.

519
Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.
We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.
Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.
We define an adjacent pair to consist of two parts that are ordered, adjacent, and produced by different speakers.
We also achieved an 8% increase in speaker identification.
We consider speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network.
We suggest that further gains can be achieved by augmenting the feature set.

520
Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction
Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules.
We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation.
Here we present our general approach and describe our ACE results.
We use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs.
We obtain improvement in results when we combine a variety of features.
We achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.

521
A High-Performance Semi-Supervised Learning Method For Text Chunking
In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning.
The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).
We utilize a multi task learner within our semi-supervised algorithm to learn feature representations which are useful across a large number of related tasks.
Our structural learning method uses alternating structural optimization (ASO).
For both computational and statistical reasons, we follow compute a low-dimensional linear approximation to the pivot predictor space.

522
Probabilistic CFG With Latent Annotations
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.
This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables.
Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.
Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.
We use a markovized grammar to get a better unannotated parse forest during decoding, but we do not markovize the training data.
We right-binarize the tree bank data to construct grammars with only unary and binary productions.

523
Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing
This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing.
The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars.
Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.
A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.
Our HPSG parser computes deeper analyses, such as predicate argument structures.
We also introduce a hybrid model, where the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, to help the process of estimation by filtering unlikely lexical entries.

524
Online Large-Margin Training Of Dependency Parsers
We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).
The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.
We have achieved parsers with O(n3) time complexity without the grammar constant.
We use the prefix of each word form instead of word form itself as features.
Our dependency parser achieves accuracy as good as Charniak (2000) with speed ten times faster than Collins (1997) and four times faster than Charniak (2000).

525
Pseudo-Projective Dependency Parsing
In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.
We show how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.
Experiments using data from the Prague Dependency Treebank show that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.
This leads to the best reported performance for robust non-projective parsing of Czech.
In our pseudo-projective approach, non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time.
We show how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing.
For handling non-projective relations, we suggest applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.
We note that since the number of non-projective dependencies is much smaller than the number of projective dependencies, it is not efficient to perform non-projective parsing for all cases.

526
Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales
We address the rating-inference problem, wherein rather than simply decide whether a review is thumbs up or thumbs down, as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five "stars").
This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, three stars is intuitively closer to four stars than to one star.
We first evaluate human performance at the task.
Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels.
We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.
We created a sentiment-annotated dataset consisting of movie reviews to train a classifier for identifying positive sentences in a full length review.

527
Extracting Semantic Orientations Of Words Using Spin Model
We propose a method for extracting semantic orientations of words: desirable or undesirable.
Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function.
We also propose a criterion for parameter selection on the basis of magnetization.
Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon.
The result is comparable to the best value ever reported.
We build lexical network from not only co-occurrence but other resources including thesaurus.
We determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration.

528
Modeling Local Coherence: An Entity-Based Approach
This paper considers the problem of automatic assessment of local coherence.
We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text.
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.
Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.
To further refine the computation of the subsequence distribution, we divide the matrix into a salient matrix and a non-salient matrix.
We show that our entity based model is able to distinguish a source text from its permutation accurately.
We exploit the use of the distributional and referential information of discourse entities to improve summary coherence.

529
Machine Learning For Coreference Resolution: From Local Classification To Global Ranking
In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems.
We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.
Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.
We emphasize the global optimization of ranking clusters obtained locally.
We cast coreference resolution as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions.
Our method ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents.
Most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions.

530
Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking
Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).
A discriminative reranker requires a source of candidate parses for each sentence.
This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).
This method generates 50-best lists that are of substantially higher quality than previously obtainable.
We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
We use pruning, where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.
We show accuracy improvements from composed local tree features on top of a lexicalized base parser.
To improve performance and robustness, features are pruned so that selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times.

531
A Hierarchical Phrase-Based Model For Statistical Machine Translation
We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases.
The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment.
In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.
We use the k-best parsing algorithm in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.
We note that whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items.
To better leverage syntactic constraint yet still allow non-syntactic translations, we introduce a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side.
Our hierarchical phrase models for machine translation is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models.

532
Dependency Treelet Translation: Syntactically Informed Phrasal SMT
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.
This method requires a source language dependency parser, target language word segmentation and an unsupervised word alignment component.
We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
Our treelet-based SMT system is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications.
We extend paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs.
We demonstrate the success of using fragments of a target language's grammar, treelets, to improve performance in phrasal translation.

533
Supervised And Unsupervised Learning For Sentence Compression
In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.
More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.
Finally, we point out problems with modeling the task in this way.
They suggest areas for future research.
We approximate the rules of compression from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions.
We argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones.
We show that applying handcrafted rules for trimming sentences can improve both content and linguistic quality.

534
Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003).
CRFs are log-linear, allowing the incorporation of arbitrary features into the model.
To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist.
We describe a novel approach, contrastive estimation.
We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient.
Applied to a sequence labeling problem - POS tagging given a tagging dictionary and unlabeled text - contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
To define an approximation to the partition function for log-linear estimation, our contrastive estimation uses a technique to generate local neighborhoods of a parses.
We generate negative evidence for the contrastive estimation method by moving or removing a word in a sentence.
We define coarse grained POS tags on PTB.
We initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data.
We attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier.
We show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods.
The contrastive estimation technique we propose is globally normalized and thus capable of dealing with arbitrary features.

535
Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling
Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use.
We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models.
By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.
We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.
This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.

536
A Semantic Approach To IE Pattern Induction
This paper presents a novel algorithm for the acquisition of Information Extraction patterns.
The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant.
Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity.
Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.
We propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns.
We use subject-verb-object triples for the features.

537
Extracting Relations With Integrated Information Using Kernel Methods
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.
This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.
Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.
Each source of information is represented by kernel functions.
Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels.
We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.
When evaluated on the official test data, our approach produced very competitive ACE value scores.
We also compare the SVM with KNN on different kernels.
We define several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.
We show that adding local information to deep syntactic information improved IE results.
We extract bigram of the words between the two mentions, aiming to provide more order information of the tokens between the two mentions.

538
Exploring Various Knowledge In Relation Extraction
Extracting semantic relationships between entities is challenging.
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.
Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.
This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.
We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.
Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.
We show that it is difficult to extract new effective features to further improve the extraction accuracy.
We use a set of flat features (i.e. word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information).

539
Log-Linear Models For Word Alignment
We present a framework for word alignment based on log-linear models.
All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible additional variables.
Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
Our experiments show that log-linear models significantly outperform IBM translation models.
We present a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment.

540
Stochastic Lexicalized Inversion Transduction Grammar For Alignment
We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
We present a model in which the nonterminals are lexicalized by English and foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules.
We propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans.
Tic-tac-toe pruning algorithm uses dynamic programming to compute inside and outside scores for a span pair in O (n4).

541
Reading Level Assessment Using Support Vector Machines And Statistical Language Models
Reading proficiency is a fundamental component of language competency.
However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.
This task can be addressed with natural language processing technology to assess reading level.
Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.
In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.
We develop a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability.
We use syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.

542
Clause Restructuring For Statistical Machine Translation
We describe a method for incorporating syntactic information in statistical machine translation systems.
The first step of the method is to parse the source language string that is being translated.
The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.
The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string.
The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system.
We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.
We present sign test to measure the siginificance of score improvement in BLUE.
We note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead.
We use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation.

543
Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.
Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
We first introduce our approach to inducing such a grammar from parallel corpora.
Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.
We introduce a polynomial time decoding algorithm for the model.
We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software.
The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.
Our approach requires some assumptions on the level of isomorphism (lexical and/or structural) between two languages.
We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.

544
Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process.
We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.
We obtain accuracy rates on all tasks in the high nineties.
For choosing the best Buckwalter morphological analyzer (BAMA) results, we simply count the number of predicted values for the set of linguistic features in each candidate analysis.

545
Semantic Role Labeling Using Different Syntactic Views
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.
In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.
We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.
Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument.
In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.
All of the reported techniques resulted in performance improvements.
We combine systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing.
We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance.
We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.

546
Joint Learning Improves Semantic Role Labeling
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.
This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.
This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.
We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.
We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.

547
Paraphrasing With Bilingual Parallel Corpora
Previous work has used monolingual parallel corpora to extract and generate paraphrases.
We show that this task can be done using bilingual parallel corpora, a much more commonly available resource.
Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.
We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.
We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.
We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.

548
Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering
In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.
We apply these algorithms to generate noun similarity lists from 70 million pages.
We reduce the running time from quadratic to practically linear in the number of elements to be computed.
We show that by using the LSH nearest neighbors calculation can be done in O(nd) time.
Our method can produce over 70% accuracy in extracting synonyms.

549
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification
Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative.
Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.
This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.
In sentiment analysis research, we use emoticons in newsgroup articles to extract instances relevant for training polarity classifiers.
We find that when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state.

550
Multi-Engine Machine Translation Guided By Explicit Word Matching
We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input.
The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality.
Our approach uses the individual MT engines as "black boxes" and does not require any explicit cooperation from the original MT systems.
A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines.
The highest scoring sentence hypothesis is selected as the final output of our system.
Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output.
We propose a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses.

551
Minimum Cut Model For Spoken Lecture Segmentation
We consider the task of unsupervised lecture segmentation.
We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.
Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies.
Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
We optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.
Our problem is to find topical boundaries in transcripts of course lectures.
We create a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.

552
Bootstrapping Path-Based Pronoun Resolution
We present an approach to pronoun resolution based on syntactic paths.
Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities.
This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints.
Highly coreferent paths also allow mining of precise probabilistic gender/number information.
We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.
Significant gains in performance are observed on several datasets.
Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.
We show that learned gender is the most important feature in their pronoun resolution systems.
We achieve achieve state-of-the-art noun gender classification performance, and we make the database of the obtained noun genders available online.
We build a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively.

553
Discriminative Word Alignment With Conditional Random Fields
In this paper we present a novel approach for inducing word alignments from sentence aligned data.
We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set.
The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data.
Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions.
We apply this alignment model to both French-English and Romanian-English language pairs.
We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.

554
Named Entity Transliteration With Comparable Corpora
In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other.
We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.
Each of these approaches works quite well, but by combining the approaches one can achieve even better results.
We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs.
This propagation method achieves further improvement over the best results from the previous step.
We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.

555
Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora
We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.
By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not.
This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.
We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
We first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words.
We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon.
We use standard information retrieval together with simple word-based translation for cross-lingual information retrieval (CLIR), and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter.
We perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction.

556
Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance
Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation.
In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English.
We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.
In our coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses.
We present an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary.
We argue that automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations.

557
Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations
In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations.
The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm.
We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations.
Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision.
In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far.
We induce specific reliable patterns in a bootstrapping manner for entity relation extraction.
Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).

558
Correcting ESL Errors Using Phrasal SMT Techniques
This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL).
Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers.
Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.
We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this data-intensive SMT approach is very promising, but we also point out SMT approach relies on the availability of large amount of training data.

559
Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words
We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning.
We utilize meta-patterns of high-frequency words and content words in order to discover pattern candidates.
Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.
Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.
We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation.
Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.
We show that pairs of words that often appear together in symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics).

560
Reranking And Self-Training For Parser Adaptation
Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years.
Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data.
This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres.
Such worries have merit.
The standard "Charniak parser" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus.
This paper should allay these fears.
In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.
Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data.
We successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation.

561
Learning Accurate Compact And Interpretable Tree Annotation
We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.
Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals.
In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.
We use hierarchical EM training.
We show that in the domain of syntactic parsing with probabilistic context-free grammars, automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure.
We introduce split-merge-smooth estimation.

562
Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
In our experiments on Chinese-to-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
We improve the reordering model for SMT based on the collocated words crossing the neighboring components.
We propose a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent.
In our maximum entropy-based reordering model, MEBTG, three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule.

563
Distortion Models For Statistical Machine Translation
In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a class for each possible jump length.
We find that deterministic word reordering is beyond the scope of optimization and cannot be undone by the decoder.

564
Annealing Structural Bias In Multilingual Weighted Grammar Induction
We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
Next, by annealing the free parameter that controls this bias, we achieve further improvements.
We then describe an alternative kind of structural bias, toward "broken" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.
We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date.
Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.
We penalize the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.
We propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.
Our annealing approach tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step.

565
Tree-To-String Alignment Template For Statistical Machine Translation
We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string.
A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.
The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts.
To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.
Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.
We perform derivation-level combination for mixing different types of translation rules within one derivation.
We also add non-syntactic PBSMT - phrase-based statistical machine translation - phrases into our tree-to-string translation system.

566
An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation
Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text.
When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied.
This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways.
We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.
We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation.
We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step.
Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets.
Our method is applicable to other languages with affix morphology.
We provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties.
We present a lattice-based modification of the BaumWelch algorithm to handle the segmentation ambiguity.

567
Contextual Dependencies In Unsupervised Word Segmentation
Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
We also show that previous probabilistic models rely crucially on sub-optimal search procedures.
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level.
We use hierarchical Dirichlet processes (HDP) to induce contextual word models.

568
A Discriminative Global Training Algorithm For Statistical MT
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.
No translation, language, or distortion model probabilities are used as in earlier work on SMT.
Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.
Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.
The training algorithm is evaluated on a standard Arabic-English translation task.
We use a BLEU oracle decoder for discriminative training of a local reordering model.
We use a perceptron style algorithm for training a large number of features.
We compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple, local reordering models.
We present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.

569
Machine Learning Of Temporal Relations
This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.
To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.
This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.
While machine learning approaches attempt to improve classification accuracy through feature engineering, we introduce a temporal reasoning component to greatly expand the training data.
We use the links introduced by closure to boost the amount of training data for a tlink classifier.

570
Semi-Supervised Training For Statistical Word Alignment
We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance.
We combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences.
We pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models.
We propose an EMD algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.

571
Semantic Taxonomy Induction From Heterogenous Evidence
We propose a novel algorithm for inducing semantic taxonomies.
Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns.
By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa.
We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1).
We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers.
Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.
We generate tens of thousands of hypernym patterns and combine these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet.
We use known hypernym / hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns.
We add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy.
We use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet.

572
Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora
Named Entity recognition (NER) is an important part of many natural language processing tasks.
Current approaches often employ machine learning techniques and require supervised data.
However, many languages lack such resources.
This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.
NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated.
We develop an algorithm that exploits both observations iteratively.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs.
We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
We explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora.
Our feature space consists of n-gram pairs from the two languages.
We find that the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC.
We introduce a Russian data set comprised of temporally aligned news articles.

573
A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features
This paper proposes a novel composite kernel for relation extraction.
The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.
The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.
Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
We use a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus.

574
An All-Subtrees Approach To Unsupervised Parsing
We investigate generalizations of the all-subtrees "DOP" approach to unsupervised parsing.
Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
We report state-of-the-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG).
We find that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score.

575
Methods For Using Textual Entailment In Open-Domain Question Answering
Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.
In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.
In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.
we applied a TE component to rerank candidate answers returned by a retrieval step for the task of Question Answering.

576
Using String-Kernels For Learning Semantic Parsers
We present a new approach for mapping natural language sentences to their formal meaning representations using string-kernel-based classifiers.
Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers.
Our experiments on two real-world data sets show that this approach compares favorably to other existing systems and is particularly robust to noise.
We use word subsequence kernel to compute the similarity between two substrings.
Our model, KRISP, takes a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.

577
Scalable Inference And Training Of Context-Rich Syntactic Translation Models
Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency.
Syntactic approaches seek to remedy these problems.
In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words.
Second, we propose probability estimates and a training procedure for weighting these rules.
We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.
We use xRS formalism to allow for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases.
Our rule composing method composes two or more minimal GHKM or SPMT rules having shared states to form larger rules.

578
Empirical Lower Bounds On The Complexity Of Translational Equivalence
This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts.
The study found that the complexity of these patterns in every bitext was higher than suggested in the literature.
These findings shed new light on why syntactic constraints have not helped to improve statistical translation models, including finite-state phrase-based models, tree-to-string models, and tree-to-tree models.
The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order.
Our methodology measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing.
Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning.
We argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two.

579
A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes
We propose a new hierarchical Bayesian n-gram model of natural languages.
Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models.
Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
We priovide a Bayesian interpretation to smoothing techniques, such as Kneser-Ney and Witten-Bell back-off schemes.
Nonparametric Bayesian modeling is able to provide priors that are especially suitable for tasks in NLP.
While the Dirichlet process is simply the Pitman Yor process with d= 0, we find that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language.

580
Word Sense And Subjectivity
Subjectivity and meaning are both important properties of language.
This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations.
We study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis.
We provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation.
We show that even reliable subjectivity clues have objective senses.
We show that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses.
We conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement.
We define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs.

581
A Phrase-Based Statistical Model For SMS Text Normalization
Short Messaging Service (SMS) texts behave quite differently from normal written texts and have some very special phenomena.
To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT).
However, such approaches suffer from customization problem as tremendous effort is required to adapt the language model of the existing translation system to handle SMS text style.
We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT.
In this paper, we view the task of SMS normalization as a translation problem from the SMS language to the English language and we propose to adapt a phrase-based statistical MT model for the task.
Evaluation by 5-fold cross validation on a parallel SMS normalized corpus of 5000 sentences shows that our method can achieve 0.80702 in BLEU score against the baseline BLEU score 0.6958.
Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score.
We also use Phrase-based SMT techniques on character level.
We use a phrase-based statistical machine translation model by splitting sentences into their k most probable phrases.

582
Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank
We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.
We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks.
This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.
We show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account.
We provide annotation for internal NP structure.
We recommend looking at accuracy figures by dependency type to understand what a parser is good at.
We re annotated DepBank using GRs scheme, and used it to evaluate the RASP parser.

583
Soft Syntactic Constraints For Word Alignment Through Discriminative Training
Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.
However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.
We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.
We use dependency structures as soft constraints to improve word alignment in an ITG framework.
We introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment.

584
Mildly Non-Projective Dependency Structures
Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency.
In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree.
While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity.
In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints.
The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.

585
On-Demand Information Extraction
At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic.
We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query.
On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort.
Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology.
It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging.
We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach.

586
Minimum Risk Annealing For Training Log-Linear Models
When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set.
Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex.
We propose training instead to minimize the expected loss, or risk.
We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis.
Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training.
We also show improvements in labeled dependency parsing.
We use a linearization technique to approximate the expectation of log BLEU score.
We present a deterministic annealing training procedure, whose objective is to minimize the expected error (together with the entropy regularization technique).
We observe test set gains by minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.

587
Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering
An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.
Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself.
We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.
Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.
The approach is evaluated on three different languages by measuring agreement with existing taggers.
We directly compare the tagger output to supervised taggers for English, German and Finnish via information-theoretic measures.
We conceptualize a network of words that capture the word co-occurrence patterns.
We cluster the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.

588
The Second Release Of The RASP System
We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text.
The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model.
We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information.

589
Tailoring Word Alignments to Syntactic Machine Translation
Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.
We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.
Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.
We also discuss the impact of various posterior-based methods of reconciling bidirectional alignments.
We refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.
We use hard union competitive thresholding.
We use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.

590
Transductive learning for statistical machine translation
Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.
In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.
We propose several algorithms with this aim, and present the strengths and weaknesses of each one.
We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large-data track.
We show a significant improvement in translation quality on both tasks.

591
Word Sense Disambiguation Improves Statistical Machine Translation
Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems.
In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero.
We show for the first time that integrating a WSD system improves the performance of a state-of-the-art statistical MT system on an actual translation task.
Furthermore, the improvement is statistically significant.
We train a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier.
We use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.

592
Domain Adaptation with Active Learning for Word Sense Disambiguation
When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed.
This highlights the importance of domain adaptation for word sense disambiguation.
In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems.
Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.
We perform supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.
We notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.

593
Forest Rescoring: Faster Decoding with Integrated Language Models
Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality.
We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems.
In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy.
We make assumptions about the the amount of reordering the language model can trigger in order to limit exploration.
We introduce cube pruning and and its variation, cube growing.
We use the forest concept to characterize the search space of decoding with integrated language models.

594
A Simple Similarity-based Model for Selectional Preferences
We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.
In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
We extract the set of seen head words from corpora with semantic role annotation, and use only a single vector space representation.
We model the contexts of a word as the distribution of words that co-occur with it.
We select a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset.

595
Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining
We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates.
We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work.
Our method is based on clustering patterns that contain concept words and other words related to them.
We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision.
We introduce the use of term frequency patterns for relationship discovery.
As a pre-requisite to extracting relations among pairs of classes, our method extracts class instances from unstructured Web documents, by submitting pairs of instances as queries and analyzing the contents of the top 1,000 documents returned by a Web search engine.
We propose a method for unsupervised discovery of concept specific relations, requiring initial word seeds.

596
Adding Noun Phrase Structure to the Penn Treebank
The Penn Treebank does not annotate within base noun phrases (NPs), committing only to at structures that ignore the complexity of English NPs.
This means that tools trained on Treebank data cannot learn the correct internal structure of NPs.
This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.
We then examine the consistency and reliability of our annotations.
Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus.
This adds detail to the Penn Treebank that is necessary for many NLP applications.
Our annotation scheme inserts NML and JJP brackets to describe the correct NP structure.
We use NE tags during the annotation process, as we find that NER based features will be helpful in a statistical model.

597
Formalism-Independent Parser Evaluation with CCG and DepBank
A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.
Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance.
In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations.
In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.
The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%.
We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types.
We develop a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998).
We demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009).

598
Instance Weighting for Domain Adaptation in NLP
Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
In this paper, we study the domain adaptation problem from the instance weighting perspective.
We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
We then propose a general instance weighting framework for domain adaptation.
Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.
We weigh training instances based on their similarity to unlabeled target domain data.
We find that balanced bootstrapping is more effective in domain adaptation than standard bootstrapping.
In our instance weighting, we assign larger weights to transferable instances so that the model trained on the source domain can adapt more effectively to the target domain.

599
Guiding Semi-Supervision with Constraint-Driven Learning
Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks.
In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms.
Our novel framework unifies and can exploit several kinds of task specific constraints.
The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.
We introduce constraint driven learning, CoDL.
We use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.

600
Supertagged Phrase-Based Statistical Machine Translation
Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate.
In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems.
We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model.
Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar.
Despite the differences between these two approaches, the supertaggers give similar improvements.
In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators.
We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents.
Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-the- art PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.

601
Improved Word-Level System Combination for Machine Translation
Recently, confusion network decoding has been applied in machine translation system combination.
Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.
This paper describes an improved confusion network based method to combine outputs from multiple MT systems.
In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring.
Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed.
A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.
The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.
We use the tercom script (Snover et al, 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.
We propose a multiple confusion network (CN) or super-network framework, where we use each of all individual system results as the backbone to build CNs based on an alignment metric, TER.
Each word in the confusion network is associated with a word posterior probability.

602
Fast Unsupervised Incremental Parsing
This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text.
The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing.
In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization.
The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text.
Our incremental parsing approach uses a novel representation called common cover links, which can be converted to constituent brackets.
Though punctuation is usually entirely ignored in unsupervised parsing research, we use phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.

603
Structured Models for Fine-to-Coarse Sentiment Analysis
In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.
Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.
The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.
Experiments show that this method can significantly reduce classification error relative to models trained in isolation.
We showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels.

604
Biographies Bollywood Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification
Automatic sentiment classification has been extensively studied and applied in recent years.
However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical.
We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.
First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline.
Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another.
This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.
We introduced a multi-domain sentiment dataset.

605
Statistical Machine Translation for Query Expansion in Answer Retrieval
We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers.
SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs.
We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages.
Experimental results show that SMT-based expansion improves retrieval performance over local expansion and over retrieval without expansion.
We demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web.

606
Randomised Language Modelling for Statistical Machine Translation
A Bloom filter (BF) is a randomised data structure for set membership queries.
Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
Here we explore the use of BFs for language modelling in statistical machine translation.
We show how a BF containing n-grams can enable us to use much larger corpora and higher-order models complementing a conventional n-gram LM within an SMT system.
We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for lower-order sub-sequences in candidate n-grams.
Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.
We present a scheme for associating static frequency information with a set of n-grams in a BF efficiently.

607
Learning to Extract Relations from the Web using Minimal Supervision
We present a new approach to relation extraction that requires only a handful of training examples.
Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web.
We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.
We provide a dataset that contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets from previous work.

608
A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation
Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.
Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.
Experiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%.
We use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node).
We model reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation.

609
Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora
Current phrase-based SMT systems perform poorly when using small training sets.
This is a consequence of unreliable translation estimates and low coverage over source and target phrases.
This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase.
Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language.
This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods.
Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.

610
A fully Bayesian approach to unsupervised part-of-speech tagging
Unsupervised learning of linguistic structure is a difficult problem.
A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.
Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.
We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.
Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.
This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.
Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.
We find improvements both when training from data alone, and using a tagging dictionary.
In our model, the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference.

611
Guided Learning for Bidirectional Sequence Classification
In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification.
The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm.
We apply this novel learning algorithm to POS tagging.
It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.
Our model is competitive to CRF in tagging accuracy but requires much less training time.
We develop new algorithms based on the easiet-first strategy and the perceptron algorithm.

612
Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification
We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking.
We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines.
Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers.
Our shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models.

613
Chinese Segmentation with a Word-Based Perceptron Algorithm
Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.
Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.
In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.
The generalized perceptron algorithm is used for discriminative training, and we use a beam-search decoder.
Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.
We also provide a feature template for Chinese word segmentation.

614
Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.
While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state.
Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.
In our model, we use the distinction bewtween pronouns, nominals, and proper nouns.
We evaluate the clustering properties of DPMMs by performing anaphora resolution with good results.

615
Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus
This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.
Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with lambda-operators is learned given a set of training sentences and their correct logical forms.
The resulting parser is shown to be the best-performing system so far in a database query domain.
We demonstrate that our meaning representation language outperforms FUNQL, another meaning representation language.

616
Learning Multilingual Subjective Language via Cross-Lingual Projections
This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English.
Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language.
We discuss different shortcomings of lexicon-based translation scheme for the more semantic-oriented task subjective analysis. Instead, we proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier.
We use a bilingual lexicon and a manually translated parallel corpus to generate a sentence classifier according to their level of subjectivity for Romanian.

617
Weakly Supervised Learning for Hedge Classification in Scientific Literature
We investigate automatic classification of speculative language ('hedging'), in biomedical text using weakly supervised machine learning.
Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented.
We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.
We use single words as input features in order to classify sentences from biological articles as speculative or non speculative.
We extend the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification.
We find that our model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically.

618
Moses: Open Source Toolkit for Statistical Machine Translation
We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models.
In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.
Our Moses decoder implements the factored phrase-based translation model.

619
The Tradeoffs Between Open and Traditional Relation Extraction
Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input.
Open IE is a relation-independent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web.
An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input.
How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system.
What are the tradeoffs between Open IE and traditional IE?
We consider this question in the context of two tasks.
First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary.
We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous state-of-the-art Open IE system.
Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall.
Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall.
We use a Conditional Random Field (CRF) classifier to perform Open Relation Extraction, improving by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system.
Our system is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones.

620
Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing
We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.
The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.
Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.
Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.
This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.
We suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed.

621
Forest-Based Translation
Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart.
However, current tree-based systems suffer from a major drawback: they only use the 2-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists.
Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline.
This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
At decoding time, we parse the input sentences into trees, and convert them into translation forest by rule pattern matching.
We propose the first direct use of packed forest.

622
A Discriminative Latent Variable Model for Statistical Machine Translation
Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
Results show that accounting for multiple derivations does indeed improve performance.
Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
We show that marginalizing out the different segmentations during decoding leads to improved performance.
We present a latent variable model that describes the relationship between translation and derivation clearly.
For the hierarchical phrase-based approach, we present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.

623
Vector-based Models of Semantic Composition
This paper proposes a framework for representing the meaning of phrases and sentences in vector space.
Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.
Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.
Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
We propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression.

624
Refining Event Extraction through Cross-Document Inference
We apply the hypothesis of "One Sense Per Discourse" (Yarowsky, 1995) to information extraction (IE), and extend the scope of "discourse" from one single document to a cluster of topically-related documents.
We employ a similar approach to propagate consistent event arguments across sentences and documents.
Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event extraction task.
Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
We employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents.

625
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects.
We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a).
Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings.
The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.
In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008).
We propose a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.

626
A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing
Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.
These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.
Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.
Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.
Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach.
We demonstrate that the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text.

627
A Tree Sequence Alignment-based Tree-to-Tree Translation Model
This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase.
The model leverages on the strengths of both phrase-based and linguistically syntax-based method.
It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts.
Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.
This gives our model stronger expressive power than other reported models.
Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.
Our method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules.

628
A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
In this paper, we propose a novel string-to-dependency algorithm for statistical machine translation.
With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model.
Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
We presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically.

629
Forest Reranking: Discriminative Parsing with Non-Local Features
Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives.
We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses.
Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.
Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
We show that the use of non-local features does in fact contribute substantially to parser performance.
To prune the packed forests, we use inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.

630
Simple Semi-supervised Dependency Parsing
We present a simple and effective semi-supervised method for training dependency parsers.
We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.
For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%.
In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
We show that for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data.
We propose to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech.

631
Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data
This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.
We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data.
Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively.
We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement.
In addition, our results are superior to the best reported results for all of the above test collections.
We run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem.
We use the automatically labeled corpus to train HMMs.

632
Unsupervised Multilingual Learning for Morphological Segmentation
For centuries, the deep connection between languages has brought about major discoveries about human communication.
In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning.
In particular, we study the task of morphological segmentation of multiple languages.
We present a non-parametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes.
We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English.
Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models.
Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.
We use bilingual information but the segmentation is learned independently from translation modeling.

633
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)
We address the task of unsupervised POS tagging.
We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
We present a family of algorithms to compute effective initial estimations p(t|w).
We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.
We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-of-the-art methods, while using simple and efficient learning methods.
We use linguistic considerations for choosing a good starting point for the EM algorithm.
We note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements.

634
Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation
In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes.
In this paper we investigate the effects of applying such a technique to higher-order n-gram models trained on large corpora.
We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens).
The resulting clusterings are then used in training partially class-based language models.
We show that combining them with word-based n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.
We introduce the predictive class bigram model.

635
Learning Bilingual Lexicons from Monolingual Corpora
We present a method for learning bilingual translation lexicons from monolingual corpora.
Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.
Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.
We only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)).
We present a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.

636
Unsupervised Learning of Narrative Event Chains
Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge.
We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text.
A narrative event chain is a partially ordered set of events related by a common protagonist.
We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments.
The second applies a temporal classifier to partially order the connected events.
Finally, the third prunes and clusters self-contained chains from the space of events.
We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order.
We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence.
We investigate unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions.

637
Joint Word Segmentation and POS Tagging Using a Single Perceptron
For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously.
A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.
Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space.
In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information.
Fast decoding is achieved by using a novel multiple-beam search algorithm.
The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.
The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach.
We use an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning.

638
A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging
We propose a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.
With a character-based perceptron as the core, combined with real-valued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.
Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline.
For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j.

639
Integrating Graph-Based and Transition-Based Dependency Parsers
Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.
In this paper, we show how these results can be exploited to improve parsing
accuracy by integrating a graph-based and a transition-based model.
By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
We first show how the MST Parser (McDonald et al, 2005) and the Malt Parser (Nivre et al, 2007) could be improved by stacking each parser on the predictions of the other.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.

640
Efficient Feature-based Conditional Random Field Parsing
Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods.
While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, feature rich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.
Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering.
On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14% relative reduction in error over previous models, while being two orders of magnitude faster.
On sentences of length 40, our system achieves an F-score of 89.0%, a 36% relative reduction in error over a generative baseline.
In our model, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008).

641
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data.
A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.
We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.
We obtain substantial improvements in performance for translation from Chinese and Arabic to English.
We revise this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary.
We find that their constituent constraints are sensitive to language pairs.

642
Generalizing Word Lattice Translation
Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.
We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammar-based models.
Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.
Our experiments evaluating the approach demonstrate substantial gains for Chinese-English and Arabic-English translation.
In our model, several different segmenters for Chinese are combined to create the lattice.
All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them.

643
Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs
We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity.
Intuitively, a candidate is popular if it was discovered many times by other instances in the hyponym pattern.
A candidate is productive if it frequently leads to the discovery of other instances.
Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.
We conducted experiments on four semantic classes and consistently achieved high accuracies.
We introduce a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking.

644
The Complexity of Phrase Alignment Problems
Many phrase alignment models operate over the combinatorial space of bijective phrase alignments.
We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.

645
Enforcing Transitivity in Coreference Resolution
A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment.
This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint.
We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments.
We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure.
We present a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier.

646
Self-Training for Biomedical Parsing
Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data.
Here we apply this technique to parser adaptation.
In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts.
This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus.
This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).

647
Reinforcement Learning for Mapping Instructions to Actions
In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions.
We assume access to a reward function that defines the quality of the executed actions.
During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward.
We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection.
We apply our method to interpret instructions in two domains -- Windows troubleshooting guides and game tutorials.
Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.
We show that performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood.

648
Learning Semantic Correspondences with Less Supervision
A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.
To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
We show that our model generalizes across three domains of increasing difficulty--Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
We propose a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. We use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.

649
Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices
Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-the-art Statistical Machine Translation (SMT) systems.
The algorithms were originally developed to work with N-best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N-best lists.
We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.
These algorithms are more efficient than the lattice-based versions presented earlier.
We show how MERT can be employed to optimize parameters for MBR decoding.
Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.
We describe an efficient approximate algorithm for computing n-gram posterior probabilities.

650
Recognizing Stances in Online Debates
This paper presents an unsupervised opinion analysis method for debate-side classification, i.e., recognizing which stance a person is taking in an online debate.
In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates.
We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem.
Our results show that our method is substantially better than challenging baseline methods.
We define stance as an overall position held by a person toward an object, idea or proposition.

651
Co-Training for Cross-Lingual Sentiment Classification
The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification.
However, there are many freely available English sentiment corpora on the Web.
This paper focuses on the problem of cross-lingual sentiment classification, which  leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.
Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are  considered as two independent views of the classification problem.
We propose a co-training approach to making use of unlabeled  Chinese data.
Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework.
We propose to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation.
We leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework.

652
Concise Integer Linear Programming Formulations for Dependency Parsing
We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program.
Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.
In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses.
The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
We introduce the multicommodity flow formulation.

653
Non-Projective Dependency Parsing in Expected Linear Time
We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input.
Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora.
Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score.
We present the projective Stack algorithm.

654
Dependency Grammar Induction via Bitext Projection Constraints
Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.
We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees.
Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees.
We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis.
We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
We use the posterior regularization (PR) approach in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser.

655
Minimized Models for Unsupervised Part-of-Speech Tagging
We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
We achieve the best results (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data.
We propose a rigid mechanism for modeling sparsity that minimizes the size of tagging grammar as measured by the number of transition types.
To avoid the need for manually pruning the tag dictionary, we propose that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary.

656
An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging
In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.
Our word-character hybrid model offers high performance since it can handle both known and unknown words.
We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error-driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.
We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-of-the-art approaches reported in the literature.
We separate the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters.

657
Unsupervised Learning of Narrative Schemas and their Participants
We describe an unsupervised system for learning narrative schemas, coherent sequences or sets of events (arrested(POLICE,SUSPECT), convicted(JUDGE, SUSPECT)) whose arguments are filled with participant semantic roles defined over words (JUDGE = {judge, jury, court}, POLICE = {police, agent, authorities}).
Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles.
Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles.
By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles.
We describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.

658
Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art
We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.
First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.
Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.
We show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention.

659
Automatic sense prediction for implicit discourse relations in text
We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.
We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.
We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.
Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a).
Our analysis shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined.

660
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
We present a phrasal synchronous grammar model of translational equivalence.
Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units.
Inference is performed using a novel Gibbs sampler over synchronous derivations.
This sampler side-steps the intractability issues of previous models which required inference over derivation forests.
Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.
We use Gibbs sampler for learning the SCFG by reasoning over the space of derivations (Blunsom et al, 2009).
We present a method for maintaining table counts without needing to record the table assignments for each translation decision.
We apply the technique of using multiple processors to perform approximate Gibbs sampling which we show achieve equivalent performance to the exact Gibbs sampler.

661
Application-driven Statistical Paraphrase Generation
Paraphrase generation (PG) is important in plenty of NLP applications.
However, the research of PG is far from enough.
In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.
In our experiments, we use the proposed method to generate paraphrases for three different applications.
The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.
We present a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression.

662
Better Word Alignments with Supervised ITG Models
This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.
Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.
For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.
Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.
Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.
We describe a pruning heuristic that results in average case runtime of O (n 3).

663
Distant supervision for relation extraction without labeled data
Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.
We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size.
Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large un-labeled corpus and extract textual features to train a relation classifier.
Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).
Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.
We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.
Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al,2008) and databases (Mintz et al, 2009), or attempted open information extraction (IE) (Banko et al, 2007) to extract all possible relations.
Distant supervision (DS) can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base.

664
Phrase Clustering for Discriminative Learning
We present a simple and scalable algorithm for clustering tens of millions of phrases and use  the resulting clusters as features in  discriminative classifiers.
To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification.
Our results show that phrase clusters offer significant improvements over word clusters.
Our NER system achieves the best current result on the widely used CoNLL benchmark.
Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.
We explore a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features.
We present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.

665
Using Syntax to Disambiguate Explicit Discourse Connectives in Text
Discourse connectives are words or phrases such as once, since, and on the contrary that explicitly signal the presence of a discourse relation.
There are two types of ambiguity that need to be resolved during discourse processing.
First, a word can be ambiguous between discourse or non-discourse usage.
For example, once can be either a temporal discourse connective or a simply a word meaning “formerly”.
Secondly, some connectives are ambiguous in terms of the relation they mark.
For example since can serve as either a temporal or causal connective.
We demonstrate that syntactic features improve performance in both disambiguation tasks.
We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation.
We show that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult.

666
Bayesian Learning of a Tree Substitution Grammar
Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.
Past approaches have resorted to heuristics.
In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size.
The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.

667
Efficient Third-Order Dependency Parsers
We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.
The set of potential edges is pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent.

668
Word Representations: A Simple and General Method for Semi-Supervised Learning
If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.
We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking.
We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.
We find further improvements by combining different word representations.
You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/
We show that monolingual word clusters are broadly applicable as features in monolingual models for linguistic structure prediction.

669
A Latent Dirichlet Allocation Method for Selectional Preferences
The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences.
By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional class-based approaches, it produces human interpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power.
We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007).
We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007).
We focus on inferring latent topics and their distributions over multiple arguments and relations (e.g., the subject and direct object of a verb).

670
Practical Very Large Scale CRFs
Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels.
Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.
In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features.
Efficiency stems here from the sparsity induced by the use of a lscript1 penalty term.
Based on our own implementation, we compare three recent proposals for implementing this regularization strategy.
Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.

671
Dynamic Programming for Linear-Time Incremental Parsing
Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.
We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.
Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.
Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008).
The essential idea in our calculation is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra.

672
Supervised Noun Phrase Coreference Research: The First Fifteen Years
The research focus of computationalcoreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
We argue that the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model.

673
Learning to Translate with Source and Target Syntax
Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years.
These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language.
But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language.
We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.
We obtain significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions.
We show that the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive.

674
Intelligent Selection of Language Model Training Data
We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy. This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words).

675
cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models
We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.
Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.
From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.
Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
We present cdec, a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system.
Our cdec decoder learns word segmentation lattices from raw text in an unsupervised manner.

676
Target-dependent Twitter Sentiment Classification
Sentiment analysis on Twitter data has attracted much attention recently.
In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query.
Here the query serves as the target of the sentiments.
The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target.
Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets).
However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification.
In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration.
According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.
We incorporate target-dependent features and considers related tweets by utilizing a graph-based optimization.
We combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets.

677
A New Dataset and Method for Automatically Grading ESOL Texts
We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts.
In particular, we use rank preference learning to explicitly model the grade relationships between scripts.
A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.
A comparison between regression and rank preference models further supports our method.
Experimental results on the first publicly available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
We publicly release a set of 1,244 FCE ESOL texts.

678
Collecting Highly Parallel Data for Paraphrase Evaluation
A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years.
We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates.
In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
Our dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing.

679
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter
Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP.
In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words.
Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.
Both word similarity and context are then exploited to select the most probable correction candidate for the word.
The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.
We use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity.

680
Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations
Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text.
Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors.
Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts.
We apply our model to learn extractors for NY Times text using weak supervision from Freebase.
Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.
We apply the greedy inference algorithm.
We use  multiple deterministic-OR constraint to train a sentential relation extractor.

681
Learning Dependency-Based Compositional Semantics
Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.
In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.
In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.
On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.
We DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins.
Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees.

682
Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections
We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.
Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.
We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010).
Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.
We build a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text.

683
Template-Based Information Extraction without the Templates
Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template).
This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance.
Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles.
We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents.
We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.
We acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles.

684
Local and Global Algorithms for Disambiguation to Wikipedia
Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing.
The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation.
Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible.
In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches.
We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.

685
Part-of-Speech Tagging for Twitter: Annotation Features and Experiments
We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter.
We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy.
The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.
Recognizing the limitations of existing systems,we develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies).
We release the Twitter POS dataset consisting of approximately 26,000 words across 1,827 tweets.
The Twitter dataset uses a domain-dependent tag set of 25 tags.

686
Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability
In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.
To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data.
In this paper, we consider how to make such experiments more statistically reliable.
We provide a systematic analysis of the effects of optimizer instability — an extraneous variable that is seldom controlled for — on experimental outcomes, and make recommendations for reporting results more accurately.
We implement a stratified approximate randomization test to account for multiple tuning replications.

687
Transition-based Dependency Parsing with Rich Non-local Features
Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations.
In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transition-based parsing and rivaling the best results overall.
For the Chinese Treebank, they give a signficant improvement of the state of the art.
An open source release of our parser is freely available.
We develop the feature template for the arc-eager model.

688
Improving Word Representations via Global Context and Multiple Word Prototypes
Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.
However, most of these models are built with only local context and one representation per word.
This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings.
We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word.
We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.
Our representation is designed to capture word sense disambiguation.

689
Parsing with Compositional Vector Grammars
Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.
Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser.
The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
Recursive neural networks, which have the ability to generate a tree structured output, have already been applied to natural language parsing, we extended them to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013).

690
Providing A Unified Account Of Definite Noun Phrases In Discourse
Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others.
However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one.
This paper focuses on particular phenomena of this sort - the use of various referring expressions such as definite noun phrases and pronouns - and examines their interaction with mechanisms used to maintain discourse coherence.
Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena.
The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact.
The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic.
The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases.
To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4.
This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic.
In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed.
To resolve referring expression, we develop centering theory.

691
Deterministic Parsing Of Syntactic Non-Fluencies
It is often remarked that natural language, used naturally, is unnaturally ungrammatical.
Spontaneous speech contains all manner of false starts, hesitations, and self-corrections that disrupt the well-formedness of strings.
It is a mystery then, that despite this apparent wide deviation from grammatical norms, people have little difficulty understanding the non-fluent speech that is the essential medium of everyday life.
And it is a still greater mystery that children can succeed in acquiring the grammar of a language on the basis of evidence provided by a mixed set of apparently grammatical and ungrammatical strings.
We address the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text.
We define a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser.

692
D-Theory: Talking About Talking About Trees
Linguists, including computational linguists, have always been fond of talking about trees.
In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory).
While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural language in a manner which is intrinsically computational.
This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.
Our D-theory model is powerful in that it allows the right-most daughter of a node to be lowered under a sibling node.

693
Parsing As Deduction
By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses.
The efficiency of this approach for an interesting class of grammars is discussed.
We extend Earley deduction work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters.
We present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.

694
Features And Values
The paper discusses the linguistic aspects of a new general purpose facility for computing with features.
The program was developed in connection with the course I taught at the University of Texas in the fall of 1983.
It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me.
Like its predecessors, the new Texas version of the "DG {directed graph}" package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representations too.
We provide examples of feature structures in which a negation operator might be useful.

695
Functional Unification Grammar: A Formalism For Machine Translation
Functional Unification Grammar provides an opportunity to encompass within one formalism and computational system the parts of machine translation systems that have usually been treated separately, natably analysis, transfer, and synthesis.
Many of the advantages of this formalism come from the fact that it is monotonic allowing data structures to grow differently as different nondeterministic alternatives in a computation are pursued, but never to be modified in any way.
A striking feature of this system is that it is fundamental reversible, allowing a to translate as b only if b could translate as a.

696
The Design Of A Computer Language For Linguistic Information
A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics.
It has been the goal of the PART group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers.
The PATR-II formalism is our current computer language for encoding linguistic information.
This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.
PATR-II is a minimal constraint-based formalism that extends context-free grammar.

697
A Syntactic Approach To Discourse Semantics
A correct structural analysis of a discourse is a prerequisite for understanding it.
This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure.
This grammar, the "Dynamic Discourse Model", uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse.
The intermediate states of the parser model the intermediate states of the social situation which generates the discourse.
The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents.
It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here.

698
Ontological Promiscuity
To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple.
In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional.
The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process.
Three classical problems - opaque adverbials, the distinction between de re and de ditto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome.
The paper closes with a statement about the view of semantics that is presupposed by this approach.
We advocate for an ontologically promiscuous representation that includes a wide variety of types of entities.
We argue that semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem.

699
Some Computational Properties Of Tree Adjoining Grammers
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
Some of the basic notions of TAG's were introduced in [Joshi, Levy, and Takakashi 1975] and by [Joshi, 1983].
A detailed investigation of the linguistic relevance of TAG's has been carried out in [Kroch and Joshi, 1985].
In this paper, we will describe some new results for TAG's, especially in the following areas: (1) parsing complexity of TAG's, (2) some closure results for TAG's, and (3) the relationship to Head grammars.
We provide parsing algorithms for TAGs that could serve to parse the base formalism of a synchronous TAG.
We present the first TAG parser in a CYK-like algorithm.

700
Using Restriction To Extend Parsing Algorithms For Complex-Feature-Based Formalisms
Grammar formalisms based on the encoding of grammatical information in complex-valued feature systems enjoy some currency both in linguistics and natural-language-processing research.
Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort.
Unfortunately, in moving to an infinite nonterminal domain, standard methods of parsing may no longer be applicable to the formalism.
Typically, the problem manifests itself as gross inefficiency or even nontermination of the algorithms.
In this paper, we discuss a solution to the problem of extending parsing algorithms to formalisms with possibly infinite nonterminal domains, a solution based on a general technique we call restriction.
As a particular example of such an extension, we present a complete, correct, terminating extension of Earley's algorithm that uses restriction to perform top-down filtering.
Our implementation of this algorithm demonstrates the drastic elimination of chart edges that can be achieved by this technique.
Finally, we describe further uses for the technique - including parsing other grammar formalisms, including definite-clause grammars; extending other parsing algorithms, including LR methods and syntactic preference modeling algorithms; and efficient indexing.
We propose a modified version of the Earley-parser, using restricted top down prediction.

701
Recovering Implicit Information
This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing natural language messages.
PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components.
Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.
This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.
The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and referenee resolution can be directed to find specific referents for the entities.
In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution.
The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents.
We propose the first attempt for the automatic annotation of implicit semantic roles.
We make one of the earliest attempts to automatically recover extra-sentential arguments.

702
A Property-Sharing Constraint In Centering
A constraint is proposed in the Centering approach to pronoun resolution in discourse.
This "property-sharing" constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property.
This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses, where different pronominal forms are primarily used to realize the Cb.
It is the zero pronominal in Japanese, and the (unstressed) overt pronoun in English.
The resulting constraint complements the original Centering, accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances.
It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism.
This reconciliation of centering/focusing and parallelism is a major advantage.
I will then add another dimension called the "speaker identification" to the constraint to handle a group of special eases in Japanese discourse.
It indicates a close association between centering and the speaker's viewpoint, and sheds light on what underlies the effect of perception reports on pronoun resolution in general.
These results, by drawing on facts in two very different languages, demonstrate the cross-linguistic applicability of the centering framework.

703
Characterizing Structural Descriptions Produced By Various Grammatical Formalisms
We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.
In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees.
We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammar.
On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.
We introduce Linear context-free rewriting system (LCFRS), which is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases.

704
A Centering Approach To Pronouns
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.
As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting.
We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns.
The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.
Our centering algorithm extends the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift.
The most common classification of transitional states are predicted to be less and less coherent in the order of CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT.
The measure M.BFP uses a lexicographic ordering on 4-tuples to determine the transition state.
Hard-core centering approaches only deal with the last sentence.

705
A Unification Method For Disjunctive Feature Descriptions
Although disjunction has been used in several unification-based grammar formalisms, existing methods of unification have been unsatisfactory for descriptions containing large quantities of disjunction, because they require exponential time.
This paper describes a method of unification by successive approximation, resulting in better average performance.
The general problem of unifying two disjunctive feature structures is non-polynomial in the number of disjunctions.
We present a technique which, for every set of n conjoined disjunctions, checks the consistency first of single disjuncts against the definite part of the description, then that of pairs and so on up ton-tuples for full consistency.

706
Interpretation As Abduction
An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized.
Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated.
It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.

707
Cues And Control In Expert-Client Dialogues
We conducted an empirical analysis into the relation between control and discourse structure.
We applied control criteria to four dialognes and identified 3 levels of discourse structure.
We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control.
Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not.
We define initiative as being held by the speaker who is driving the conversation at any point in the conversation.
We propose rules for tracking initiative based on utterance types: for example, statements, proposals and questions show initiative while answers and acknowledgements do not.

708
Planning Coherent Multisentential Text
Though most text generators are capable of simply stringing together more than one sentence, they cannot determine which order will ensure a coherent paragraph.
A paragraph is coherent when the information in successive sentences follows some pattern of inference or of knowledge with which the hearer is familiar.
To signal such inferences, speakers usually use relations that link successive sentences in fixed ways.
A set of 20 relations that span most of what people usually say in English is proposed in the Rhetorical Structure Theory of Mann and Thompson.
This paper describes the formalization of these relations and their use in a prototype text planner that structures input elements into coherent paragraphs.
We use plan-operators in order to create coherent stretches of text.

709
A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms
We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable.
In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion.
The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.
We point out a termination problem in the left-recursive rules.

710
Cooking Up Referring Expressions
This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes.
Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings.
We produce a description entailing the minimal number of attributes possible at the price of suffering NP-hard complexity.
Our algorithm attempts to build a minimal distinguishing description by always selecting the most discriminatory property available.
We define minimality as the proportion of descriptions produced by a system that are maximally brief.
We propose a solution to the problem of generating definite descriptions that evoke a discourse entity already introduced in the context.

711
Word Association Norms Mutual Information And Lexicography
The term word association is used in a very particular sense in the psycholinguistic literature.
(Generally speaking, subjects respond quicker than normal to the word "nurse" if it follows a highly associated word such as "doctor").
We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
(The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable).
The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.

712
Evaluating Discourse Processing Algorithms
In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study.
We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.
We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general.
We illustrate the general difficulties encoun- tered with quantitative evaluation.
These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.
We refer to error chaining as the case in which a pronoun x correctly says that it is coreferent with another pronoun y while the program misidentifies the antecedent of y.

713
Structural Disambiguation With Constraint Propagation
We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on word-to-word modifications.
CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees.
The weak generative capacity and the computational complexity of CDG parsing are also discussed.
Our constraint dependency grammar maps to the notation of constratin satisfaction techniques.

714
Mixed Initiative In Dialogue: An Investigation Into Discourse Segmentation
Conversation between two people is usually of MIXED-INITIATIVE, with CONTROL over the conversation being transferred from one person to another.
We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns.
The application of the control rules lets us derive domain-independent discourse structures.
The derived structures indicate that initiative plays a role in the structuring of discourse.
In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets.
This distribution indicates that some control segments are hierarchically related to others.
The analysis suggests that discourse participants often mutually agree to a change of topic.
We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types.
These differences can be explained in terms of collaborative planning principles.
We find that as initiative passes back and fourth between discourse participants, control over the conversation similarly transfers from one speaker to another.
We develope utterance-based rules for allocation of control.

715
Automatically Extracting And Representing Collocations For Language Generation
Collocational knowledge is necessary for language generation.
The problem is that collocations come in a large variety of forms.
They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways.
This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation.
We address both problems in this paper, focusing on the acquisition problem.
We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.

716
Noun Classification From Predicate-Argument Structures
A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
We make use of a mutual-information based metric to determine noun similarity.
We use thenotion of distributional similarity, i.e., two words with similar meanings will be used in similar contexts.

717
Two Languages Are More Informative Than One
This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language.
This approach exploits the differences between mappings of words to senses in different languages.
We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism.
The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.

718
Aligning Sentences In Parallel Corpora
In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora.
In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain.
Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text.
We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand.
We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%.
Thus, the technique may be applicable to a wider variety of texts than we have yet tried.
We are able to achieve these results while completely ignoring the lexical content of the tests.

719
A Program For Aligning Sentences In Bilingual Corpora
Researchers in both machine Iranslation (e.g., Brown et al., 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English).
This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths.
The method was developed and tested on a small trilingual sample of Swiss economic reports.
A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI.
We extract pairs of anchor words such as numbers, proper nouns (organiziation, person, title), dates and monetary information.
We find that the byte length ratio of target sentence to source sentence is normally distributed.
We demonstrate the effectiveness of a global alignment dynamic program algorithm where the basic similarity score is based on the difference in sentence lengths, measured in characters.

720
Automatic Acquisition Of Subcategorization Frames From Untagged Text
This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur.
Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980).
The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus.
False positive rates are one to three percent of observations.
Five SFs are currently detected and more are planned.
Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.

721
Structural Ambiguity And Lexical Relations
We propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb, estimated on the basis of word distribution in a large corpus.
This work suggests that a distributional approach can be effective in resolving parsing problems that apparently call for complex reasoning.
We find that human arbiters constitently reach a higher agreement in propositional phrase attachment when they are given the entire sentence rather than just the four words concerned.

722
Word-Sense Disambiguation Using Statistical Methods
We describe a statistical technique for assigning senses to words.
An instance of a word is assigned a sense by asking a question about the context in which the word appears.
The question is constructed to have high mutual information with the translation of that instance in another language.
When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.
We propose a word-sense disambiguation algorithm to disambiguate English translations of French target words based on the single most imformative context feature.
We perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.

723
Monotonic Semantic Interpretation
Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations.
The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation.
Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.
We make use of Quasi Logical Form, a monotonic representation for compositional semantics.
A quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations.

724
Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog
We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences.
We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction.
The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
We are able to correctly identify 309 of 406 utterences containing nontrivial repairs, while 191 fluent utterances were incorrectly identified as containing repairs.
We speculate that acoustic information might be used to filter out false positives for canditate matching.
We find location of word fragments to be an invaluable cue to both the detection and correction of disfluencies.

725
Inside-Outside Reestimation From Partially Bracketed Corpora
The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus.
Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one.
In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of hand-parsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus.
Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.
We adapt the inside-outside algorithm to apply over semi-supervised data extract from the Penn TreeBank.

726
Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs
We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia).
After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good.
Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures.
Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.
Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance.
This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation.
An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases.
An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants.
Not surprisingly, the upper bound is very dependent on the instructions given to the judges.
Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected.
In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system.
Under quite different conditions, we have found 96.8% agreement over judges.
We argue that any wide-coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worth of serious consideration.

727
Char Align: A Program For Aligning Parallel Texts At The Character Level
There have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown et al (1991), Gale and Church (to appear), Isabelle (1992), Kay and R/Ssenschein (to appear), Simard et al (1992), Warwick-Armstrong and Russell (1990).
On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence).
Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences.
This paper describes a new program, charalign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard et al.
We show that cheap alignment of text segments is possible by exploiting orthographic cognates.
Char_align is designed for language pairs that share a common alphabet.

728
Aligning Sentences In Bilingual Corpora Using Lexical Information
In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus.
Existing efficient algorithms ignore word identities and only consider sentence length (Brown el al., 1991b; Gale and Church, 1991).
Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment.
We find the alignment that maximizes the probability of generating the corpus with this translation model.
We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results.
The algorithm is language independent.
We find that sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates.
We find that dynamic programming is particularly susceptible to deletions occurring in one of the two languages.
We use manually aligned pairs of sentences to train our alignment models.

729
An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora
The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.
The taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.
Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers.
The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated.
Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.
We attempt to find noun phrase correspondence in parallel corpora using part-of-speech tagging and noun phrase recognition methods.

730
Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing
We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse.
In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.

731
GEMINI: A Natural Language System For Spoken-Language Understanding
We report a syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus.
Gemini is an expressive formalism in which to write formal grammars.
We present Gemini natural language parser/generator, which attempts to parse the speech recognition output.

732
Principle-Based Parsing Without Overgeneration
Overgeneration is the main source of computational complexity in previous principle-based parsers.
This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem.
This algorithm has been implemented in C++ and successfully tested with example sentences from (van Riemsdijk and Williams, 1986).
Our parser produces functional relations for the components in a sentence, including subject and object relations with respect to a verb.
In our dependency trees nodes represent text expressions and edges represent the syntactic relations between them.

733
Intention-Based Segmentation: Human Reliability And Correlation With Linguistic Cues
Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units.
Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena.
However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them.
We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues.
The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion.
We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
We conflat multiple manual segmentations into one that contains only those boundaries which the majority of coders agree upon.
We adopt a flat model of topic segmentation for our gold standard based on discourse segment purpose.

734
Contextual Word Similarity And Estimation From Sparse Data
In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context.
This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data.
We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric.
Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models.
We argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information.
Clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.

735
Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning
In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales.
We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.
We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives.
We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives.
We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained.
We learn attributes by clustering adjectives that denote values of the same attribute.

736
Distributional Clustering Of English Words
We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts.
Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering.
Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership.
In many cases, the clusters can be thought of as encoding coarse sense distinctions.
Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data.
Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
We make use of deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.

737
Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora
This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.
It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.
Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem.
We used the 4 million word corpus of the New York Times and selected only clauses with auxiliary verbs followed by automatically analyzing them with a finite-state parser.

738
Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach
In this paper we describe a new technique for parsing free text: a transformational grammar I is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.
The algorithm works by beginning in a very naive state of knowledge about phrase structure.
By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.
After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
Our transformation-based tagging requires a handtagged text for training.

739
Text Segmentation Based On Similarity Between Words
This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text.
A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations.
LCP records mutual similarity of words in a sequence of text.
The similarity of words, which represents their cohesiveness, is computed using a semantic network.
Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments.
LCP may provide valuable information for resolving anaphora and ellipsis.
We find that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to better results than work that is based only on word distribution in texts.

740
Multi-Paragraph Segmentation Of Expository Text
This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.
Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
We compute similarities between textual units based on the similarities of word space vectors.
TextTiling is able to partition messages into multi-paragraph segments with an overall precision of 83% and recall of 78%.

741
Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria
We describe our experience with automatic alignment of sentences in parallel English-Chinese texts.
Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale and Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.
We find that the lengths of English and Chinese texts are not as highly correlated as in the French-English task, leading to lower success rates for length-based aligners.

742
Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French
This paper presents a statistical decision procedure for lexical ambiguity resolution.
The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity.
By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies.
Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text.
Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.
We note that lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together.
The strategy we adopt to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to descrive local contexts.

743
Verb Semantics And Lexical Selection
This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT).
Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments.
A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT.
We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems.
Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.
We design our mesure such that shallow nodes are less similar than nodes that are deeper in the WordNet hierarchy.
Our mesure is purely taxonomic; it does not require any corpus statistics.
Our similarity metric measures the depth of the two concepts in the WordNet taxonomy and the depth of the least common subsumer and then combines these figures into a similarity score.

744
Word-Sense Disambiguation Using Decomposable Models
Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.
In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest.
We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.
Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.
We manually assign 2,476 usages of interest with sense tags from the Longman Dictionary of Contemporary English.

745
Corpus Statistics Meet The Noun Compound: Some Empirical Results
A variety of statistical methods for noun compound analysis are implemented and compared.
The results support two main conclusions.
First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy.
Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature.
We propose an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.
We test both adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words.

746
D-Tree Grammars
DTG are designed to share some of the advantages of TAG while overcoming some of its limitations.
DTG involve two composition operations called subsertion and sister-adjunction.
The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification.
Furthermore, DTG, unlike TAG, can provide a uniform analysis for em wh-movement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.
In the quest of modeling dependency correctly, we expand weak generative capacity and thus end up with much greater parsing complexity.

747
Unsupervised Word Sense Disambiguation Rivaling Supervised Methods
This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure.
Tested accuracy exceeds 96%.
We introduce the idea of sense consistency and extend it to operator across related documents.
We propose the self training, a semi-supervised algorithm which we apply do word sense disambiguation.

748
Two-Level Many-Paths Generation
Large-scale natural language generation requires the integration of vast mounts of knowledge: lexical, grammatical, and conceptual.
A robust generator must be able to operate well even when pieces of knowledge are missing.
It must also be robust against incomplete or inaccurate inputs.
To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.
We describe algorithms and show experimental results.
We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.
We use a sampling technique in which a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal.

749
Statistical Decision-Tree Models For Parsing
Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general.
In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.
This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models.
In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser.
Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% pre- cision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.
We create FTB-UC-DEP, a depenency tree bank derived from FTB-UC using the technique of head propagation rules.
We find that lexicalization substantially improves performance compared to an unlexicalized baseline model such as a probabilistic context-free grammar.

750
Identifying Word Translations In Non-Parallel Texts
Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts.
This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts.
The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages.
We propose a computationally demanding matrix purmutation method which maximizes a similarity between co-occurence matrices in two languages.
An underlying assumption in our work is that translations of words that are related in one language are also related in the other language.

751
Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach
In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.
LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET.
We obtain an overall accuracy for the noun interest of 87% and find that when our feature sets consists only of co-occurrence features the accuracy only drops to 80%.
We conclude taht collocational information is more important than syntactic information to WSD.
Our DSO corpus focuses on 191 frequent and polysemous words (nouns and verbs) and contains around 1,000 sentences per words.

752
A Fully Statistical Approach To Natural Language Interfaces
We present a natural language interface system which is based entirely on trained statistical models.
The system consists of three stages of processing: parsing, semantic interpretation, and discourse.
Each of these stages is modeled as a statistical process.
The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.
Our approach is fully supervised and produces a final meaning representation in SQL.
We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel.

753
Efficient Normal-Form Parsing For Combinatory Categorial Grammar
Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses.
Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input.
This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique.
The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated.
We provide a safe and complete parsing algorithm which can return non-NF derivations when necessary to preserve an interpretation if composition is bounded or the grammar is restricted in other ways.

754
A Polynomial-Time Algorithm For Statistical Machine Translation
We introduce a polynomial-time algorithm for statistical machine translation.
This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures.
The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model.
The new algorithm in our experience yields major speed improvement with no significant loss of accuracy.
We test our algorithm on Chinese-English translation.

755
Parsing Algorithms And Metrics
Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.
However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree.
By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved.
We present two new algorithms: the "Labelled Recall Algorithm," which maximizes the expected Labelled Recall Rate, and the "Bracketed Recall Algorithm," which maximizes the Bracketed Recall Rate.
Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize.
We observe that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse.

756
A New Statistical Parser Based On Bigram Lexical Dependencies
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.
Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task.
The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes.
With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.
We make use of a backed-off smoothing technique to alleviate sparse data problems.

757
Chart Generation
Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases.
We propose to reduce the number of constituents build during realisation by only considering for combination constituents with non overlapping semantics and compatible indices.
We propose a chart based generation process which takes packed representations as input and generates all paraphrases without expanding first into disjunctive normal form.

758
A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.
We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.
We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.
We find that speech is able to improve inter-annotator agreement in discourse segmentation of monologues.
We introduce the Boston Directions Corpus, a publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling.

759
An Empirical Study Of Smoothing Techniques For Language Modeling
We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991).
We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data.
In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.
Our smoothing technique can smooth together the predictions of unigram, bigram, trigram and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity.

760
Minimizing Manual Annotation Cost In Supervised Training From Corpora
Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora.
This paper investigates methods for reducing annotation cost by sample selection.
In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage.
This avoids redundantly annotating examples that contribute little new information.
This paper extends our previous work on committee-based sample selection for probabilistic classifiers.
We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging.
We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs.
In particular, the simplest method, which has no parameters to tune, gives excellent results.
We also show that sample selection yields a significant reduction in the size of the model used by the tagger.
We use HMMs for POS tagging and find that selective samplying of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies.
We use the vote entropy metric, the entropy of the distribution of labels assigned to an example by the ensemble of classifiers, to estimate the disagreement within an ensemble.

761
Three Generative Lexicalized Models For Statistical Parsing
In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.
We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement.
Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).
We provide a 29-million word parsed corpus from the Wall Street Journal.

762
Automatic Detection Of Text Genre
As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification.
We propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.
We believe that parsing and word-sense disambiguation can also benefit from genre classification.
We avoid structured markers since they require tagged or parsed text and replace them with character-level markers (e.g., punctuation mark counts) and derivative markers, i.e., ratios and variation measures derived from measure of lexical and character-level markers.

763
Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity
Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word.
Separate classifiers have to be trained for different words.
We present an algorithm that uses the same knowledge sources to disambiguate different words.
The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts.
We define the similarity between two objects to be the amount of information contained in the commonolity between the objects divided by the amount of information in the descriptions of the objects.

764
The Rhetorical Parsing Of Unrestricted Natural Language Texts
We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts.
The algorithms use information that was derived from a corpus analysis of cue phrases.
We describe a method for text summarization based on a nuclearity and selective retention of hierarchical fragments.

765
Machine Transliteration
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.
These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.
For example, computer in English comes out as (konpyuutaa) in Japanese.
Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.
We describe and evaluate a method for performing backwards transliterations by machine.
This method uses a generative model, incorporating several distinct stages in the transliteration process.
We propose to compose a set of weighted finite state transducers to solve the problem of back-transliteration from Japanese Katakana to English.

766
Predicting The Semantic Orientation Of Adjectives
We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives.
A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently.
Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative.
Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus.
We cluster adjectives into + and - sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning and clustering.

767
PARADISE: A Framework For Evaluating Spoken Dialogue Agents
This paper presents PARADISE (PARAdigm for Dialogue System Evaluation), a general framework for evaluating spoken dialogue agents.
The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.
We identify three factors which carry an influence on the performance of SDSs: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factor (e.g. factors related to the acoustic environment and the transmission channel.
We aim to evaluate diaglogue agent strategies by relating overall user satisfaction to other metrics such as task success, efficiency measure and qualitative measures.

768
A Trainable Rule-Based Algorithm For Word Segmentation
This paper presents a trainable rule-based algorithm for performing word segmentation.
The algorithm provides a simple, language-independent alternative to large-scale lexicai-based segmenters requiring large amounts of knowledge engineering.
As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation.
In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages.
Our Chinese segmenter makes use of only a manually segmented corpus without referring to any lexicon.

769
A Word-To-Word Model Of Translational Equivalence
Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model.
For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter.
This feature makes the model more suitable for applications that are not fully statistical.
The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..
Our model can link word tokens in parallel texts as well as other translation models in the literature.
Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
We propose the Competitive Linking Algorithm for linking word pairs and a method which calculates the optimized correspondence level between the word pairs by hill climbing.
One problem that arises in word-to-word alignment is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation.

770
A Memory-Based Approach to Learning Shallow Natural Language Patterns
Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing.
The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata.
This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus.
The training data are stored as-is, in efficient suffix-tree data structures.
Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus.
This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training.
The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English.
Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.
We segment the POS sequence of a multi-word into small POS titles, count tile frequency in the new word and non-new-word on the training set respectively and detect new words using these counts.

771
Entity-Based Cross-Document Core f erencing Using the Vector Space Model
Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source.
Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to examine information about a particular entity from multiple text sources at the same time.
In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.
In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.


We proposed entity-based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.

772
The Berkeley FrameNet Project
FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building").
The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.
The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits).
This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.
We present the FrameNet project in which we havee been developing a frame-semantic lexicon for the core vocabulary of English.

773
Classifier Combination for Improved Lexical Disambiguation
One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier.
In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary.
Next, we show how this complementary behavior can be used to our advantage.
By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers.
We define the complementarity between two learners in order to quantify the percentage of time when one system is wrong while another system is correct, therefore providing an upper bound on combination accuracy.

774
Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification
Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications.
While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task.
In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences.
The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a "treebank" corpus; then the grammar is improved by selecting rules with high "benefit" scores.
Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.
We store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data.

775
Exploiting Syntactic Structure for Language Modeling
The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies.
The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner -- therefore usable for automatic speech recognition.
The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.
We choose the lexical heads of the two previous constituents as determined by a shift-reduce parser and find that this works better than a trigram model.
We condition on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.

776
Investigating Regular Sense Extensions based on Intersective Levin Classes
In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.
We see verb classes as the key to making generalizations about regular extensions of meaning.
Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes.
We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components.
We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes.
We also have begun to examine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.
We show that multiple listings could in some cases be interpreted as regular sense extensions and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes.
We argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses.

777
An IR Approach for Translating New Words from Nonparallel Comparable Texts
We demonstrate that the associations between a word and its context seed words are preserved in comparable texts of different languages.
We propose to represent the contexts of a word or phrase with a real-valued vector, which one element corresponds to one word in the contexts.

778
Improving Data Driven Wordclass Tagging by System Combination
In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.
We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.
Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.
After comparison, their outputs are combined using several voting strategies and second stage classifiers.
All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.
We suggest three voting strategies: equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting.

779
Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar
The pseudo-projective grammar we propose can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective.

780
Role of Verbs in Document Analysis
We present results of two methods for assessing the event profile of news articles as a function of verb type.
The unique contribution of this research is the focus on the role of verbs, rather than nouns.
Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile.
The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases.
An evaluation is performed on the results using Kendall's tau.
We present convincing evidence for using verb semantic classes as a discriminant in document classification.
We demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class.

781
Automatic Retrieval and Clustering of Similar Words
Bootstrapping semantics from text is one of the greatest challenges in natural language learning.
We first define a word similarity measure based on the distributional pattern of words.
The similarity measure allows us to construct a thesaurus using a parsed corpus.
We then present a new evaluation methodology for the automatically constructed thesaurus.
The evaluation results show that the thesaurns is significantly closer to WordNet than Roget Thesaurus is.
We use dependency relation as word features to compute word similarities from large corpora.

782
Robust Pronoun Resolution with Limited Knowledge
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.
One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task.
This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
Input is checked against agreement and for a number of antecedent indicators.
Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.
Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data.
In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications.
We first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.
We find that the current evaluation of anaphora resolution algorithms and systems is befeft of any common ground for comparison due to the difference in evaluation data as well as the diversity of pre-processing tools employed by each anaphora resolution system.

783
Multilingual Authoring using Feedback Texts
There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions).
Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base.
For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning.
We introduce here a new technique which employs M-NLG during the phase of knowledge editing.
A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it.
This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering.
We propose WYSIWYM (What You See Is What You Mean) as a method for the authoring of semantic information through direct manipulation of structures rendered in natural language text.
In this system logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages.

784
Statistical Models for Unsupervised Prepositional Phrase Attachment
We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.
Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.
We present results for prepositional phrase attachment in both English and Spanish.
We first assume noun attachment for all of-PPs and then apply our disambiguation methods to all remaining PPs.

785
MindNet: Acquiring and Structuring Semantic Information from Text
As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs.
It is, however, more than this static resource alone.
MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text.
MindNet is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources.

786
Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction
Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand.
In this paper, we present an algorithm for extracting potential entries for a category from an online corpus, based upon a small set of exemplars.
Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area.
Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand.
Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources.
We use co-occurrence statistics in local context to discover sibling relations.
Our experiments were performed using the MUC-4 and Wall Street Journal corpuses (about 30 million words).
To select seed words we rank all of the head nouns in the training corpus by frequency and manually select the first 10 nouns that unambiguously belong to each category.
We find that 3 of every 5 words learned by our system are not present in WordNet.

787
Never Look Back: An Alternative to Centering
I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list).
The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model.
The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora.
The model is the basis for an algorithm which operates incrementally, word by word.
We argue that the information status of an antecedent is more important than the grammatical role in which it occurs.
We evaluate on hand-annotated data.
We restrict our algorithm to the current and last sentence.

788
Measures Of Distributional Similarity
We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.
We use verb-object relations in both active and passive voice constructions.
We find that our asymmetric skew divergence, a generalisation of Kullback-Leibler divergence, persorms best for improving probability estimates for unseen word co-occurrences.

789
Finding Parts In Very Large Corpora
We present a method for extracting parts of objects from wholes (e.g."speedometer" from "car").
Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system.
The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
To filter out attributes that are regarded as qualities (like driving ability) rather than parts (like steering wheels), we remove words ending with the suffixes -ness, -ing, and -ity.

790
Inducing A Semantically Annotated Lexicon Via EM-Based Clustering
We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.
The models are empirically evalutated by a general decision test.
Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora.
We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.
We test 3000 random verb-noun pairs, requiring the erbs and nouns to appear between 30 and 3000 times in training.
We use soft clustering to form classes for generalization and do not take recourse to any hand-crafter resources in our approach to selectional preference induction.

791
Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text
Previous work has shown that automatic methods can be used in building semantic lexicons.
This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet.
We let three judges evaluate ten internal nodes in the hyponym hierarchy that had at least twenty descendants.

792
Development And Use Of A Gold-Standard Data Set For Subjectivity Classifications
This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using statistical techniques.
Biascorrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier.
We use a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation and sentence position.
We define a subjective sentences as sentences expressing evaluations, opinions, emotions and speculations.

793
Automatic Identification Of Non-Compositional Phrases
Non-compositional expressions present a special challenge to NLP applications.
We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus.
Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word.
We use LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds.
We define a decision criterion for non compositional phrases based on the change in the mutual information of a phrase when substituting one word for a similar one based on an automatically constructed thesaurus.

794
Deep Read: A Reading Comprehension System
This paper describes initial work on Deep Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it.
We have acquired a corpus of 60 development and 60 test stories of 3rd to 6th grade material; each story is followed by short-answer questions (an answer key was also provided).
We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).
This simple system retrieves the sentence containing the answer 30-40% of the time.
We use a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story.

795
Corpus-Based Identification Of Non-Anaphoric Noun Phrases
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.
But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., "the White House" or "the news media").
We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems.
Our algorithm generates lists of non-anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents.
We develop a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recogznizing predicative and established DDs, additional techniques for mining from corpora unfamiliar DDS including proper names, larger situation and semantically functional.
We develop an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.

796
Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars
Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words.
We present O(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5).
For a common special case that was known to allow O(n3) parsing (Eisner, 1997), we present an O(n3) algorithm with an improved grammar constant.
We show that the dynamic programming algorithms for lexicalized PCFGs require O(m3) states.

797
A Statistical Parser For Czech
This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order.
These differences are likely to pose new problems for techniques that have been developed on English.
We describe our experience in building on the parsing model of (Collins 97).
Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
We use a transformed tree bank from the Prague Dependency Treebank for constituent parsing on Czech.

798
Automatic Identification Of Word Translations From Unrelated English And German Corpora
Algorithms for the alignment of words in translated texts are well established.
However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.
This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.
The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly.
We create bag-of-words context vectors around both the source and target language words and then project the source into the English target space via the current small translation dictionary.
We filter out bilingual term pairs with low monolingual frequencies (those below 100 times).
We show that accurate translations can be learned for 100 German nouns that are not contained in the seed bilingual dictionary.

799
Mining The Web For Bilingual Text
STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web.
This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance.
The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.
We use structure markup information from pages without looking at their content to attempt to align them.

800
Estimators For Stochastic Unification-Based Grammars
Log-linear models provide a statistically sound framework for Stochastic "Unification-Based" Grammars (SUBGs) and stochastic versions of other kinds of grammars.
We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar.
We incorporate general linguistic principles into a log-linear model.
We use parses generated by a LFG parser as input to a MRF approach.

801
Information Fusion In The Context Of Multi-Document Summarization
We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
Our approach is unique in its usage of language generation to reformulate the wording of the summary.
We observe for that task of multi-document summarization of news articles that extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources.

802
SemEval-2010 Task 13: TempEval-2
Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four subtasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.
Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish.
One of the tasks of this workshop is to determine the temporal relation between an event and a time expression in the same sentence.

803
SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation
This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems.
In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
System answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.
In constructing the dataset we use WordNet to first randomly select one sense of the word and then construct a set of words in relation to the first word's chosen synset.

804
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization
This paper presents the first round of the task on Cross-lingual Textual Entailment for Content Synchronization, organized within SemEval-2012.
The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.
Participants were presented with datasets for different language pairs, where multi-directional entailment relations (forward, backward, bidirectional, no entailment) had to be identified.
We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.

805
Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies
We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system.
We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries.
Finally, we describe two user studies that test our models of multi-document summarization.
Our centroid-based extractive summarizer scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence.

806
Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis
Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction.
Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate.
Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes ("ally" stemming to "all").
We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically.
We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.
We generate a list of N candidate suffixes and use this list to identify word pairs which share the same stem.
We attempt to cluster morphologically related words starting with an unrefined trie search, which contains a parameter of minimum possible stem length and an upper bound on potential affix candidates, that is constrained by semantic similarity in a word context vector space.

807
Inducing Syntactic Categories By Context Distribution Clustering
This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora.
Previous techniques give good results, but fail to cope well with ambiguity or rare words.
An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems.
We apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters.
In our bootstrapping approach we first cluster the most distributionally reliable words and then incrementallly augment each cluster with words that are distributionally similar to those already in the cluster.

808
Introduction To The CoNLL-2000 Shared Task: Chunking
We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.
The dataset is extracted from the WSJ Penn Tree bank and contains 211,727 training examples and 47,377 test instances.

809
Use Of Support Vector Learning For Chunk Identification

810
Two Statistical Parsing Models Applied To The Chinese Treebank
This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al. , 1998) and a TAG-based parsing model, adapted from (Chiang, 2000).
On sentences with < 40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
Our parser operates at word-level with the assumption that input sentences are pre-segmented.

811
Japanese Dependency Structure Analysis Based On Support Vector Machines
This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs).
Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features.
On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space.
Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality.
We apply SVMs to Japanese dependency structure identification problem.
Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences).
We introduce a new type of feature called dynamic features which are created dynamically during the parsing process.

812
Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger
This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.
We achieve 96.9% on seen words and 86.9% on unseen with a MEMM.

813
Evaluation Metrics For Generation
Certain generation applications may profit from the use of stochastic methods.
In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.
In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.
This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.
To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.
The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
We propose Simple String Accuracy as a baseline evaluation metric for natural language generation.

814
Robust Applied Morphological Generation
In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing.
We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required.
We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application.

815
Limitations Of Co-Training For Natural Language Learning From Large Datasets
Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.
This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.
However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.
To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling.
Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.
We show that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.

816
Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy
We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).
In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.
We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.
We classify noun compounds from the domain of medicine using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound.
We use a discriminative classifier to assing 18 relations for noun compounds from biomedical text and achieve 60% accuracy.

817
Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem?
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.
We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement.
We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach.
We show that WordNet is as effective an evaluation resource as the web for MWE detection methods, despite its inherent size limitations and static nature.
We compare the semantic vector of a phrase and the vectors of its component words in two ways: one includes the phrases's contexts in the construction of the semantic vectors of the parts and one does not.

818
Latent Semantic Analysis For Text Segmentation
This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a).
Inter-sentence similarity is estimated by latent semantic analysis (LSA).
Boundary locations are discovered by divisive clustering.
Test results show LSA is a more accurate similarity measure.
We use all vocabulary words to compute low-dimensional document vectors.

819
Corpus Variation And Parser Performance
Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.
We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
This leads us to a technique for pruning parameters to reduce the size of the parsing model.
We show that the accuracy of parsers trained on the Penn Treebank degrades when applied to different genres and domains.
We report results on sentences of 40 or less words on all the Brown corpus sections combined, for which we obtain 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.

820
Assigning Time-Stamps To Event-Clauses
We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation.
We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references.
Evaluations show a performance of 52%, compared to humans.
We infer time values based on the most recently assigned date of the date of the article.

821
Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory
We describe our experience in developing a discourse-annotated corpus for community-wide use.
Working in the framework of Rhetorical Structure Theory, we were able to create a large annotated resource with very high consistency, using a well-defined methodology and protocol.
This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications.
In our Discourse Tree Bank only 26% of Contrast relations are indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases.
Our corpus contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory.

822
NLTK: The Natural Language Toolkit
NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.
NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.
Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials and problem sets.

823
Tuning Support Vector Machines For Biomedical Named Entity Recognition
We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition.
To make the SVM training with the available largest corpus, the GENIA corpus, tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information.
In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.
Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy.
The proposed new features also contribute to improve the accuracy.
We compare our SVM-based recognition system with a system using Maximum Entropy tagging method.
For protein name recognition we achieve scores of 0.492, 0.664 and 0.565 for precision, recall and f-score respectively.
We use a feature set containing lexical information, POS tags, affixes and their combinations in order to recognise and classify terms into a set of general biological classes used within the GENIA project (GENIA, 2003).

824
Machine Transliteration Of Names In Arabic Texts
We present a transliteration algorithm based on sound and spelling mappings using finite state machines.
The transliteration models can be trained on relatively small lists of names.
We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-to-obtain training data.
We apply our transliteration algorithm to the transliteration of names from Arabic into English.
We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation.
We also compare the accuracy of our system to the accuracy of human translators.
We transliterate named entities in Arabic text to English by combining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named entities co-reference, and contextual web counts.
We show that the use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.
Out spelling-based model directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations.

825
Unsupervised Discovery Of Morphemes
We present two methods for unsupervised segmentation of words into morpheme-like units.
The model utilized is especially suited for languages with a rich morphology, such as Finnish.
The first method is based on the Minimum Description Length (MDL) principle and works online.
In the second method, Maximum Likelihood (ML) optimization is used.
The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis.
Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.
Our method is based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function.

826
Building A Sense Tagged Corpus With Open Mind Word Expert
Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.
It is available at http://teach-computers.org.
We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers.
We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
If successful, the collection process can be extended to create the definitive corpus of word sense information.
Finally, in an effort related to the Wikipedia collection process, we implemente the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web.
we presented another interesting proposal which turns to Web users to produce sense-tagged corpora.

827
Learning A Translation Lexicon From Monolingual Corpora
This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.
We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
Experimental results for the construction of a German-English noun lexicon are reported.
Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.
We automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts.

828
Improvements In Automatic Thesaurus Extraction
The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use.
We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency.
We propose an approximation algorithm, based on canonical attributes and coarse and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
We show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases.
We demonstrate that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality.
We find the JACCARD measure and the TTEST weight to have the best performance in our comparison of distance measures.

829
Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs).
The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates.
We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems.
We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
We describe how the voted perceptron can be used to train maximum-entropy style taggers and also give a discussion of the theory behind the perceptron algorithm applied to ranking tasks.
Voted perceptron training attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.

830
An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation
In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.
We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.
In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.
Our feature set consists of the following four types: local context n-grams of nearby words, global context from all the words in the given context, parts-of-speech n-grams of nearby words and syntactic information obtained from parser output.

831
Thumbs Up? Sentiment Classification Using Machine Learning Techniques
We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.
Using movie reviews as data, we find that standard machine learning techniques deflnitively outperform human-produced baselines.
However, the three machine learning methods we employed (Naive Bayes, maximum entropy classiflcation, and support vector machines) do not perform as well on sentiment classiflcation as on traditional topic-based categorization.
We conclude by examining factors that make the sentiment classiflcation problem more challenging.
We collect reviews form a movie database and rate them as positive, negative or neutral based on the training given by the reviewer.
We suggest that term-based models perform better than the frequency-based alternatives.

832
A Phrase-Based Joint Probability Model For Statistical Machine Translation
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.
Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.
We propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristics for phrase extraction.

833
Generation Of Word Graphs In Statistical Machine Translation
The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet, and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: station of departure, station of arrival, earliest departure time and/or latest arrival time.
We generate word graphs for a bottom-top search with the IBM constraints.
A word graph is a weighted directed acyclic graph in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by the model.

834
A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts
This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories.
Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.
Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
We evaluate Basilisk on six semantic categories.
The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
We learn multiple semantic categories simultaneously, relying on the assumption that a word cannot belong to more than one semantic category.

835
Phrasal Cohesion And Statistical Machine Translation
There has been much interest in using phrasal movement to improve statistical machine translation.
We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not.
We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system.
We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.
We measure phrasal cohesion in gold standard alignments by counting crossings.
We compare tree-bank parser style analyses, a variant with flattened VPs and dependency structures.

836
Efficient Deep Processing Of Japanese
We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics.
The grammar is created for use in real world applications, such that robustness and performance issues play an important role.
It is connected to a POS tagging and word segmentation tool.
This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.
Our hand-crafted Japanese HPSG grammar, JACY, provides semantic information as well as linguistically motivated analysis of complex constructions.

837
The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars
The grammar matrix is an open-source starter-kit for the development of broad-coverage HPSGs.
By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding.
Our LinGO Grammar Matrix project is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar.

838
The Parallel Grammar Project
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu.
The ParGram English LFG is a hand-crafter broad-coverage grammar develope d with the XLE platform.

839
Japanese Dependency Analysis Using Cascaded Chunking
In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.
Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.
We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.
Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.
Our cascaded chunking model does not require the probabilities of dependencies and parses a sentence deterministically.

840
A Comparison Of Algorithms For Maximum Entropy Parameter Estimation
Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.
However, the flexibility of ME models is not without cost.
While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters.
In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.
Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.
We introduce the open-source Toolkit for Advanced Discriminative Model which uses a limited-memory variable metric.

841
Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition
We describe the CoNLL-2002 shared task: language-independent named entity recognition.
We give background information on the data sets and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.
We focus on named entity recognition for Spanish and Dutch.

842
Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages
This paper presents a method for inducing translation lexicons between two distant languages without the need for either parallel bilingual corpora or a direct bilingual seed dictionary.
The algorithm successfully combines temporal occurrence similarity across dates in news corpora, wide and local cross-language context similarity, weighted Levenshtein distance, relative frequency and burstiness similarity measures.
These similarity measures are integrated with the bridge language concept under a robust method of classifier combination for both the Slavic and Northern Indian language families.
We induce translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages.
We create bag-of-words context vectors around both the source and target language words and then project the source vectors into the target space via the current small translation dictionary.

843
An Evaluation Exercise For Word Alignment
This paper presents the task definition, resources, participating systems, and comparative results for the shared task on word alignment, which was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.
The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.
We present a small dataset of 447 pairs of non-overlapping sentences which can be used to evaluate the performance of word-alignment systems.

844
Learning Subjective Nouns Using Extraction Pattern Bootstrapping
We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms.
The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences.
First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns.
Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research.
The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.
We use manually derived pattern templates to extract subjective nouns by bootstrapping.
We mine subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds.

845
Unsupervised Personal Name Disambiguation
This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision.
The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process.
The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data.
Performance is evaluated based on both a test set of hand-labeled multi-referent personal names and via automatically generated pseudonames.
We extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people.

846
Bootstrapping POS-Taggers Using Unlabelled Data
This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output.
Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.
We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature.
Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.
Further results show that this form of co-training considerably out-performs self-training.
However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.
We report positive results with little labeled training data but negative results when the amount of labeled training data increases.
We define self-training as a procedure in which a tagger is retrained on its own labeled cache at each round.

847
Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition
We describe the CoNLL-2003 shared task: language-independent named entity recognition.
We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.

848
Language Independent NER Using A Maximum Entropy Tagger
Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance.
This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy.
The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.
We condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document.
Our named entity recogniser is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics.

849
Named Entity Recognition Through Classifier Combination
This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
We test different methods for combining the results of four systems and found that robust risk minimization works best.

850
Named Entity Recognition With Character-Level Models
We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation.
The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
Our best model achieves an overall F1 of 86.07% on the English test data (92.31% on the development data).
This number represents a 25% error reduction over the same model without word-internal (substring) features.
We find that the introduction of character n-gram features improved the overall F1 score by over 20%.

851
Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons

852
Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation
This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline.
We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story.
In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.
Our approach focuses on extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length.

853
Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments
We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.
We also show that predicting labels from a light-weight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.
We argue that deep linguistic features harvested from FrameNet are beneficial for the successful assignment of PropBank roles to constituents.
We use LTAG-based decomposition of parse trees for SRL.
Instead of using the typical parse tree features used in SRL models, we use the path within the elementary tree from the predicate to the constituent argument.

854
Identifying Semantic Roles Using Combinatory Categorial Grammar
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.
We find that using features extracted from a Combinatory Categorical Grammar representation improves semantic labeling performance on core arguments.

855
A General Framework For Distributional Similarity
We present a general framework for distributional similarity based on the concepts of precision and recall.
Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.
We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
We propose a general framework for distributional similarity that consists of notions of precision and recall.

856
Learning Extraction Patterns For Subjective Expressions
This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions.
High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm.
The learned patterns are then used to identify more subjective sentences.
The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision.
We construct a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences.
We introduce a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates using a high-precision sentence-level subjectivity classifier and a large unannotated corpus.

857
Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences
Opinion question answering is a challenging task for natural language processing.
In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.
We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.
We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion.
Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).
At sentence level, we propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences.

858
Improved Automatic Keyword Extraction Given More Linguistic Knowledge
In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
We propose a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results.

859
Transliteration Of Proper Names In Cross-Lingual Information Retrieval
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
We demonstrate the application of statistical machine translation techniques to "translate" the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and finals, commonly used subword units of pronunciation for Chinese.
We then use another statistical translation model to map the initial/final sequence to Chinese characters.
We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries.
We adopt the noisy channel modeling framework.

860
The First International Chinese Word Segmentation Bakeoff
This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.
We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.

861
Chinese Word Segmentation As LMR Tagging
In this paper we present Chinese word segmentation algorithms based on the so-called LMR tagging.
Our LMR taggers are implemented with the Maximum Entropy Markov Model and we then use Transformation-Based Learning to combine the results of the two LMR taggers that scan the input in opposite directions.
Our system achieves F-scores of 95.9% and 91.6% on the Academia Sinica corpus and the Hong Kong City University corpus respectively.
we describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word.

862
HHMM-Based Chinese Lexical Analyzer ICTCLAS
This document presents the results from Inst. of Computing Tech., CAS in the ACL SIGHAN-sponsored First International Chinese Word Segmentation Bake-off.
The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks.
Then provide the evaluation results and give more analysis.
Evaluation on ICTCLAS shows that its performance is competitive.
Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track.
In PK open track, it ranks second position.
ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks.
Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach.
At the same time, we really find our problems during the evaluation.
The bakeoff is interesting and helpful.

863
A Statistical Approach To The Semantics Of Verb-Particles
This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off).
We report first on a framework for implementing and evaluating such models.
We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions.

864
Detecting A Continuum Of Compositionality In Phrasal Verbs
We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.
We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set.
We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.

865
An Empirical Model Of Multiword Expression Decomposability
This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis.
We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.
We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet.
Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet.
we studied vector extraction for phrases because they were interested in the decomposability of multi word expressions.
we propose a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability.

866
Incrementality In Deterministic Dependency Parsing
Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text.
In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework.
However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm.
This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text.

867
Senseval-3 Task: Automatic Labeling Of Semantic Roles
The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset.
The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky.
The FrameNet data provide an extensive body of "gold standard" data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications.
Eight teams participated in the task, with a total of 20 runs.
Discussions among participants during development of the task and the scoring of their runs contributed to a successful task.
Participants used a wide variety of techniques, investigating many aspects of the FrameNet data.
They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study.
Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible.
They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future.
we conduct an evaluation exercise in the Senseval-3 workshop.

868
The Senseval-3 English Lexical Sample Task
This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.
The task drew the participation of 27 teams from around the world, with a total of 47 systems.

869
The English All-Words Task
We describe our experience in preparing the sense-tagged corpus used in the English all-words task and we tabulate the scores.

870
ROUGE: A Package For Automatic Evaluation Of Summaries
ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.
It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.
The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.
This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations.
Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.

871
Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets


872
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus
We describe a new corpus of over 180,000 hand-annotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.
We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.

873
A Linear Programming Formulation For Global Inference In Natural Language Tasks
Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints.
Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc.
We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations.
Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the human-like quality of the inferences.
we use ILP to deal with the joint inference problem of named entity and relation identification.
we applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them.
we described a classification-based framework in which they jointly learn to identify named entities and relations.

874
Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces
This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces.
The context of each instance is represented as a vector in a high dimensional feature space.
Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.
We employ two different representations of the context in which a target word occurs.
First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context.
Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context.
We evaluate the discriminated clusters by carrying out experiments using sense–tagged instances of 24 SENSEVAL2 words and the well known Line, Hard and Serve sense–tagged corpora.

875
Memory-Based Dependency Parsing
This paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text.
Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.
The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank.
The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further.

876
Models For The Semantic Classification Of Noun Phrases
This paper presents an approach for detecting semantic relations in noun phrases.
A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation.
We propose a 35 class scheme to classify relations in various phrases.
We propose a method called semantic scattering for interpreting NCs.

877
The NomBank Project: An Interim Report
This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus.
NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus.
The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text.
This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource.
we provide coarse annotations for some of the possessive con st ructions in the Penn Treebank, but only those that meet their criteria.

878
The Language Of Bioscience: Facts Speculations And Statements In Between
We explore the use of speculative language in MEDLINE abstracts.
Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans.
In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed.
Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language.
we focus on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, and present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts.
we explore issues with annotating speculative language in biomedicine and outline potential applications.
we present a study on annotating hedges in biomedical documents.

879
Integrated Annotation For Biomedical Information Extraction
We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics.
We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities.
Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events.
We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process.

880
Max-Margin Parsing
We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.
Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.
Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.
We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.
we suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems.

881
VerbOcean: Mining The Web For Fine-Grained Semantic Verb Relations
Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks.
We present a semi-automatic method for extracting fine-grained semantic relations between verbs.
We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5 % accuracy.
Analysis of error types shows that on the relation strength we achieved 75 % accuracy.
We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.
we introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations.
we use patterns to extract a set of relations between verbs, such as similarity, strength and antonymy.

882
Scaling Web-Based Acquisition Of Entailment Relations
Paraphrase recognition is a critical step for natural language interpretation.
Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.
However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.
We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases.
We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.
Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.
Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.
we describe the TEASE method for extracting entailing relation templates from the Web.

883
Bilingual Parsing With Factored Estimation: Using English To Parse Korean
We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.
The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.
We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.
we proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model.

884
Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM
We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM.
Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.
But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of “find-one-get-more”, which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity.
We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.
This novel “find-one-get-more” principle allows us to add more parallel sentences from dissimilar documents, to the baseline set.
Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.
We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.

885
Calibrating Features For Semantic Role Labeling
This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited.
We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed.
We further show that different features are needed for different subtasks.
Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models.
We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.

886
Unsupervised Semantic Role Labeling
We present an unsupervised method for labelling the arguments of verbs with their semantic roles.
Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based.
A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model.
We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data.
we present an unsupervised method for labeling the arguments of verbs with their semantic roles.
we perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.

887
Monolingual Machine Translation For Paraphrase Generation
We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.
The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web.
Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.
A monotone phrasal decoder generates contextual replacements.
Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
we built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions.
we present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings.

888
Applying Conditional Random Fields To Japanese Morphological Analysis
This paper presents Japanese morphological analysis based on conditional random fields (CRFs).
Previous work in CRFs assumed that observation sequence (word) boundaries were fixed.
However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible.
We show how CRFs can be applied to situations where word boundary ambiguity exists.
CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis.
First, flexible feature designs for hierarchical tagsets become possible.
Second, influences of label and length bias are minimized.
We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task.
Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.
we studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rule based unknown word processing.

889
Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based?
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.
However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite.
We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at- once approach).
Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).
This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.
We found that while the all-at-once, character-based approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run.
As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.

890
Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot
A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
The technique is applied to the problem of automatically capitalizing uniformly cased text.
Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking.
Capitalization can be also used as a preprocessing step in named entity extraction or machine translation.
A “background” capitalizer trained on 20 M words of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996.
The “in-domain” performance of the WSJ capitalizer is 45% better relative to the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60% relative; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25–70k words – is about 20–25% relative.
Overall, automatic capitalization error rate of 1.4% is achieved on BN data.
The performance gain obtained by employing our adaptation technique using a tiny amount of out-of-domain training data on top of the background data is striking: as little as 0.14 M words of in-domain data brings more improvement than using 10 times more background training data (from 2 M words to 20 M words).
we proposed method for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior.
we use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.

891
A Boosting Algorithm For Classification Of Semi-Structured Text
The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification.
Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.
Accordingly, learning algorithms must be created that can handle the structures observed in texts.
In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.
The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.
We also discuss the relation between our algorithm and SVMs with tree kernel.
Two experiments on opinion/modality classification confirm that subtree features are important.
We adopt the BACT learning algorithm to effectively learn subtrees useful for both antecedent identification and zero pronoun detection.

892
LexPageRank: Prestige In Multi-Document Text Summarization
Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.
Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence.
We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
In this model, a sentence connectivity matrix is constructed based on cosine similarity.
If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix.
We provide an evaluation of our method on DUC 2004 data.
The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems.
we propose Lex PageRank, which is an approach for computing sentence importance based on the concept of eigenvector centrality.

893
Statistical Significance Tests For Machine Translation Evaluation
Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU, METEOR and the related NIST metric, are becoming increasingly important in MT research and development.
This paper presents a significance test-driven comparison of n-gram-based automatic MT evaluation metrics.
Statistical significance tests use bootstrapping methods to estimate the reliability of automatic machine translation evaluations.
Based on this reliability estimation, we study the characteristics of different MT evaluation metrics and how to construct reliable and efficient evaluation suites.

894
TextRank: Bringing Order Into Texts
In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.
In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.
we propose TextRank, which is one of the most well-known graph based approaches to key phrase extraction.
we propose the TextRank model to rank key words based on the co-occurrence links between words.

895
Sentiment Analysis Using Support Vector Machines With Diverse Information Sources
This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text.
Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models.
Experiments on movie review data from the Internet Movie Database demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data.
Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews hand-annotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement.

896
A Statistical Semantic Parser That Integrates Syntax And Semantics
We introduce a learning semantic parser, SCISSOR, that maps natural-language sentences to a detailed, formal, meaning- representation language.
It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label.
A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation.
We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer.
We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches.
we introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax.

897
Generalized Inference With Multiple Semantic Role Labeling Systems
We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicate-argument output by solving an optimization problem.
The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling.
We illustrate a significant improvement in overall SRL performance through this inference.
we adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem.

898
Syntactic Features For Evaluation Of Machine Translation
Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems.
We explore the use of syntactic information, including constituent labels and head-modier dependencies, in computing similarity between output and reference.
Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
we measure the syntactic similarity between MT output and reference translation.
we used syntactic structure and dependency information to go beyond the surface level matching.

899
METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments
We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations.
Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies.
Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference.
We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality.
We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets.
We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data.
This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination.
We also perform experiments to show the relative contributions of the various mapping modules.

900
Measuring The Semantic Similarity Of Texts
This paper presents a knowledge-based method for measuring the semantic-similarity of texts.
While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored.
In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
we proposed a hybrid method by combining six existing knowledge-based methods.

901
Better K-Best Parsing
We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing.
To demonstrate the efficiency, scal- ability and accuracy of these algorithms, we present experiments on Bikel’s implementation of Collins’ lexicalized PCFG model, and on Chiang’s CFG-based decoder for hierarchical phrase-based translation.
We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.

902
A Classifier-Based Parser With Linear Run-Time Complexity
We present a classifier-based parser that produces constituent trees in linear time.
The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar.
This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.
We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.
We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
we propose a constituency based parsing method to determine sentence dependency structures.

903
Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text
This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts.
We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.
This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet.
We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.
For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.
Our experimental results show that our system performs significantly better than the baseline.
we identify opinion holders and targets with semantic role labeling.

904
Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis
Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non-compositional meanings.
We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.
We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.
we devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainningdata.
we used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors.

905
SPMT: Statistical Machine Translation With Syntactified Target Language Phrases
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.

906
Phrasetable Smoothing For Statistical Machine Translation
We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.
We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used.
The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric.

907
Domain Adaptation With Structural Correspondence Learning
Discriminative learning methods are widely used in natural language processing.
These methods work best when their training and test data are drawn from the same distribution.
For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent.
In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain.
We introduce structural correspondence learning to automatically induce correspondences among features from different domains.
We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.
Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005).
Our approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data.
We introduce SCL that is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification.
We apply the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP.
We append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.

908
Incremental Integer Linear Programming For Non-Projective Dependency Parsing
Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints.
However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable.
We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs.
This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over state-of-the-art.
For dependency parsing, we study a method using integer linear programming which can incorporate global linguistic constraints.
Our work in dependency parsing demonstrate that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner.
We show that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used.
We tackle the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated.

909
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts
We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.
To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.
We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.
We present a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates.

910
Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis
This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain.
The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses.
As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts.
Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values.
The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon.
We validate that polar text units with the same polarity tend to appear together to make contexts coherent.
We propose an algorithm to automatically expand an initial opinion lexicon based on context coherency, the tendency for same polarities to appear successively in contexts.
We use conjunction rules to solve this problem from large domain corpora.
We adopt domain knowledge by extracting sentiment words from the domain-specific corpus.

911
Joint Extraction Of Entities And Relations For Opinion Recognition
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
We identify two types of opinion-related entities — expressions of opinions and sources of opinions — along with the linking relation that exists between them.
Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.
Performance further improves when a semantic role labeling system is incorporated.
The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area.
We propose an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference.
Others extend the token-level approach to jointly identify opinion holders (Choi et al 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010).

912
Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger
In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.
The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs.
Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.
Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named entity information.
We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.
Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known "first-sense" baseline.
Our supersense tagger annotates text with a 46-label tag set of WNSS categories.

913
Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts
In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text.
We use the co–occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet.
Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.
We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.
This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech.
In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information.
We create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss.
We introduce a vector measure to determine the relatedness between pairs of concepts.

914
Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels
In this paper we investigate a new problem of identifying the perspective from which a document is written.
By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans.
Can computers learn to identify the perspective of a document?
Not every sentence is written strongly from a perspective.
Can computers learn to identify which sentences strongly convey a particular perspective?
We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict.
The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.
We use hierarchical Bayesian modelling for opinion modelling (Lin et al, 2006).
Our experiments were conducted in political debate corpus (Lin et al 2006).
We explore relationships between sentence-level and document-level classification for a stance-like prediction task.
We introduce implicit sentiment a topic of study in computational linguistics under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science.

915
CoNLL-X Shared Task On Multilingual Dependency Parsing
Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.
The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.
In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.
We also give an overview of the parsing approaches that participants took and the results that they achieved.
Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?
The CoNLL-X shared tasks focused on multilingual dependency parsing.

916
Experiments With A Multilanguage Non-Projective Dependency Parser
We present a deterministic classifier-based Shift/Reduce parser.
We develop DeSR, an incremental deterministic classifier-based parser.
We propose a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.

917
Multilingual Dependency Analysis With A Two-Stage Discriminative Parser
We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages.
The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.
The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.
We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.
We use post-processing for non-projective dependencies and for labeling.
We treat the labeling of dependencies as a sequence labeling problem.
The specific graph-based model studied in this work factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc.

918
Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines
We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
We present evaluation results and an error analysis focusing on Swedish and Turkish.
Our pseudo-projective approach transforms non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006).

919
Why Generative Phrase Models Underperform Surface Heuristics
We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation.
We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics.
The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.
In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.
Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score.
We also show that interpolation of the two methods can result in a modest increase in BLEU score.
We try a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again find that the standard model outperforms their generative model.
We explore estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation.
We conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.

920
Discriminative Reordering Models For Statistical Machine Translation
We present discriminative reordering models for phrase-based statistical machine translation.
The models are trained using the maximum entropy principle.
We use several types of features: based on words, based on word classes, based on the local context.
We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus.
Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.
Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model.
To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used.
We use clustered word classes in a discriminate reordering model, and show that they reduce the classification error rate.

921
Manual And Automatic Evaluation Of Machine Translation Between European Languages
We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.
Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.
The results of the workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems.
We report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric.

922
Syntax Augmented Machine Translation Via Chart Parsing
We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.
We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.
Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar.
We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system.
Our translation system is available open-source under the GNU General Public License.
In our work, syntax is successfully integrated into hierarchical SMT.
We start with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.
We use broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process.

923
A Syntax-Directed Translator With Extended Domain Of Locality
A syntax-directed translator first parses the source-language input into a parse tree, and then recursively converts the tree into a string in the target-language.
We model this conversion by an extended tree-to-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.
We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.
The model is then extended to the general log-linear frame work in order to rescore with other features like n-gram language models.
We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.
Initial experimental results on English-to-Chinese translation are presented.
We study a TSG-based tree-to-string alignment model.
We define the Extended Tree-to-String Transducer.

924
Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.
Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.
In particular, we are interested in the situation where labeled data is scarce.
We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating inference performance.
We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.
We then solve an optimization problem to obtain a smooth rating function over the whole graph.
When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.
We adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features.
We propose a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarce.

925
Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems
We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.
After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation.
At this, the fact is employed that the small-world property holds for many graphs in NLP.
We introduce the co-occurrence based graph clustering framework.

926
Inversion Transduction Grammar for Joint Phrasal Translation Modeling
We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models.
This syntactic model is similar to its flat-string phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training.
We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm.
We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score.
Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method.
We use synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences.

927
CCG Supertags in Factored Statistical Machine Translation
Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.
The challenge is incorporating this information into the translation process.
Factored translation models allow the inclusion of supertags as a factor in the source or target language.
We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.
we exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse.

928
Mixture-Model Adaptation for SMT
We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
We investigate a number of variants on this approach, including cross-domainversus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
We conclude that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly.
We interpolate the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.

929
(Meta-) Evaluation of Machine Translation
This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.
We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.
We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation.
We measured the correlation of automatic evaluation metrics with human judgments.
This meta-evaluation reveals surprising facts about the most commonly used methodologies.
We show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency.

930
Experiments in Domain Adaptation for Statistical Machine Translation
The special challenge of the WMT 2007 shared task was domain adaptation.
We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches).
This paper also gives a description of the submission of the University of Edinburgh to the shared task.
Factored translation models is used for the integration of domain adaptation.
We use two language models and two translation models: one in-domain and other out-of-domain to adapt the system.
We learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.

931
METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments
Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric.
It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.
In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages.

932
The Third PASCAL Recognizing Textual Entailment Challenge
This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems.
In creating this year’s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios.
Additionally, a pool of resources was offered so that the participants could share common tools.
A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions.
26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.
The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007).
Textual Entailment (TE) has become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications.

933
Detection of Grammatical Errors Involving Prepositions
This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.
Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students.
To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks.
A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the preposition, plus the head verb in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available.

934
SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems
The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.
In total there were 6 participating systems.
We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using Onto Notes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).
We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.
The object of the sense induction task of SENSEVAL-4 is to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes.
Graph-based methods have been employed for word sense induction.

935
SemEval-2007 Task 07: Coarse-Grained English All-Words Task
This paper presents the coarse-grained English all-words task at SemEval-2007.
We describe our experience in producing a coarse version of the WordNet sense inventory and preparing the sense-tagged corpus for the task.
We present the results of participating systems and discuss future directions.
We show that the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007).

936
SemEval-2007 Task 10: English Lexical Substitution Task
In this paper we describe the English Lexical Substitution task for SemEval.
In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.
The task involves both finding the synonyms and disambiguating the context.
Participating systems are free to use any lexical resource.
There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is.
In the lexical substitution task, a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved.
We establish a benchmark for context-sensitive lexical similarity models.

937
The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task
This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.
This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
We consider the problem of disambiguating person names in a Web searching scenario.
The goal of the Web People Search task is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity.
Our Web Persona Search (WePS) task has created a benchmark dataset.

938
SemEval-2007 Task 15: TempEval Temporal Relation Identification
The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations.
It avoids the pitfalls of evaluating a graph of inter-related labels by defining three subtasks that allow pairwise evaluation of temporal relations.
The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing.
Temporal information processing is a topic of natural language processing boosted by our evaluation campaign TempEval.
TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and.

939
SemEval-2007 Task-17: English Lexical Sample SRL and All Words
This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 - Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively.
We tabulate and analyze the results of participating systems.
The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90%.

940
SemEval-2007 Task 19: Frame Semantic Structure Extraction
This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http://framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects).
The training data was FN annotated sentences.
In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles.
Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.
Our shared tasks shows that frame-semantic SRL of running text is a hard problem, partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.

941
On the Complexity of Non-Projective Data-Driven Dependency Parsing
In this paper we investigate several non-projective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model.
We also investigate algorithms for non-projective parsing that account for non-local information, and present several hardness results.
This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.
We claim that the main obstacle is that non-projective parsing is NP-hard beyond arc-factored models.

942
Further Meta-Evaluation of Machine Translation
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.
We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort.
We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.
Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make.
Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns.

943
Optimizing Chinese Word Segmentation for Machine Translation Performance
Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood.
In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.
We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation.
Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.
We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase.
We develop the CRF-based Stanford Chinese segmenter that is trained on the segmentation of the Chinese Treebank for consistency.
We enhance a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence.

944
Parallel Implementations of Word Alignment Tool
Training word alignment models on large corpora is a very time-consuming processes.
This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process.
One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology.
Results show a near-linear speed-up according to the number of CPUs used, and alignment quality is preserved.
We use a multi-threaded version of the GIZA++ tool. This speeds up the process and corrects an error of GIZA++ that can appear with rare words.

945
The Stanford Typed Dependencies Representation
This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding.
For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations.
We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations.
Finally, we address the question of the suitability of the Stanford scheme for parser evaluation.
Stanford dependencies provide a simple description of relations between pairs of words in a sentence.

946
TAG Dynamic Programming and the Perceptron for Efficient Feature-Rich Parsing
We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees.
The formalism allows a rich set of parse-tree features, including PCFG-based features, bigram and trigram dependency features, and surface features.
A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved.
We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.
A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient.
Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.
Many edges can be ruled out beforehand based on the marginals computed from a simpler parsing model (Carreras et al2008).

947
The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies
The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting.
In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies.
This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year's syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates.
In this paper, we define the shared task and describe how the data sets were created.
Furthermore, we report and analyze the results and describe the approaches of the participating systems.
We first introduce the predicate classification task, which can be regarded as the predicate sense disambiguation.
The complete merging process and the conversion from the constituent representation to dependencies is detailed in this work.

948
Dependency-based Syntactic&#x2013;Semantic Analysis with PropBank and NomBank
This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008).
To tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic subcomponent.
The syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.
The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems.
The system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49.
Our system use two 30 different subsystems to handle verbal and nominal predicates, respectively.
We present importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis.
In our work, the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined.

949
Findings of the 2009 Workshop on Statistical Machine Translation
This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task.
We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries.
We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics.
We present a new evaluation technique whereby system output is edited and judged for correctness.
Our Fr-En 109 corpus aggregates huge numbers of parallel French English sentences from the web.
We show that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method.

950
Joshua: An Open Source Toolkit for Parsing-Based Machine Translation
We describe Joshua, an open source toolkit for statistical machine translation.
Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array grammar extraction and minimum error rate training.
It uses parallel and distributed computing techniques for scalability.
We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.
We develop the syntax-based MT system Joshua, which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimization.

951
Domain Adaptation for Statistical Machine Translation with Monolingual Resources
Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.
The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.
Previous work showed small performance gains by adapting from limited in-domain bilingual data.
Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.
We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.
Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanish–English part of the UN corpus, and adapted on the corresponding Europarl data.
Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.
By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.
In order to use source-side monolingual data, we employ the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.
We adapt an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses.

952
Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric
Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance.
Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics.
We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation.
Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
We extend the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment.

953
A Metalearning Approach to Processing the Scope of Negation
Finding negation signals and their scope in text is an important subtask in information extraction.
In this paper we present a machine learning system that finds the scope of negation in biomedical texts.
The system combines several classifiers and works in two phases.
To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types.
It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results.
we describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009).
we pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal.

954
Design Challenges and Misconceptions in Named Entity Recognition
We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system.
In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system.
In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.
We use the IOBES notation to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels.
We investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself.
We have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time.

955
Learning the Scope of Hedge Cues in Biomedical Texts
Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information.
In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts.
The system is based on a similar system that finds the scope of negation cues.
We show that the same scope finding approach can be applied to both negation and hedging.
To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types.
We develop a scope detector following a supervised sequence labeling approach.
We present a meta-learning system that finds the scope of hedge cues in biomedical texts.
We use shallow syntactic features.

956
Overview of BioNLP&rsquo;09 Shared Task on Event Extraction
Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons.
In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk.
In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level).
We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech.
We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.
The BioNLP 09 Shared Task on Event Extraction, the first large scale evaluation of biomedical event extraction systems, drew the participation of 24 groups and established a standard event representation scheme and datasets.
The BioNLP 09 Shared Task is the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations.

957
Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon
Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons.
In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk.
In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level).
We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech.
We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand.
We focus on emotion evoked by common words and phrases.
We explore the use of Mechanical Turk to build the lexicon based on human judgment.
We create a crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions.

958
Creating Speech and Language Data With Amazon&rsquo;s Mechanical Turk
In this paper we give an introduction to using Amazon’s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies.
We survey the papers published in the NAACL2010 Workshop.
24 researchers participated in the workshop’s shared task to create data for speech and language applications with $100.
We experiment with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010).
We provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data.

959
Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation
This paper presents the results of the WMT10 and Metrics MATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task.
We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries.
We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics.
This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk.
We release the News test set in the 2010 Workshop on Statistical Machine Translation.

960
A Regression Model of Adjective-Noun Compositionality in Distributional Semantics
In this paper we explore the computational modelling of compositionality in distributional models of semantics.
In particular, we model the semantic composition of pairs of adjacent English Adjectives and Nouns from the British National Corpus.
We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.
We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.
We propose two evaluation methods for the implemented models.
Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.
Our approach to the semantic composition of adjectives with nouns draws on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors.
Our main innovation is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model.
We look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.

961
Driving Semantic Parsing from the World&rsquo;s Response
Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms.
Providing this supervision is a major bottleneck in scaling semantic parsers.
This paper presents a new learning paradigm aimed at alleviating the supervision burden.
We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world.
In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision.
Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers.
We train systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.

962
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text
The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction.
The CoNLL-2010 shared task, aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information.
The goal of the CoNLL 2010 Shared Task is to develop linguistic scope detectors as well.

963
Sentiment Analysis of Twitter Data
We examine sentiment analysis on Twitter data.
The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering.
The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.
In our work a study was conducted on a reduced corpus of tweets labelled manually.

964
Overview of BioNLP Shared Task 2011
The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams.
Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully fully generalize in various aspects.
The BioNLP 2011 Shared Task series generalized this defining a series of tasks involving more text types, domains and target event types.

965
Overview of Genia Event Task in BioNLP Shared Task 2011
The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.
As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.
After a 3-month system development period, 15 teams submitted their performance results on test cases.
The results show the community has made a significant advancement in terms of both performance improvement and generalization.

966
CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes
The CoNLL-2011 shared task involved predicting coreference using OntoNotes data.
Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities.
OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types.
OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure.
This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems.
Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.
An overview of all systems participating in the CONLL-2011 shared task and their results is provided here.

967
Stanfordâ€™s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task
This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task.
Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information.
All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster.
We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.
Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.
The Stanford coreference resolver, which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.

968
Findings of the 2011 Workshop on Statistical Machine Translation
This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.
We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.
We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.
This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.
Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources.

969
Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems
This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.
New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.
We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.

970
KenLM: Faster and Smaller Language Model Queries
We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs.
The PROBING data structure uses linear probing hash tables and is designed for speed.
Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory.
The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption.
TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline.
Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.
This paper describes the several performance techniques used and presents benchmarks against alternative implementations.
We describe a language modeling library.

971
Findings of the 2012 Workshop on Statistical Machine Translation
This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.
We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.
We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.
We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.
We report for several automatic metrics on the whole WMT12 English-to-Czech dataset.

972
Robust Bilingual Word Alignment For Machine Aided Translation
We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages.
The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues.
Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992).
The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone.
More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology.
We show that knowledge of target-text length is not crucial to the model's performance.

973
Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible?
I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.
Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions.
We show that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects.

974
Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging
In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus.
We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers.
Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
We present a rule-based part-of-speech tagger for unsupervised training corpus.
We propose a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.

975
Prepositional Phrase Attachment Through A Backed-Off Model
Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.
Typically, ambiguous verb phrases of the form v rip1 p rip2 are resolved through a model which considers values of the four head words (v, nl, p and 77,2).
This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable.
Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.
We use a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results).
We introduce modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model.

976
A Bayesian Hybrid Method For Context-Sensitive Spelling Correction
Two classes of methods have been shown to be useful for resolving lexical ambiguity.
The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.
These methods have complementary coverage: the former captures the lexical "atmosphere" (discourse topic, tense, etc.), while the latter captures local syntax.
Yarowsky has exploited this complementarity by combining the two methods using decision lists.
The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.
This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.
Decision lists are found, by and large, to outperform either component method.
However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.
A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.
We build a classifier based on a rich set of context features.

977
Disambiguating Noun Groupings With Respect To Wordnet Senses
Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve.
However, for many tasks, one is interested in relationships among word senses, not words.
This paper presents a method for automatic sense disambiguafton of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.
Disambiguation is performed with respect to WordNet senses, which are fairly fine-gained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels.
The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented.
In this work, the assessment of semantic similarity using a dictionary database as knowledge source is recognized as providing significant cues for word clustering.
We define the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus.
We attempt to combine paradigmatic and syntagmatic similarity strategies.

978
Text Chunking Using Transformation-Based Learning
Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy.
The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive "baseNP" chunks.
For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word.
In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence.
Some interesting adaptations to the transformation-based learning approach are also suggested by this application.
We formalize chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O) outside of a chunk.
We pioneer the machine learning techniques to chunking problem.

979
Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons
This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources.
The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework.
A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades.
The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance.
Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used.
This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable.
Moreover, three of the four filters prove useful even when used with large training corpora.
We use the Longest Common Subsequence Ratio (LCSR) to measure similarity.

980
MBT: A Memory-Based Part Of Speech Tagger-Generator
We introduce a memory-based approach to part of speech tagging.
Memory-based learning is a form of supervised learning based on similarity-based reasoning.
The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory.
Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger.
Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably.
Memory-based tagging shares this advantage with other statistical or machine learning approaches.
Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.
In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases.
The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.
Our tagger uses a very fine-grained tag set.

981
Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning
This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
The specific problem tested involves disambiguating six senses of the word "line" using the words in the current and proceeding sentence as context.
The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference.
We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.
We argue that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination.

982
A Maximum Entropy Model For Part-Of-Speech Tagging
This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%).
The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the POS tag.
Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
We assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words.
We release a publicly available maximum entropy tagger.

983
Efficient Algorithms For Parsing The DOP Model
Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c).
Unfortunately, existing algorithms are both computationally intensive and difficult to implement.
Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.
In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar.
We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree.
Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate.
This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data.
We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
We give a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.

984
Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary
This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.
We propose a new supervised learning method for PP- attachment based on a semantically tagged corpus.
Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags.
We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.
we developed a customized, explicit WSD algorithm as part of their decision tree system.

985
Finding Terminology Translations From Non-Parallel Corpora
We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups.
Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures.
Word Relation Matrices are then mapped across the corpora to find translation pairs.
Translation accuracies are around 30% when only the top candidate is counted.
Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.
In our work, a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed.

986
Selectional Preference And Sense Disambiguation
The absence of training data is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon.
Selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation.
We define selectional preference as the amount of information a verb provides about its semantic argument classes.
We present a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes.
In determining selectional preferences, we use uniformly distributing observed frequencies for a given word across all its senses.

987
A Linear Observed Time Statistical Parser Based On Maximum Entropy Models
This paper presents a statistical parser for natural language that obtains a parsing accuracy--roughly 87% precision and 86% recall--which surpasses the best previously published results on the Wall St. Journal domain.
The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework.
The observed running time of the parser on a test sentence is linear with respect to the sentence length.
Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.
We introduce the idea of oracle re-ranking: suppose there exists a perfect re-ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence.
We take a pipeline approach in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure.

988
Global Thresholding And Multiple-Pass Parsing
We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level.
We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement.
We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms.
we describe a method for producing a simple but crude approximate grammar of a standard context-free grammar.

989
Automatic Discovery Of Non-Compositional Compounds In Parallel Data
Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English.
Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word.
This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit.
The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages.
It can discover hundreds of non-compositional compounds on each iteration, and constructs longer compounds out of shorter ones.
Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output.
The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations.
we propose a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model.
we investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take non compositional compounds into account are more accurate.

990
A Corpus-Based Approach For Building Semantic Lexicons
Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application.
Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic.
In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories.
The input to the system is a small set of seed words for a category and a representative text corpus.
The output is a ranked list of words that are associated with the category.
A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon.
In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.
We find that nouns in conjunctions or appositives tend to be semantically related.
We suggest using conjunction and appositive data to cluster nouns; we approximate this data by looking at the nearest NP on each side of a particular NP.
We also give credit for words associated with but not belonging to a particular category.

991
Distinguishing Word Senses In Untagged Text
This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text.
The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text.
These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs.
Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.
we propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word.

992
Using Lexical Chains For Text Summarization
We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains.
We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger and shallow parser for the identification of nominal groups, and a segmentation algorithm derived from (Hearst, 1994).
Summarization proceeds in three steps: the original text is first segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted from the text.
We present in this paper empirical results on the identification of strong chains and of significant sentences.
We find that the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet.
In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary.
Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions.

993
From Discourse Structures To Text Summaries
We describe experiments that show that the concepts of rhetorical analysts and nuclearity can be used effectively for determining the most important units in a text.
We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summarization program.

994
GermaNet - A Lexical-Semantic Net For German
We present the lexical-semantic net for German "GermaNet" which integrates conceptual ontological information with lexical semantics, within and across word classes.
It is compatible with the Princeton WordNet but integrates principle-based modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations.
GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs.
It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interaction of syntax and semantics.
The development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora.
GermaNet is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy.

995
CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources
This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns.
It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution.
The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied.
The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient.
Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension.
The system has been evaluated in two distinct experiments which support the overall validity of the approach.
our method, CogNiac is a knowledge poor approach to anaphora resolution based on a set of high confidence rules which are successively applied over the pronoun under consideration.

996
Indexing With WordNet Synsets Can Improve Text Retrieval
The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms.
This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the SEMCOR semantic concordance.
The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured.
Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing.
We point out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.

997
An Empirical Approach To Conceptual Case Frame Acquisition
Conceptual natural language processing systems usually rely on case frame instantiation to recognize events and role objects in text.
But generating a good set of case frames for a domain is time-consuming, tedious, and prone to errors of omission.
We have developed a corpus-based algorithm for acquiring conceptual case frames empirically from unannotated text.
Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons.
Given extraction patterns and a semantic lexicon for a domain, our algorithm learns semantic preferences for each extraction pattern and merges the syntactically compatible patterns to produce multi-slot case frames with selectional restrictions.
The case frames generate more cohesive output and produce fewer false hits than the original extraction patterns.
Our system requires only preclassified training texts and a few hours of manual review to filter the dictionaries, demonstrating that conceptual case frames can be acquired from unannotated text without special training resources.
our Conceptual Case Frame Acquisition project, extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions.

998
Edge-Based Best-First Chart Parsing
Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged "best" by some probabilistic figure of merit (FOM).
Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM.
This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finer-grained control over parsing effort.
We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG.
The results obtained are about a factor of twenty improvement over the best prior results - that is, our parser achieves equivalent results using one twentieth the number of edges.
Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing.
We introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found.

999
Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition
This paper describes a novel statistical named-entity (i.e. "proper name") recognition system built around a maximum entity framework.
By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.
These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.
The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems.
However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.

1000
A Statistical Approach To Anaphora Resolution
This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.
We incorporate multiple anaphora resolution factors into a statistical framework -- specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
We combine them into a single probability that enables us to identify the referent.
Our first experiment shows the relative contribution of each source of information and demonstrates a success rate of 82.9% for all sources combined.
The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.
We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy.
We add annotation of the antecedents of definite pronouns to Treebank.
We implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm.
We count the number of times a discourse entities has been mentioned in the discourse already.
Our probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occurring in the local context of the pronoun.
We describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information.

1001
Experiments Using Stochastic Search For Text Planning
Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text?
We describe experiments with a number of heuristic Search methods for this task.
We investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations.
We advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.

1002
WordNet 2 - A Morphologically And Semantically Enhanced Resource
This paper presents an on-going project intended to enhance WordNet morphologically and semantically.
The motivation for this work steams from the current limitations of WordNet when used as a linguistic knowledge base.
We envision a software tool that automatically parses the conceptual defining glosses, attributing part-of-speech tags and phrasal brackets.
The nouns, verbs, adjectives and adverbs from every definition are then disambiguated and linked to the corresponding synsets.
This increases the connectivity between synsets allowing the retrieval of topically related concepts.
Furthermore, the tool transforms the glosses, first into logical forms and then into semantic forms.
Using derivational morphology new links are added between the synsets.
We propose a scheme for attaching sense tags to predicates within the framework of transforming WordNet glosses into a logical form.
The eXtended WordNet is a publicly available version of WordNet in which (among other things) each term occurring in a WordNet gloss (except those in example phrases) is lemmatized and mapped to the synset in which it belongs.

1003
Improved Alignment Models For Statistical Machine Translation
In this paper, we describe improved alignment models for statistical machine translation.
The statistical translation approach uses two types of information: a translation model and a language model.
The language model used is a bigram or general m-gram model.
The translation model is decomposed into a lexical and an alignment model.
We describe two different approaches for statistical translation and present experimental results.
The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.
We present results using the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domain spoken-language task.
The experimental tests were performed on both the text transcription and the speech recognizer output.
To obtain the best single alignment, we use a post-hoc algorithm to merge directional alignments.
We propose a heuristic, where all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?.

1004
Noun Phrase Coreference As Clustering
This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution.
It differs from existing methods in that it views coreference resolution as a clustering task.
In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation.
More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem.
The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes.
We combine the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm.
Approaches to coreference resolution that rely only on clustering can easily enforce transitivity.
We use pairwise NP distances to cluster document mentions.
Our system uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm.
Coreference resolution is performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion.

1005
Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence
Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications.
This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.
The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools.
We consider one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document.
We take a character-level approach to named entity recognition (NER) using prefix and suffix tries.
The bootstrapping stage uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached.

1006
Unsupervised Models For Named Entity Classification
This paper discusses the use of unlabeled examples for the problem of named entity classification.
A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier.
However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple "seed" rules.
The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.
We present two algorithms.
The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).
The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).
We extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree.

1007
Exploiting Diversity In Natural Language Processing: Combining Parsers
Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.
Two general approaches are presented and two combination techniques are described for each approach.
Both parametric and non-parametric models are explored.
The resulting parsers surpass the best previously published performance results for the Penn Treebank.
We improve the best parser's F-measure of 89.7 to 91.3, using naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction).
Regarding the system combination study, we propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees.
We perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined.

1008
Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning
We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units.
Several potential features are investigated and an optimal combination is selected via machine learning.
We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem.
Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units.
At the level of short passages or sentences, we go beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words.

1009
Cascaded Grammatical Relation Assignment
In this paper we discuss cascaded Memory-Based grammatical relations assignment.
In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).
In the last stage, we assign grammatical relations to pairs of chunks.
We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.
We achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences.

